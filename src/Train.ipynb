{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from sklearn.linear_model import Ridge\n",
    "import pickle\n",
    "# custom functions #\n",
    "import utils\n",
    "from sc_reconstructor import SC_Reconstructor\n",
    "from vdvae import VDVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588554b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: jonathan_refiner\n",
      "--data_path=/weka/proj-medarc/shared/mindeyev2_dataset                     --cache_dir=/weka/proj-medarc/shared/cache                     --model_name=jonathan_refiner                     --subj=1 --num_sessions=40                     --no-dual_guidance --no-blurry_recon --caption_type medium\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"jonathan_refiner\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --cache_dir=/weka/proj-medarc/shared/cache \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --subj=1 --num_sessions=40 \\\n",
    "                    --no-dual_guidance --no-blurry_recon --caption_type medium\"\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(model_name='jonathan_refiner', data_path='/weka/proj-medarc/shared/mindeyev2_dataset', cache_dir='/weka/proj-medarc/shared/cache', subj=1, num_sessions=40.0, prompt_recon=True, blurry_recon=False, seed=42, weight_decay=100000, max_iter=50000, dual_guidance=False, caption_type='medium', retrieval=True)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=float, default=40,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prompt_recon\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Use for prompt generating\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",type=int,default=100000,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_iter\",type=int,default=50000,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--caption_type\",type=str,default='medium',choices=['coco','short', 'medium', 'schmedium'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--retrieval\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "print(f\"args: {args}\")\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "os.makedirs(outdir,exist_ok=True)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4743c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27000, 15724]) (27000,)\n",
      "Loaded subj 1 betas!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, valid_nsd_ids_train, x_test, test_nsd_ids = utils.load_nsd(subject=subj, num_sessions=num_sessions, data_path=data_path)\n",
    "print(x_train.shape, valid_nsd_ids_train.shape)\n",
    "\n",
    "print(f\"Loaded subj {subj} betas!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6bbe77-fea5-4761-9949-e34e6f1bf66a",
   "metadata": {},
   "source": [
    "## Prepare git feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b65a845-8c49-4fda-826a-2a3194ab8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git_image_features.hdf5 already exist!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f'{data_path}/git_image_features.hdf5'):\n",
    "    print(\"Creating Git Feature...\")\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    from transformers import AutoProcessor, GitVisionModel, AutoModelForCausalLM, GitModel\n",
    "    from modeling_git import GitForCausalLMClipEmb\n",
    "    # Load 73k NSD images\n",
    "    f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "    beta_images = f['images'] \n",
    "    print(\"Loaded all 73k possible NSD images to cpu!\", beta_images.shape)\n",
    "\n",
    "    git_images = []\n",
    "    processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "    \n",
    "    git_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "    git_text_model.to(device)\n",
    "    git_text_model.eval().requires_grad_(False)\n",
    "    print(\"success load Git model\")\n",
    "    for i, image in enumerate(tqdm(beta_images)):\n",
    "        pil_image = (image.transpose((1, 2, 0))*255).astype(np.uint8)\n",
    "        inputs = processor(images=pil_image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        outputs = git_text_model.git.image_encoder(inputs).last_hidden_state\n",
    "        # valid the captions\n",
    "        if i <= 5:\n",
    "            generated_ids = git_text_model.generate(pixel_values=outputs, max_length=50)\n",
    "            generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            print(generated_caption)\n",
    "        git_images.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    with h5py.File(f'{data_path}/git_image_features.hdf5', 'w') as f:\n",
    "        f.create_dataset('features', data=np.array(git_images))\n",
    "    print(\"Finished!\")\n",
    "    del beta_images, git_images\n",
    "else:\n",
    "    print(\"git_image_features.hdf5 already exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! (73000, 3, 224, 224)\n",
      "Loaded all 73k NSD captions to cpu! (73000,)\n",
      "Filtered down to only the 27000 training images for subject 1!\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)\n",
    "\n",
    "# Load 73k NSD captions\n",
    "if caption_type == \"schmedium\":\n",
    "    captions_small = np.load(f'{data_path}/preprocessed_data/short_length_captions.npy')\n",
    "    captions_medium = np.load(f'{data_path}/preprocessed_data/mid_length_captions_73K.npy')\n",
    "    # Create a mask to randomly select elements from both arrays\n",
    "    mask = np.random.rand(len(captions_small)) > 0.5\n",
    "    # Mix the arrays based on the mask\n",
    "    captions = np.where(mask, captions_small, captions_medium)\n",
    "else:\n",
    "    if caption_type == \"coco\":\n",
    "        caption_file = \"annots_73k.npy\"\n",
    "    elif caption_type == \"short\":\n",
    "        caption_file = \"short_length_captions.npy\"\n",
    "    elif caption_type == \"medium\":\n",
    "        caption_file = \"mid_length_captions_73K.npy\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid caption type\")\n",
    "    captions = np.load(f'{data_path}/preprocessed_data/{caption_file}')\n",
    "print(\"Loaded all 73k NSD captions to cpu!\", captions.shape)\n",
    "\n",
    "train_images = torch.zeros((len(valid_nsd_ids_train), 3, 224, 224))\n",
    "train_captions = np.zeros((len(valid_nsd_ids_train),), dtype=object)\n",
    "\n",
    "# Load specific training data\n",
    "for i, idx in enumerate(valid_nsd_ids_train):\n",
    "    train_images[i] =  torch.from_numpy(images[idx])\n",
    "    train_captions[i] = captions[idx]\n",
    "    \n",
    "print(f\"Filtered down to only the {len(valid_nsd_ids_train)} training images for subject {subj}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168051b",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a17b31",
   "metadata": {},
   "source": [
    "### Feature extractor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable Cascade Reconstructor: Loading model...\n",
      "['model_version', 'effnet_checkpoint_path', 'previewer_checkpoint_path']\n",
      "['transforms', 'clip_preprocess', 'gdf', 'sampling_configs', 'effnet_preprocess']\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 79.11 GiB of which 3.25 MiB is free. Process 4066995 has 67.03 GiB memory in use. Including non-PyTorch memory, this process has 12.06 GiB memory in use. Of the allocated memory 11.32 GiB is allocated by PyTorch, and 243.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m clip_emb_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1664\u001b[39m\n\u001b[1;32m     11\u001b[0m clip_seq_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m---> 13\u001b[0m clip_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mSC_Reconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedder_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m retrieval_embedding_variant \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstable_cascade_hidden\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m retrieval_emb_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n",
      "File \u001b[0;32m/weka/proj-fmri/jonxu/MindEye_Imagery/src/sc_reconstructor.py:33\u001b[0m, in \u001b[0;36mSC_Reconstructor.__init__\u001b[0;34m(self, device, cache_dir, embedder_only, compile_models)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# SETUP MODELS & DATA\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextras \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39msetup_extras_pre()\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextras\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_models:\n",
      "File \u001b[0;32m/weka/proj-fmri/jonxu/MindEye_Imagery/src/StableCascade/train/train_c.py:165\u001b[0m, in \u001b[0;36mWurstCore.setup_models\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m param_name, param \u001b[38;5;129;01min\u001b[39;00m load_or_fail(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgenerator_checkpoint_path)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    164\u001b[0m             set_module_tensor_to_device(generator, param_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mparam)\n\u001b[0;32m--> 165\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(generator, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generator_ema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 79.11 GiB of which 3.25 MiB is free. Process 4066995 has 67.03 GiB memory in use. Including non-PyTorch memory, this process has 12.06 GiB memory in use. Of the allocated memory 11.32 GiB is allocated by PyTorch, and 243.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder # bigG embedder\n",
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "image_embedding_variant = \"ViT-bigG-14\"\n",
    "clip_emb_dim = 1664\n",
    "clip_seq_dim = 256\n",
    "\n",
    "clip_extractor = SC_Reconstructor(compile_models=False, embedder_only=True, device=device, cache_dir=cache_dir)\n",
    "retrieval_embedding_variant = \"stable_cascade_hidden\"\n",
    "retrieval_emb_dim = 1024\n",
    "retrieval_seq_dim = 257\n",
    "\n",
    "prompt_embedding_variant = \"git\"\n",
    "git_seq_dim = 257\n",
    "git_emb_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2e145",
   "metadata": {},
   "source": [
    "# Creating block of CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb00346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Retrieval embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images...:   0%|                                                                         | 0/540 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clip_extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m train_images[i \u001b[38;5;241m*\u001b[39m emb_batch_size:i \u001b[38;5;241m*\u001b[39m emb_batch_size \u001b[38;5;241m+\u001b[39m emb_batch_size]:\n\u001b[1;32m     27\u001b[0m         batch_list\u001b[38;5;241m.\u001b[39mappend(transforms\u001b[38;5;241m.\u001b[39mToPILImage()(img))\n\u001b[0;32m---> 28\u001b[0m     retrieval_image_train[i \u001b[38;5;241m*\u001b[39m emb_batch_size:i \u001b[38;5;241m*\u001b[39m emb_batch_size \u001b[38;5;241m+\u001b[39m emb_batch_size] \u001b[38;5;241m=\u001b[39m \u001b[43mclip_extractor\u001b[49m\u001b[38;5;241m.\u001b[39membed_image(batch_list, hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Normalize for optimal cosine similarity\u001b[39;00m\n\u001b[1;32m     30\u001b[0m retrieval_image_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(retrieval_image_train, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_extractor' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = f\"{data_path}/preprocessed_data/subject{subj}/{image_embedding_variant}_image_embeddings_train.pt\"\n",
    "emb_batch_size = 50\n",
    "if not os.path.exists(file_path):\n",
    "    # Generate CLIP Image embeddings\n",
    "    print(\"Generating Image embeddings!\")\n",
    "    clip_image_train = torch.zeros((len(train_images), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    for i in tqdm(range(len(train_images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "        batch = train_images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "        batch = batch.to(device).to(dtype=torch.float16)\n",
    "        embedding = clip_img_embedder(batch).to(\"cpu\")\n",
    "        clip_image_train[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = embedding\n",
    "\n",
    "    torch.save(clip_image_train, file_path)\n",
    "else:\n",
    "    clip_image_train = torch.load(file_path)\n",
    "\n",
    "if retrieval:\n",
    "    file_path = f\"{data_path}/preprocessed_data/subject{subj}/{retrieval_embedding_variant}_retrieval_embeddings_train.pt\"\n",
    "    emb_batch_size = 50\n",
    "    if not os.path.exists(file_path):\n",
    "        # Generate CLIP Retrieval embeddings\n",
    "        print(\"Generating Retrieval embeddings!\")\n",
    "        retrieval_image_train = torch.zeros((len(train_images), retrieval_seq_dim, retrieval_emb_dim)).to(\"cpu\")\n",
    "        for i in tqdm(range(len(train_images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "            batch_list = []\n",
    "            for img in train_images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]:\n",
    "                batch_list.append(transforms.ToPILImage()(img))\n",
    "            retrieval_image_train[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = clip_extractor.embed_image(batch_list, hidden=True).to(\"cpu\")\n",
    "        # Normalize for optimal cosine similarity\n",
    "        retrieval_image_train = torch.nn.functional.normalize(retrieval_image_train, p=2, dim=2)\n",
    "        torch.save(retrieval_image_train, file_path)\n",
    "    else:\n",
    "        retrieval_image_train = torch.load(file_path)\n",
    "        \n",
    "# Load 73k GiT NSD features\n",
    "if prompt_recon:\n",
    "    file_path_git = f\"{data_path}/preprocessed_data/subject{subj}/{prompt_embedding_variant}_prompt_embeddings_train.pt\"\n",
    "    if not os.path.exists(file_path_git):\n",
    "        with h5py.File(f'{data_path}/git_image_features.hdf5', 'r') as f:\n",
    "            git_features = f['features'][:]\n",
    "        train_git_images = torch.zeros((len(valid_nsd_ids_train), 257,1024))\n",
    "        for i, idx in enumerate(valid_nsd_ids_train):\n",
    "            train_git_images[i] = torch.from_numpy(git_features[idx])\n",
    "        torch.save(train_git_images, file_path_git)\n",
    "        del git_features\n",
    "    else:\n",
    "        train_git_images = torch.load(file_path_git)\n",
    "\n",
    "# Cut down vectors to only samples used for training based on num_sessions, this assumed scanIDs are in order:\n",
    "clip_image_train = clip_image_train[:len(train_images)]\n",
    "if retrieval:\n",
    "    retrieval_image_train = retrieval_image_train[:len(train_images)]\n",
    "if prompt_recon:\n",
    "    train_git_images = train_git_images[:len(train_images)]\n",
    "    \n",
    "print(f\"Loaded vectors for subj{subj}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991d756",
   "metadata": {},
   "source": [
    "# Train Ridge regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65570b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ridge_weights = np.zeros((clip_seq_dim * clip_emb_dim, x_train.shape[-1])).astype(np.float32)\n",
    "ridge_biases = np.zeros((clip_seq_dim * clip_emb_dim)).astype(np.float32)\n",
    "print(f\"Training Ridge Image model with alpha=100000\")\n",
    "model = Ridge(\n",
    "    alpha=100000,\n",
    "    max_iter=max_iter,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "param_count = ridge_weights.size + ridge_biases.size  # + n_outputs if intercept is included\n",
    "print(f\"Estimated parameter count before fitting: {param_count}\")\n",
    "\n",
    "model.fit(x_train, clip_image_train.reshape(len(clip_image_train), -1))\n",
    "ridge_weights = model.coef_\n",
    "ridge_biases = model.intercept_\n",
    "datadict = {\"coef\" : ridge_weights, \"intercept\" : ridge_biases}\n",
    "# Save the regression weights\n",
    "with open(f'{outdir}/ridge_image_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(datadict, f)\n",
    "\n",
    "if retrieval:\n",
    "    ridge_weights = np.zeros((retrieval_seq_dim * retrieval_emb_dim, x_train.shape[-1])).astype(np.float32)\n",
    "    ridge_biases = np.zeros((retrieval_seq_dim * retrieval_emb_dim)).astype(np.float32)\n",
    "    print(f\"Training Ridge Retrieval model with alpha={weight_decay}\")\n",
    "    model = Ridge(\n",
    "        alpha=weight_decay,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42,\n",
    "    )\n",
    "    x_train_norm = torch.nn.functional.normalize(x_train, p=2, dim=1)\n",
    "    model.fit(x_train_norm, retrieval_image_train.reshape(len(retrieval_image_train), -1))\n",
    "    ridge_weights = model.coef_\n",
    "    ridge_biases = model.intercept_\n",
    "    datadict = {\"coef\" : ridge_weights, \"intercept\" : ridge_biases}\n",
    "    # Save the regression weights\n",
    "    with open(f'{outdir}/ridge_retrieval_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(datadict, f)\n",
    "    \n",
    "    del retrieval_image_train\n",
    "    del ridge_weights\n",
    "    del ridge_biases\n",
    "    del datadict\n",
    "        \n",
    "if prompt_recon:\n",
    "    ridge_weights_prompt = np.zeros((git_seq_dim*git_emb_dim, x_train.shape[-1])).astype(np.float32)\n",
    "    ridge_biases_prompt = np.zeros((git_seq_dim*git_emb_dim)).astype(np.float32)\n",
    "    print(f\"Training Ridge prompt recon model with alpha={weight_decay}\")\n",
    "    model = Ridge(\n",
    "        alpha=weight_decay,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(x_train, train_git_images.reshape(len(train_git_images), -1))\n",
    "    ridge_weights_prompt = model.coef_\n",
    "    ridge_biases_prompt = model.intercept_\n",
    "    datadict = {\"coef\" : ridge_weights_prompt, \"intercept\" : ridge_biases_prompt}\n",
    "    # Save the regression weights\n",
    "    with open(f'{outdir}/ridge_prompt_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(datadict, f)\n",
    "        \n",
    "    del train_git_images\n",
    "    del ridge_weights_prompt\n",
    "    del ridge_biases_prompt\n",
    "    del datadict\n",
    "\n",
    "print(f\"Elapsed training time for {model_name}: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc23ec0-3e69-4da3-8caa-c22b106a4018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "12bcf72e37cf41bd9375f06ed56c1e97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a0adf6afac444a83aeb275a380ebc60f",
       "style": "IPY_MODEL_9b2845d8a3f0485f924840ad8dd41235",
       "value": " 2/2 [00:13&lt;00:00,  5.81s/it]"
      }
     },
     "1f36be16a8e74ca3a3461cf91ec5fe27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "50543afe618b468481889d8a7621a97f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_da431fdaf3b149ca95fae003e453103b",
        "IPY_MODEL_e0c099ffd0b6461ab682f743406572a9",
        "IPY_MODEL_12bcf72e37cf41bd9375f06ed56c1e97"
       ],
       "layout": "IPY_MODEL_dc145c70be2c4acabbbb683e3b790902"
      }
     },
     "516c3d23d7134a95abb29d1a5e4276ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "87449040fee04db7a648228c6dbb3c50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9b2845d8a3f0485f924840ad8dd41235": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a0adf6afac444a83aeb275a380ebc60f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "da431fdaf3b149ca95fae003e453103b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1f36be16a8e74ca3a3461cf91ec5fe27",
       "style": "IPY_MODEL_516c3d23d7134a95abb29d1a5e4276ea",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "dc145c70be2c4acabbbb683e3b790902": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e0c099ffd0b6461ab682f743406572a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_e21a239290794a31a46609c978f69369",
       "max": 2,
       "style": "IPY_MODEL_87449040fee04db7a648228c6dbb3c50",
       "value": 2
      }
     },
     "e21a239290794a31a46609c978f69369": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
