{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from versatile_diffusion import Reconstructor\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from sklearn.linear_model import Ridge\n",
    "import pickle\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd260e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 8\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]\n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588554b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"testing\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path=../dataset/ \\\n",
    "                    --cache_dir=../cache/ \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --batch_size=64 \\\n",
    "                    --no-multi_subject --subj=1 --num_sessions=40 \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --no-blurry_recon --blur_scale=.5  \\\n",
    "                    --seq_past=0 --seq_future=0 \\\n",
    "                    --no-use_prior --prior_scale=30 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=150 --no-use_image_aug \\\n",
    "                    --ckpt_interval=1 --ckpt_saving\"\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=1,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires much more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",type=float,default=60000,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_imageryrf\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the ImageryRF dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_nsd\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Don't use the Natural Scenes Dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr_threshold\",type=float,default=-1.0,\n",
    "    help=\"Used for calculating SNR on a whole brain to narrow down voxels.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"all\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    if train_imageryrf:\n",
    "            # 9,10,11 is ImageryRF subjects\n",
    "        if no_nsd:\n",
    "            subj_list = np.arange(9,12)\n",
    "        else:\n",
    "            subj_list = np.arange(1,12)\n",
    "    else:\n",
    "        subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4743c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = utils.create_snr_betas(subject=subj, data_type=torch.float16, data_path=data_path, threshold = snr_threshold)\n",
    "x_train, valid_nsd_ids_train, x_test, test_nsd_ids = utils.load_nsd(subject=subj, betas=betas, data_path=data_path)\n",
    "print(x_train.shape, valid_nsd_ids_train.shape)\n",
    "\n",
    "print(f\"Loaded subj {subj} betas!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)\n",
    "\n",
    "# Load 73k NSD captions\n",
    "captions = np.load(f'{data_path}/preprocessed_data/annots_73k.npy')\n",
    "print(\"Loaded all 73k NSD captions to cpu!\", captions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168051b",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f46cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    # Create a mapping from the old layer names to the new layer names\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a17b31",
   "metadata": {},
   "source": [
    "### VD/CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_emb_dim = 768\n",
    "clip_text_emb_dim = 768\n",
    "clip_seq_dim = 257\n",
    "clip_text_seq_dim=77\n",
    "clip_extractor = Reconstructor(device=device, cache_dir=cache_dir)\n",
    "clip_variant = \"ViT-L-14\"\n",
    "latent_dim = 3136\n",
    "latent_variant = \"autoenc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2e145",
   "metadata": {},
   "source": [
    "# Creating block of CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb00346",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{data_path}/preprocessed_data/{image_embedding_variant}_image_embeddings.pt\"\n",
    "emb_batch_size = 50\n",
    "if not os.path.exists(file_path):\n",
    "    # Generate CLIP Image embeddings\n",
    "    print(\"Generating CLIP Image embeddings!\")\n",
    "    clip_image = torch.zeros((len(images), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    for i in tqdm(range(len(images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "        batch_images = images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "        batch_embeddings = clip_extractor.embed_image(torch.from_numpy(batch_images)).detach().to(\"cpu\")\n",
    "        clip_image[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = batch_embeddings\n",
    "\n",
    "    torch.save(clip_image, file_path)\n",
    "else:\n",
    "    clip_image = torch.load(file_path)\n",
    "clip_image_train = torch.zeros((len(valid_nsd_ids_train), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "for i, idx in enumerate(valid_nsd_ids_train):\n",
    "    clip_image_train[i] = clip_image[idx]\n",
    "file_path = f\"{data_path}/preprocessed_data/subject{subj}/{image_embedding_variant}_image_embeddings_train.pt\"\n",
    "if not os.path.exists(file_path):\n",
    "    torch.save(clip_image_train, file_path)\n",
    "        \n",
    "if dual_guidance:\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/{text_embedding_variant}_text_embeddings.pt\"\n",
    "    if not os.path.exists(file_path_txt):\n",
    "        # Generate CLIP Text embeddings\n",
    "        print(\"Generating CLIP Text embeddings!\")\n",
    "        clip_text = torch.zeros((len(captions), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "        for i in tqdm(range(len(captions) // emb_batch_size), desc=\"Encoding captions...\"):\n",
    "            batch_captions = captions[i * emb_batch_size:i * emb_batch_size + emb_batch_size].tolist()\n",
    "            clip_text[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = clip_extractor.embed_text(batch_captions).detach().to(\"cpu\")\n",
    "        torch.save(clip_text, file_path_txt)\n",
    "    else:\n",
    "        clip_text = torch.load(file_path_txt)\n",
    "    clip_text_train = torch.zeros((len(valid_nsd_ids_train), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "    for i, idx in enumerate(valid_nsd_ids_train):\n",
    "        clip_text_train[i] = clip_text[idx]\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{text_embedding_variant}_text_embeddings_train.pt\"\n",
    "    if not os.path.exists(file_path_txt):\n",
    "        torch.save(clip_text_train, file_path_txt)\n",
    "        \n",
    "\n",
    "if blurry_recon:\n",
    "    file_path = f\"{data_path}/preprocessed_data/{latent_variant}_image_embeddings.pt\"\n",
    "    if not os.path.exists(file_path):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating VAE Image embeddings!\")\n",
    "        vae_image = torch.zeros((len(images), latent_dim)).to(\"cpu\")\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "\n",
    "            for i in tqdm(range(len(images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "                batch_images = images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "                batch_images = 2 * torch.from_numpy(batch_images).unsqueeze(0).detach().to(device=device, dtype=torch.float16) - 1\n",
    "                batch_embeddings = (autoenc.encode(batch_images).latent_dist.mode() * 0.18215).detach().to(\"cpu\").reshape(emb_batch_size, -1)\n",
    "                vae_image[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = batch_embeddings\n",
    "\n",
    "\n",
    "    else:\n",
    "        vae_image = torch.load(file_path)\n",
    "    vae_image_train = torch.zeros((len(valid_nsd_ids_train), latent_dim)).to(\"cpu\")\n",
    "    for i, idx in enumerate(valid_nsd_ids_train):\n",
    "        vae_image_train[i] = vae_image[idx]\n",
    "    file_path_vae = f\"{data_path}/preprocessed_data/subject{subj}/{latent_variant}_image_embeddings_train.pt\"\n",
    "    if not os.path.exists(file_path_vae):\n",
    "        torch.save(vae_image_train, file_path_vae)\n",
    "print(f\"Loaded train image clip and text clip for subj{subj}!\", clip_image_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991d756",
   "metadata": {},
   "source": [
    "# Train Ridge regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65570b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ridge_weights = np.zeros((clip_seq_dim * clip_emb_dim, x_train.shape[-1])).astype(np.float32)\n",
    "ridge_biases = np.zeros((clip_seq_dim * clip_emb_dim)).astype(np.float32)\n",
    "print(f\"Training Ridge CLIP Image model with alpha={weight_decay}\")\n",
    "model = Ridge(\n",
    "    alpha=weight_decay,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.fit(x_train, clip_image_train.reshape(len(clip_image_train), -1))\n",
    "ridge_weights = model.coef_\n",
    "ridge_biases = model.intercept_\n",
    "datadict = {\"coef\" : ridge_weights, \"intercept\" : ridge_biases}\n",
    "# Save the regression weights\n",
    "with open(f'{outdir}/ridge_image_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(datadict, f)\n",
    "    \n",
    "if dual_guidance:\n",
    "    ridge_weights_txt = np.zeros((clip_text_seq_dim * clip_emb_dim, x_train.shape[-1])).astype(np.float32)\n",
    "    ridge_biases_txt = np.zeros((clip_text_seq_dim * clip_emb_dim)).astype(np.float32)\n",
    "    print(f\"Training Ridge CLIP Text model with alpha={weight_decay}\")\n",
    "    model = Ridge(\n",
    "        alpha=weight_decay,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, clip_text_train.reshape(len(clip_text_train), -1))\n",
    "    ridge_weights_txt = model.coef_\n",
    "    ridge_biases_txt = model.intercept_\n",
    "    datadict = {\"coef\" : ridge_weights_txt, \"intercept\" : ridge_biases_txt}\n",
    "    # Save the regression weights\n",
    "    with open(f'{outdir}/ridge_text_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(datadict, f)\n",
    "            \n",
    "if blurry_recon:\n",
    "    ridge_weights_blurry = np.zeros((latent_dim,x_train.shape[-1])).astype(np.float32)\n",
    "    ridge_biases_blurry = np.zeros((latent_dim,)).astype(np.float32)\n",
    "    print(f\"Training Ridge Blurry recon model with alpha={weight_decay}\")\n",
    "    model = Ridge(\n",
    "        alpha=weight_decay,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(x_train, vae_image_train)\n",
    "    ridge_weights_blurry = model.coef_\n",
    "    ridge_biases_blurry = model.intercept_\n",
    "    datadict = {\"coef\" : ridge_weights_blurry, \"intercept\" : ridge_biases_blurry}\n",
    "    # Save the regression weights\n",
    "    with open(f'{outdir}/ridge_blurry_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(datadict, f)\n",
    "\n",
    "\n",
    "print(f\"Elapsed training time for {model_name}: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
