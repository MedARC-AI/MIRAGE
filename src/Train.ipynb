{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from versatile_diffusion import Reconstructor\n",
    "from sklearn.linear_model import Ridge\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 8\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]\n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588554b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"testing_txt_combined\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --cache_dir=/weka/proj-medarc/shared/cache \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} --num_sessions=3 \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --no-blurry_recon --blur_scale=.5  \\\n",
    "                    --seq_past=0 --seq_future=0 \\\n",
    "                    --use_prior --prior_scale=30 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=150 --no-use_image_aug \\\n",
    "                    --ckpt_interval=1 --ckpt_saving\"\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=1,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires much more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_imageryrf\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the ImageryRF dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_nsd\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Don't use the Natural Scenes Dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr_threshold\",type=float,default=-1.0,\n",
    "    help=\"Used for calculating SNR on a whole brain to narrow down voxels.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",type=float,default=1e-2,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"all\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    if train_imageryrf:\n",
    "            # 9,10,11 is ImageryRF subjects\n",
    "        if no_nsd:\n",
    "            subj_list = np.arange(9,12)\n",
    "        else:\n",
    "            subj_list = np.arange(1,12)\n",
    "    else:\n",
    "        subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4743c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3964338",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    # If an NSD subject\n",
    "    if s < 9:\n",
    "        if multi_subject:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "        else:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "        print(train_url)\n",
    "        \n",
    "        train_data[f'subj{s:02d}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                            .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                            .decode(\"torch\")\\\n",
    "                            .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                            .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "        betas = utils.create_snr_betas(subject=s, data_type=data_type, data_path=data_path, threshold = snr_threshold)\n",
    "        x_train, valid_nsd_ids_train, _, _ = utils.load_nsd(subject=s, betas=betas, data_path=data_path)\n",
    "        print(betas.shape)\n",
    "        num_voxels_list.append(betas[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        voxels[f'subj{s:02d}'] = betas\n",
    "    elif s < 12:\n",
    "        train_url = \"\"\n",
    "        test_url = \"\"\n",
    "        betas, images, _, _ = utils.load_imageryrf(subject=int(s-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=False, split=True)\n",
    "        betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "        betas = betas.to(\"cpu\").to(data_type)\n",
    "        num_voxels_list.append(betas[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        num_nan_values = torch.sum(torch.isnan(betas))\n",
    "        print(\"Number of NaN values in betas:\", num_nan_values.item())\n",
    "        indices = torch.randperm(len(betas))\n",
    "        shuffled_betas = betas[indices]\n",
    "        shuffled_images = images[indices]\n",
    "        train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        \n",
    "        \n",
    "    # elif s < 15:\n",
    "    #     betas, images = utils.load_imageryrf(subject=int(s-11), mode=\"imagery\", mask=True, stimtype=\"object\", average=False, nest=False)\n",
    "    #     betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    #     betas = betas.to(\"cpu\").to(data_type)\n",
    "    #     num_voxels_list.append(betas[0].shape[-1])\n",
    "    #     num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        \n",
    "    #     indices = torch.randperm(len(betas))\n",
    "    #     shuffled_betas = betas[indices]\n",
    "    #     shuffled_images = images[indices]\n",
    "    #     train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "    #     train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    print(f\"num_voxels for subj{s:02d}: {num_voxels[f'subj{s:02d}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject (doesn't support ImageryRF)\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "if subj < 9:\n",
    "    test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "else:\n",
    "    _, _, betas, images = utils.load_imageryrf(subject=int(subj-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "    num_test = len(betas)\n",
    "    betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    betas = betas.to(\"cpu\").to(data_type)\n",
    "    num_nan_values = torch.sum(torch.isnan(betas))\n",
    "    print(\"Number of NaN values in test betas:\", num_nan_values.item())\n",
    "    test_data = torch.utils.data.TensorDataset(betas, images)\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")\n",
    "\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)\n",
    "# Load 73k NSD captions\n",
    "captions = np.load(f'{data_path}/preprocessed_data/annots_73k.npy')\n",
    "print(\"Loaded all 73k NSD captions to cpu!\", captions.shape)\n",
    "images_train = torch.zeros((len(valid_nsd_ids_train), 3, 224, 224))\n",
    "text_train = np.zeros((len(valid_nsd_ids_train),), dtype=object)\n",
    "for i, idx in enumerate(valid_nsd_ids_train):\n",
    "    images_train[i] =  torch.from_numpy(images[idx])\n",
    "    text_train[i] = captions[idx]\n",
    "print(f\"Loaded train images and captions for subj{subj}!\", images_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168051b",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a17b31",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_emb_dim = 768\n",
    "clip_seq_dim = 257\n",
    "clip_text_seq_dim=77\n",
    "clip_extractor = Reconstructor(device=device, cache_dir=cache_dir)\n",
    "clip_variant = \"ViT-L-14\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b7031",
   "metadata": {},
   "source": [
    "### Precompute all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{data_path}/preprocessed_data/subject{subj}/{clip_variant}_image_embeddings_train.pt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # Generate CLIP Image embeddings\n",
    "    print(\"Generating CLIP Image embeddings!\")\n",
    "    clip_image_train = torch.zeros((len(images_train), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    for i in tqdm(range(0, len(images_train)), desc=\"Encoding images...\"):\n",
    "        clip_image_train[i] = clip_extractor.embed_image(images_train[i].unsqueeze(0)).detach().to(\"cpu\")\n",
    "    torch.save(clip_image_train, file_path)\n",
    "else:\n",
    "    clip_image_train = torch.load(file_path)\n",
    "    \n",
    "if dual_guidance:\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{clip_variant}_text_embeddings_train.pt\"\n",
    "    if not os.path.exists(file_path_txt):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating CLIP Text embeddings!\")\n",
    "        clip_text_train = torch.zeros((len(text_train), clip_text_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "        for i in tqdm(range(0, len(text_train)), desc=\"Encoding captions...\"):\n",
    "            clip_text_train[i] = clip_extractor.embed_text(text_train[i]).detach().to(\"cpu\")\n",
    "        torch.save(clip_text_train, file_path_txt)\n",
    "    else:\n",
    "        clip_text_train = torch.load(file_path_txt)\n",
    "\n",
    "if blurry_recon:\n",
    "    file_path = f\"{data_path}/preprocessed_data/subject{subj}/autoenc_image_embeddings_train.pt\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating VAE Image embeddings!\")\n",
    "        vae_image_train = torch.zeros((len(images_train), 3136)).to(\"cpu\")\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            for i in tqdm(range(0, len(images_train)), desc=\"Encoding images...\"):\n",
    "                vae_image_train[i] = (autoenc.encode(2*images_train[i].unsqueeze(0).detach().to(device=device, dtype=torch.float16)-1).latent_dist.mode() * 0.18215).detach().to(\"cpu\").flatten()\n",
    "            torch.save(vae_image_train, file_path)\n",
    "    else:\n",
    "        vae_image_train = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2fee8",
   "metadata": {},
   "source": [
    "# Train Ridge regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_weight_decay = 60000\n",
    "start = time.time()\n",
    "clip_image_model = Ridge(\n",
    "        alpha=ridge_weight_decay,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "model_path = f'{outdir}/ridge_image_weights.pkl'\n",
    "if not os.path.exists(model_path):\n",
    "    ridge_weights = np.zeros((clip_seq_dim * clip_emb_dim, num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "    ridge_biases = np.zeros((clip_seq_dim * clip_emb_dim)).astype(np.float32)\n",
    "    print(f\"Training Ridge CLIP Image model with alpha={ridge_weight_decay}\")\n",
    "    clip_image_model.fit(x_train, clip_image_train.reshape(len(clip_image_train), -1))\n",
    "    ridge_weights = clip_image_model.coef_\n",
    "    ridge_biases = clip_image_model.intercept_\n",
    "    image_datadict = {\"coef\" : ridge_weights, \"intercept\" : ridge_biases}\n",
    "    # Save the regression weights\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(image_datadict, f)\n",
    "else:\n",
    "    with open(model_path, 'rb') as f:\n",
    "        image_datadict = pickle.load(f)\n",
    "    clip_image_model.coef_ = image_datadict[\"coef\"]\n",
    "    clip_image_model.intercept_ = image_datadict[\"intercept\"]\n",
    "    \n",
    "if dual_guidance:\n",
    "    clip_text_model = Ridge(\n",
    "            alpha=ridge_weight_decay,\n",
    "            max_iter=50000,\n",
    "            random_state=42,\n",
    "        )\n",
    "    model_path = f'{outdir}/ridge_text_weights.pkl'\n",
    "    if not os.path.exists(model_path):\n",
    "        ridge_weights_txt = np.zeros((clip_text_seq_dim * clip_emb_dim, num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "        ridge_biases_txt = np.zeros((clip_text_seq_dim * clip_emb_dim)).astype(np.float32)\n",
    "        print(f\"Training Ridge CLIP Text model with alpha={ridge_weight_decay}\")\n",
    "\n",
    "        clip_text_model.fit(x_train, clip_text_train.reshape(len(clip_text_train), -1))\n",
    "        ridge_weights_txt = clip_text_model.coef_\n",
    "        ridge_biases_txt = clip_text_model.intercept_\n",
    "        text_datadict = {\"coef\" : ridge_weights_txt, \"intercept\" : ridge_biases_txt}\n",
    "        # Save the regression weights\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(text_datadict, f)\n",
    "    else:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            text_datadict = pickle.load(f)\n",
    "        clip_text_model.coef_ = text_datadict[\"coef\"]\n",
    "        clip_text_model.intercept_ = text_datadict[\"intercept\"]\n",
    "            \n",
    "if blurry_recon:\n",
    "    vae_image_model = Ridge(\n",
    "            alpha=ridge_weight_decay,\n",
    "            max_iter=50000,\n",
    "            random_state=42,\n",
    "        )\n",
    "    model_path = f'{outdir}/ridge_blurry_weights.pkl'\n",
    "    if not os.path.exists(model_path):\n",
    "        ridge_weights_blurry = np.zeros((3136,num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "        ridge_biases_blurry = np.zeros((3136,)).astype(np.float32)\n",
    "        print(f\"Training Ridge Blurry recon model with alpha={ridge_weight_decay}\")\n",
    "        \n",
    "        vae_image_model.fit(x_train, vae_image_train)\n",
    "        ridge_weights_blurry = vae_image_model.coef_\n",
    "        ridge_biases_blurry = vae_image_model.intercept_\n",
    "        blurry_datadict = {\"coef\" : ridge_weights_blurry, \"intercept\" : ridge_biases_blurry}\n",
    "        # Save the regression weights\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(blurry_datadict, f)\n",
    "    else:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            blurry_datadict = pickle.load(f)\n",
    "        vae_image_model.coef_ = blurry_datadict[\"coef\"]\n",
    "        vae_image_model.intercept_ = blurry_datadict[\"intercept\"]\n",
    "\n",
    "print(f\"{model_name} model trained/loaded in {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\")\n",
    "# If we arent going to train the diffusion prior, stop here:\n",
    "if not use_prior:\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a6f17",
   "metadata": {},
   "source": [
    "### Adding diffusion prior if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 64\n",
    "    heads = clip_emb_dim//64 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    if dual_guidance:\n",
    "        prior_network_txt = PriorNetwork(\n",
    "                dim=out_dim,\n",
    "                depth=depth,\n",
    "                dim_head=dim_head,\n",
    "                heads=heads,\n",
    "                causal=False,\n",
    "                num_tokens = clip_text_seq_dim,\n",
    "                learned_query_mode=\"pos_emb\"\n",
    "            )\n",
    "    \n",
    "\n",
    "        model.diffusion_prior_txt = BrainDiffusionPrior(\n",
    "            net=prior_network_txt,\n",
    "            image_embed_dim=out_dim,\n",
    "            condition_on_text_encodings=False,\n",
    "            timesteps=timesteps,\n",
    "            cond_drop_prob=0.2,\n",
    "            image_embed_scale=None,\n",
    "        )\n",
    "        utils.count_params(model.diffusion_prior_txt)\n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    if dual_guidance:\n",
    "        opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior_txt.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.diffusion_prior_txt.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ])\n",
    "        \n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(f'{outdir}/{tag}.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeye2'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "      \"train_imageryrf\": train_imageryrf,\n",
    "      \"mode\": mode,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=None,\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453c316-0cb0-4bee-8585-f44dff746e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved ckpt model weights into current model\n",
    "if resume_from_ckpt:\n",
    "    load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "# elif wandb_log:\n",
    "#     if wandb.run.resumed:\n",
    "#         load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj{s:02d}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "test_caption = []\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    recon_cossim_txt = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    test_recon_cossim_txt = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    loss_prior_total_txt = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "    test_loss_prior_total_txt = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    caption_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    # print(f\"num_iterations_per_epoch: {num_iterations_per_epoch}, batch_size: {batch_size}, len(subj_list): {len(subj_list)}\")\n",
    "    for s, (cur_subj, train_dl) in enumerate(zip(subj_list, train_dls)):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for i, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):\n",
    "                image_idx = behav0[:,0,0].cpu().long().numpy()\n",
    "                local_idx, image_sorted_idx = np.unique(image_idx, return_index=True)                \n",
    "                # if len(image0) != len(image_idx): # hdf5 cant handle duplicate indexing\n",
    "                #     continue\n",
    "                image0 = torch.tensor(images[local_idx], dtype=data_type)\n",
    "                image_iters[i, s*batch_size:s*batch_size+image0.shape[0]] = image0\n",
    "                caption0 = captions[local_idx]\n",
    "                caption_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = caption0\n",
    "                \n",
    "                voxel_idx = behav0[:,0,5].cpu().long().numpy()\n",
    "                voxel_sorted_idx = voxel_idx[image_sorted_idx]\n",
    "                voxel0 = voxels[f'subj0{subj_list[s]}'][voxel_sorted_idx]\n",
    "                voxel0 = voxel0\n",
    "                \n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = perm\n",
    "                    betas_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = betas\n",
    "                    select_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = voxel0\n",
    "                if i >= num_iterations_per_epoch-1:\n",
    "                    break\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj{s:02d}_iter{train_i}\"] for s in subj_list]\n",
    "            # print(f\"voxel_list {voxel_list}\")\n",
    "            image = image_iters[train_i].detach()\n",
    "            caption = np.concatenate([caption_iters[f\"subj{s:02d}_iter{train_i}\"] for s in subj_list])\n",
    "            image = image[0:len(caption)].to(device)   # fix the last batch problem\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "            if dual_guidance:\n",
    "                clip_target_txt = clip_extractor.embed_text(caption.tolist()).float()\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj{s:02d}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj{s:02d}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj{s:02d}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            # voxel_ridge_list = [model.ridge(voxel_list[si].detach().to(device), si) for si, s in enumerate(subj_list)]\n",
    "            # voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            # backbone, backbone_txt, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "            \n",
    "            backbone=torch.cat([torch.from_numpy(clip_image_model.predict(voxel_list[si]).reshape(-1, clip_seq_dim, clip_emb_dim)) for si, s in enumerate(subj_list)],dim=0).to(device, data_type)\n",
    "            clip_voxels = backbone\n",
    "            backbone_txt=torch.cat([torch.from_numpy(clip_text_model.predict(voxel_list[si]).reshape(-1, clip_text_seq_dim, clip_emb_dim)) for si, s in enumerate(subj_list)],dim=0).to(device, data_type)\n",
    "            image_enc_pred = torch.cat([torch.from_numpy(vae_image_model.predict(voxel_list[si]).reshape(-1, 4, 28, 28)) for si, s in enumerate(subj_list)],dim=0).to(device, data_type)\n",
    "            # print(f\"Ridge output shapes: {backbone.shape}, {backbone_txt.shape}, {clip_voxels.shape}, {image_enc_pred.shape}\")\n",
    "\n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                # print(f\"loss prior {loss_prior.item()}\")\n",
    "                loss += loss_prior\n",
    "                # for txt\n",
    "                if dual_guidance:\n",
    "                    loss_prior_txt, prior_out_txt = model.diffusion_prior_txt(text_embed=backbone_txt, image_embed=clip_target_txt)\n",
    "                    loss_prior_total_txt += loss_prior_txt.item()\n",
    "                    loss_prior_txt *= prior_scale\n",
    "                \n",
    "                # print(f\"loss prior {loss_prior.item()}\")\n",
    "                    loss += loss_prior_txt\n",
    "                    recon_cossim_txt += nn.functional.cosine_similarity(prior_out_txt, clip_target_txt).mean().item()\n",
    "                \n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            caption = []\n",
    "            for test_i, data in enumerate(test_dl):  \n",
    "                if subj < 9:\n",
    "                    behav, past_behav, future_behav, old_behav = data\n",
    "                    # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                    assert len(behav) == num_test\n",
    "\n",
    "                    ## Average same-image repeats ##\n",
    "                    if test_image is None:\n",
    "                        voxel = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "                        \n",
    "                        if seq_len==1:\n",
    "                            voxel = voxel.unsqueeze(1)\n",
    "                        else:\n",
    "                            if seq_past>0:\n",
    "                                past_behavior = past_behav[:,:(seq_past),5].cpu().long()\n",
    "                                past_voxels = voxels[f'subj0{subj}'][past_behavior]\n",
    "                                if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                    past_voxels[torch.where(past_behavior==-1)[0]] = 0\n",
    "\n",
    "                            if seq_future>0:\n",
    "                                future_behavior = future_behav[:,:(seq_future),5].cpu().long()\n",
    "                                future_voxels = voxels[f'subj0{subj}'][future_behavior]                    \n",
    "                                if torch.any(future_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                    future_voxels[torch.where(future_behavior==-1)[0]] = 0\n",
    "                                \n",
    "                            if seq_past > 0 and seq_future > 0:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                                voxel = torch.cat((voxel, future_voxels), axis=1)\n",
    "                            elif seq_past > 0:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                            else:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), future_voxels), axis=1)\n",
    "\n",
    "                        image = behav[:,0,0].cpu().long()\n",
    "\n",
    "                        unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                        for im in unique_image:\n",
    "                            locs = torch.where(im == image)[0]\n",
    "                            if len(locs)==1:\n",
    "                                locs = locs.repeat(3)\n",
    "                            elif len(locs)==2:\n",
    "                                locs = locs.repeat(2)[:3]\n",
    "                            assert len(locs)==3\n",
    "                            if test_image is None:\n",
    "                                test_image = torch.tensor(images[im][None], dtype=torch.float16, device=\"cpu\")\n",
    "                                if dual_guidance:\n",
    "                                    test_caption.append(captions[im])\n",
    "                                test_voxel = voxel[locs][None]\n",
    "                            else:\n",
    "                                test_image = torch.vstack((test_image, torch.tensor(images[im][None], dtype=torch.float16, device=\"cpu\")))\n",
    "                                if dual_guidance:\n",
    "                                    test_caption.append(captions[im])\n",
    "                                test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "\n",
    "                    loss=0.\n",
    "                                \n",
    "                    test_indices = torch.arange(len(test_voxel))[:300]\n",
    "                    # print(test_image.shape,len(test_caption),test_indices.max())\n",
    "                    voxel = test_voxel[test_indices].to(device)\n",
    "                    image = test_image[test_indices].to(device)\n",
    "                    if dual_guidance:\n",
    "                        for idx in test_indices:\n",
    "                            # if isinstance(idx, torch.Tensor):\n",
    "                            #     idx = idx.item()\n",
    "                            caption.append(test_caption[idx])\n",
    "                        assert len(caption) == 300\n",
    "                    assert len(image) == 300\n",
    "                else:\n",
    "                    voxel, image = data\n",
    "                    voxel = voxel.unsqueeze(2).to(device)\n",
    "                    image = torch.nn.functional.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    image = image.to(device)\n",
    "                clip_target = clip_extractor.embed_image(image.float()).float()\n",
    "                if dual_guidance:\n",
    "                    clip_target_txt = clip_extractor.embed_text(caption).float()\n",
    "                voxel = torch.mean(voxel, dim=1)\n",
    "                # voxel_ridge = model.ridge(voxel,0)\n",
    "                # backbone, backbone_txt, clip_voxels, blurry_image_enc = model.backbone(voxel_ridge)\n",
    "                backbone=torch.cat([torch.from_numpy(clip_image_model.predict(voxel_list[si]).reshape(-1, clip_seq_dim, clip_emb_dim)) for si, s in enumerate(subj_list)],dim=0).to(device, data_type)\n",
    "                clip_voxels = backbone\n",
    "                backbone_txt=torch.cat([torch.from_numpy(clip_text_model.predict(voxel_list[si]).reshape(-1, clip_text_seq_dim, clip_emb_dim)) for si, s in enumerate(subj_list)],dim=0).to(device, data_type)\n",
    "                image_enc_pred = torch.cat([torch.from_numpy(vae_image_model.predict(voxel_list[si]).reshape(-1, 4, 28, 28)) for si, s in enumerate(subj_list)],dim=0).to(device, data_type)\n",
    "                # print(f\"Ridge output shapes: {backbone.shape}, {backbone_txt.shape}, {clip_voxels.shape}, {image_enc_pred.shape}\")\n",
    "            \n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                if use_prior:\n",
    "                    # image part\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    if dual_guidance:\n",
    "                        # txt part\n",
    "                        loss_prior_txt, contaminated_prior_out_txt = model.diffusion_prior_txt(text_embed=backbone_txt[random_samps], image_embed=clip_target_txt[random_samps])\n",
    "                        test_loss_prior_total_txt += loss_prior_txt.item()\n",
    "                        loss_prior_txt *= prior_scale\n",
    "                        loss += loss_prior_txt\n",
    "                    \n",
    "                    if visualize_prior:\n",
    "                        # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                        prior_out = model.diffusion_prior.p_sample_loop(backbone[random_samps].shape, \n",
    "                                        text_cond = dict(text_embed = backbone[random_samps]), \n",
    "                                        cond_scale = 1., timesteps = 20)\n",
    "                        if dual_guidance:\n",
    "                            prior_out_txt = model.diffusion_prior_txt.p_sample_loop(backbone_txt[random_samps].shape, \n",
    "                                            text_cond = dict(text_embed = backbone_txt[random_samps]), \n",
    "                                            cond_scale = 1., timesteps = 20)\n",
    "                            \n",
    "                            test_recon_cossim_txt += nn.functional.cosine_similarity(prior_out_txt, clip_target_txt[random_samps]).mean().item()\n",
    "                        \n",
    "                        test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target[random_samps]).mean().item()\n",
    "                        test_recon_mse += mse(prior_out, clip_target[random_samps]).item()\n",
    "                        \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            print(\"---\")\n",
    "\n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"train/recon_txt_cossim\": recon_cossim_txt / (train_i +1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"test/recon_txt_cossim\": test_recon_cossim_txt / (test_i +1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                \"train/loss_prior_txt\": loss_prior_total_txt / (train_i + 1),\n",
    "                \"test/loss_prior_txt\": test_loss_prior_total_txt / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = np.random.randint(0, 3)\n",
    "                    print(f\"reconstructing... idx={idx}\")\n",
    "                    samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/orig\"] = wandb.Image(transforms.ToPILImage()(image[idx]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                        logs[f\"test/recons\"] = wandb.Image(transforms.ToPILImage()(samples[0]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                    if utils.is_interactive():\n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.plot(losses)\n",
    "# plt.show()\n",
    "# plt.plot(test_losses)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73882c-8d74-4d80-b9d3-2f02aa71d5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77ad2d-c1e2-4e1d-a99e-d69d0e84957b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
