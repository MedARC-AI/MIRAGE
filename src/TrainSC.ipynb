{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.4.1+cu121)\n",
      "    Python  3.11.6 (you have 3.11.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/xformers/triton/softmax.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16 if _triton_softmax_fp16_enabled else None)\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/xformers/triton/softmax.py:86: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/xformers/ops/swiglu_op.py:106: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/xformers/ops/swiglu_op.py:127: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from versatile_diffusion import Reconstructor\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from sc_reconstructor import SC_Reconstructor\n",
    "from vdvae import VDVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 32\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]\n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 1307635\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588554b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: testing_txt_SC\n",
      "--data_path=/weka/proj-medarc/shared/mindeyev2_dataset                     --cache_dir=/weka/proj-medarc/shared/cache                     --model_name=testing_txt_SC                     --no-multi_subject --subj=1 --batch_size=32 --num_sessions=40                     --hidden_dim=1024 --clip_scale=1.                     --blurry_recon --blur_scale=.5                      --seq_past=0 --seq_future=0                     --use_prior --prior_scale=30                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=150 --no-use_image_aug                     --ckpt_interval=1 --ckpt_saving --dual_guidance --caption_type medium\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"testing_txt_SC\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --cache_dir=/weka/proj-medarc/shared/cache \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} --num_sessions=40 \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --blurry_recon --blur_scale=.5  \\\n",
    "                    --seq_past=0 --seq_future=0 \\\n",
    "                    --use_prior --prior_scale=30 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=150 --no-use_image_aug \\\n",
    "                    --ckpt_interval=1 --ckpt_saving --dual_guidance --caption_type medium\"\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 \n",
    "    %reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 40\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=1,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires much more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_imageryrf\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the ImageryRF dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_nsd\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Don't use the Natural Scenes Dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr_threshold\",type=float,default=-1.0,\n",
    "    help=\"Used for calculating SNR on a whole brain to narrow down voxels.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",type=float,default=1e-2,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"all\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--caption_type\",type=str,default='medium',choices=['coco','short', 'medium', 'schmedium'],\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    if train_imageryrf:\n",
    "            # 9,10,11 is ImageryRF subjects\n",
    "        if no_nsd:\n",
    "            subj_list = np.arange(9,12)\n",
    "        else:\n",
    "            subj_list = np.arange(1,12)\n",
    "    else:\n",
    "        subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4743c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3964338",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 32 num_iterations_per_epoch = 937 num_samples_per_epoch = 30000\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 40 sessions\n",
      "/weka/proj-medarc/shared/mindeyev2_dataset/wds/subj01/train/{0..39}.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/webdataset/compat.py:136: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
      "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 15724])\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/weka/proj-medarc/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n",
      "currently using 1 seq_len (chose 0 past behav and 0 future behav)\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    # If an NSD subject\n",
    "    if s < 9:\n",
    "        if multi_subject:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "        else:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "        print(train_url)\n",
    "        \n",
    "        train_data[f'subj{s:02d}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                            .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                            .decode(\"torch\")\\\n",
    "                            .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                            .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "        betas = utils.create_snr_betas(subject=s, data_type=data_type, data_path=data_path, threshold = snr_threshold)\n",
    "        print(betas.shape)\n",
    "        num_voxels_list.append(betas[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        voxels[f'subj{s:02d}'] = betas\n",
    "    elif s < 12:\n",
    "        train_url = \"\"\n",
    "        test_url = \"\"\n",
    "        betas, images, _, _ = utils.load_imageryrf(subject=int(s-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=False, split=True)\n",
    "        betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "        betas = betas.to(\"cpu\").to(data_type)\n",
    "        num_voxels_list.append(betas[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        num_nan_values = torch.sum(torch.isnan(betas))\n",
    "        print(\"Number of NaN values in betas:\", num_nan_values.item())\n",
    "        indices = torch.randperm(len(betas))\n",
    "        shuffled_betas = betas[indices]\n",
    "        shuffled_images = images[indices]\n",
    "        train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        \n",
    "        \n",
    "    # elif s < 15:\n",
    "    #     betas, images = utils.load_imageryrf(subject=int(s-11), mode=\"imagery\", mask=True, stimtype=\"object\", average=False, nest=False)\n",
    "    #     betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    #     betas = betas.to(\"cpu\").to(data_type)\n",
    "    #     num_voxels_list.append(betas[0].shape[-1])\n",
    "    #     num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        \n",
    "    #     indices = torch.randperm(len(betas))\n",
    "    #     shuffled_betas = betas[indices]\n",
    "    #     shuffled_images = images[indices]\n",
    "    #     train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "    #     train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    print(f\"num_voxels for subj{s:02d}: {num_voxels[f'subj{s:02d}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject (doesn't support ImageryRF)\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "if subj < 9:\n",
    "    test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "else:\n",
    "    _, _, betas, images = utils.load_imageryrf(subject=int(subj-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "    num_test = len(betas)\n",
    "    betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    betas = betas.to(\"cpu\").to(data_type)\n",
    "    num_nan_values = torch.sum(torch.isnan(betas))\n",
    "    print(\"Number of NaN values in test betas:\", num_nan_values.item())\n",
    "    test_data = torch.utils.data.TensorDataset(betas, images)\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")\n",
    "\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! (73000, 3, 224, 224)\n",
      "Loaded all 73k NSD captions to cpu! (73000,)\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)\n",
    "# Load 73k NSD captions\n",
    "# captions = np.load(f'{data_path}/preprocessed_data/annots_73k.npy')\n",
    "captions = np.load(f'{data_path}/annots_73k.npy')\n",
    "print(\"Loaded all 73k NSD captions to cpu!\", captions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168051b",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a17b31",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d4ab35b-5144-40a1-9bfb-8a30d56d745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable Cascade Reconstructor: Loading model...\n",
      "['model_version', 'effnet_checkpoint_path', 'previewer_checkpoint_path']\n",
      "['transforms', 'clip_preprocess', 'gdf', 'sampling_configs', 'effnet_preprocess']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60287a4a380c401cbecfe5ded6c67b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenizer', 'text_model', 'generator', 'effnet', 'previewer']\n",
      "STAGE C READY\n",
      "Restoring ema vae from /weka/proj-medarc/shared/cache/imagenet64-iter-1600000-model-ema.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/ckadirt/imagery_clean/MindEye_Imagery/src/vdvae/train_helpers.py:126: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(distributed_maybe_download(path, local_rank, mpi_size), map_location='cpu' if map_cpu else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_emb_dim 768\n",
      "clip_seq_dim 1\n",
      "retrieval_emb_dim 1024\n",
      "retrieval_seq_dim 257\n",
      "text_embedding_variant stable_cascade_medium\n",
      "clip_text_seq_dim 77\n",
      "clip_text_emb_dim 1280\n",
      "latent_embedding_variant vdvae\n",
      "latent_emb_dim 91168\n",
      "prompt_embedding_variant git\n",
      "git_seq_dim 257\n",
      "git_emb_dim 1024\n"
     ]
    }
   ],
   "source": [
    "clip_extractor = SC_Reconstructor(compile_models=False, embedder_only=True, device=device, cache_dir=cache_dir)\n",
    "vdvae = VDVAE(device=device, cache_dir=cache_dir)\n",
    "image_embedding_variant = \"stable_cascade\"\n",
    "clip_emb_dim = 768\n",
    "clip_seq_dim = 1\n",
    "retrieval_embedding_variant = \"stable_cascade_hidden\"\n",
    "retrieval_emb_dim = 1024\n",
    "retrieval_seq_dim = 257\n",
    "text_embedding_variant = \"stable_cascade\"\n",
    "clip_text_seq_dim=77\n",
    "clip_text_emb_dim=1280\n",
    "latent_embedding_variant = \"vdvae\"\n",
    "latent_emb_dim = 91168\n",
    "prompt_embedding_variant = \"git\"\n",
    "git_seq_dim = 257\n",
    "git_emb_dim = 1024\n",
    "if caption_type != \"coco\":\n",
    "    text_embedding_variant += f\"_{caption_type}\"\n",
    "\n",
    "print(\"clip_emb_dim\", clip_emb_dim)\n",
    "print(\"clip_seq_dim\", clip_seq_dim)\n",
    "print(\"retrieval_emb_dim\", retrieval_emb_dim)\n",
    "print(\"retrieval_seq_dim\", retrieval_seq_dim)\n",
    "print(\"text_embedding_variant\", text_embedding_variant)\n",
    "print(\"clip_text_seq_dim\", clip_text_seq_dim)\n",
    "print(\"clip_text_emb_dim\", clip_text_emb_dim)\n",
    "print(\"latent_embedding_variant\", latent_embedding_variant)\n",
    "print(\"latent_emb_dim\", latent_emb_dim)\n",
    "print(\"prompt_embedding_variant\", prompt_embedding_variant)\n",
    "print(\"git_seq_dim\", git_seq_dim)\n",
    "print(\"git_emb_dim\", git_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "595926d7-6e88-42a7-acdc-fb8966e6c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"clip_emb_dim\", clip_emb_dim)\n",
    "# print(\"clip_seq_dim\", clip_seq_dim)\n",
    "# print(\"retrieval_emb_dim\", retrieval_emb_dim)\n",
    "# print(\"retrieval_seq_dim\", retrieval_seq_dim)\n",
    "# print(\"text_embedding_variant\", text_embedding_variant)\n",
    "# print(\"clip_text_seq_dim\", clip_text_seq_dim)\n",
    "# print(\"clip_text_emb_dim\", clip_text_emb_dim)\n",
    "# print(\"latent_embedding_variant\", latent_embedding_variant)\n",
    "# print(\"latent_emb_dim\", latent_emb_dim)\n",
    "# print(\"prompt_embedding_variant\", prompt_embedding_variant)\n",
    "# print(\"git_seq_dim\", git_seq_dim)\n",
    "# print(\"git_emb_dim\", git_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip_emb_dim = 768\n",
    "# clip_seq_dim = 257\n",
    "# clip_text_seq_dim=77\n",
    "# clip_extractor = Reconstructor(device=device, cache_dir=f'{cache_dir}/versatile_diffusion_ckpts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2b29e4-ea65-4182-8079-54acfda159f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, valid_nsd_ids_train, x_test, test_nsd_ids = utils.load_nsd(subject=subj, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd2c031-1be6-4a99-a60f-7f6f34e1fff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! (73000, 3, 224, 224)\n",
      "Loaded all 73k NSD captions to cpu! (73000,)\n",
      "Filtered down to only the 27000 training images for subject 1!\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)\n",
    "\n",
    "# Load 73k NSD captions\n",
    "if caption_type == \"schmedium\":\n",
    "    captions_small = np.load(f'{data_path}/preprocessed_data_/short_length_captions.npy')\n",
    "    captions_medium = np.load(f'{data_path}/preprocessed_data_/mid_length_captions_73K.npy')\n",
    "    # Create a mask to randomly select elements from both arrays\n",
    "    mask = np.random.rand(len(captions_small)) > 0.5\n",
    "    # Mix the arrays based on the mask\n",
    "    captions = np.where(mask, captions_small, captions_medium)\n",
    "else:\n",
    "    if caption_type == \"coco\":\n",
    "        caption_file = \"annots_73k.npy\"\n",
    "    elif caption_type == \"short\":\n",
    "        caption_file = \"short_length_captions.npy\"\n",
    "    elif caption_type == \"medium\":\n",
    "        caption_file = \"mid_length_captions_73K.npy\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid caption type\")\n",
    "    captions = np.load(f'{data_path}/preprocessed_data_/{caption_file}')\n",
    "print(\"Loaded all 73k NSD captions to cpu!\", captions.shape)\n",
    "\n",
    "train_images = torch.zeros((len(valid_nsd_ids_train), 3, 224, 224))\n",
    "train_captions = np.zeros((len(valid_nsd_ids_train),), dtype=object)\n",
    "\n",
    "# Load specific training data\n",
    "for i, idx in enumerate(valid_nsd_ids_train):\n",
    "    train_images[i] =  torch.from_numpy(images[idx])\n",
    "    train_captions[i] = captions[idx]\n",
    "    \n",
    "print(f\"Filtered down to only the {len(valid_nsd_ids_train)} training images for subject {subj}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dff86598-b1ba-4924-912f-520f82057534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A person is parasailing over the ocean, holding onto a rope attached to a parachute. They are flying above the water, with a house visible in the background. The parasailer is wearing a black wetsuit and appears to be enjoying the thrilling experience.',\n",
       "       'A black cat is sitting on the rim of a toilet seat, looking at the camera.',\n",
       "       'A person is cutting a pizza on a wooden cutting board, with a salad in a bowl nearby. There are also two wine glasses and a bottle of wine on the table.',\n",
       "       ...,\n",
       "       'A large elephant walks next to a baby elephant in a dirt area.',\n",
       "       'A computer monitor is open to a webpage, with a laptop and a book placed on the desk in front of it. The laptop is on the left side of the desk, while the book is on the right side. There are also two cell phones on the desk, one near the center and the other towards the right side. Additionally, there are two pairs of scissors, one near the center and the other towards the left side of the desk.',\n",
       "       'A red cardinal perched on the side mirror of a car.'],\n",
       "      dtype='<U3768')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if blurry_recon:\n",
    "#     from diffusers import AutoencoderKL    \n",
    "#     autoenc = AutoencoderKL(\n",
    "#         down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "#         up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "#         block_out_channels=[128, 256, 512, 512],\n",
    "#         layers_per_block=2,\n",
    "#         sample_size=256,\n",
    "#     )\n",
    "#     ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "#     autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "#     autoenc.eval()\n",
    "#     autoenc.requires_grad_(False)\n",
    "#     autoenc.to(device)\n",
    "#     utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "torch.Size([2, 1, 15724]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim, seq_len=seq_len)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,seq_len,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "669,256,760 total\n",
      "669,256,760 trainable\n",
      "param counts:\n",
      "669,256,760 total\n",
      "669,256,760 trainable\n",
      "param counts:\n",
      "685,359,160 total\n",
      "685,359,160 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 257, 1024]) torch.Size([2, 77, 1280]) torch.Size([2, 1, 768]) torch.Size([91168]) torch.Size([91168])\n"
     ]
    }
   ],
   "source": [
    "from modelsME2 import BrainNetwork\n",
    "hidden_dim = 1024\n",
    "n_blocks = 4\n",
    "seq_len = 1\n",
    "clip_scale = 1.\n",
    "dual_guidance = True\n",
    "retrieval_embedding_dim = 1024\n",
    "clip_embedding_dim = 768\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, n_blocks=n_blocks,\n",
    "                          clip_size=retrieval_embedding_dim, out_dim=retrieval_embedding_dim*257, \n",
    "                          blurry_recon=blurry_recon, clip_scale=clip_scale, text_clip=dual_guidance, text_clip_dim=1280, clip_embedding_dim=768)\n",
    "\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,seq_len,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_,backbone_txt_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, backbone_txt_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a6f17",
   "metadata": {},
   "source": [
    "### Adding diffusion prior if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "149,749,776 total\n",
      "149,749,760 trainable\n",
      "param counts:\n",
      "55,465,360 total\n",
      "55,465,344 trainable\n",
      "param counts:\n",
      "890,574,296 total\n",
      "890,574,264 trainable\n"
     ]
    }
   ],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 64\n",
    "    heads = clip_emb_dim//64 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    if dual_guidance:\n",
    "        out_dim_txt = clip_text_emb_dim\n",
    "        depth_txt = 6\n",
    "        dim_head_txt = 64\n",
    "        heads_txt = retrieval_embedding_dim//64 # heads * dim_head = clip_emb_dim\n",
    "        timesteps_txt = 100\n",
    "        \n",
    "        prior_network_txt = PriorNetwork(\n",
    "                dim=out_dim_txt,\n",
    "                depth=depth_txt,\n",
    "                dim_head=dim_head_txt,\n",
    "                heads=heads_txt,\n",
    "                causal=False,\n",
    "                num_tokens = clip_text_seq_dim,\n",
    "                learned_query_mode=\"pos_emb\"\n",
    "            )\n",
    "    \n",
    "\n",
    "        model.diffusion_prior_txt = BrainDiffusionPrior(\n",
    "            net=prior_network_txt,\n",
    "            image_embed_dim=out_dim_txt,\n",
    "            condition_on_text_encodings=False,\n",
    "            timesteps=timesteps_txt,\n",
    "            cond_drop_prob=0.2,\n",
    "            image_embed_scale=None,\n",
    "        )\n",
    "        utils.count_params(model.diffusion_prior_txt)\n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 140550\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "890,574,296 total\n",
      "890,574,264 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ])\n",
    "    if dual_guidance:\n",
    "        opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior_txt.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.diffusion_prior_txt.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ])\n",
    "        \n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(f'{outdir}/{tag}.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb mindeye2 run testing_txt_SC\n",
      "wandb_config:\n",
      " {'model_name': 'testing_txt_SC', 'global_batch_size': 32, 'batch_size': 32, 'num_epochs': 150, 'num_sessions': 40, 'num_params': 890574264, 'clip_scale': 1.0, 'prior_scale': 30.0, 'blur_scale': 0.5, 'use_image_aug': False, 'max_lr': 0.0003, 'mixup_pct': 0.33, 'num_samples_per_epoch': 30000, 'num_test': 3000, 'ckpt_interval': 1, 'ckpt_saving': True, 'seed': 42, 'distributed': False, 'num_devices': 1, 'world_size': 1, 'train_url': '/weka/proj-medarc/shared/mindeyev2_dataset/wds/subj01/train/{0..39}.tar', 'test_url': '/weka/proj-medarc/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar', 'train_imageryrf': False, 'mode': 'all'}\n",
      "wandb_id: testing_txt_SC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mckadirt\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/weka/proj-fmri/ckadirt/imagery_clean/MindEye_Imagery/src/wandb/run-20241108_023027-testing_txt_SC</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://stability.wandb.io/ckadirt/mindeye2/runs/testing_txt_SC' target=\"_blank\">testing_txt_SC</a></strong> to <a href='https://stability.wandb.io/ckadirt/mindeye2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://stability.wandb.io/ckadirt/mindeye2' target=\"_blank\">https://stability.wandb.io/ckadirt/mindeye2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://stability.wandb.io/ckadirt/mindeye2/runs/testing_txt_SC' target=\"_blank\">https://stability.wandb.io/ckadirt/mindeye2/runs/testing_txt_SC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeye2'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "      \"train_imageryrf\": train_imageryrf,\n",
    "      \"mode\": mode,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=None,\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5453c316-0cb0-4bee-8585-f44dff746e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved ckpt model weights into current model\n",
    "if resume_from_ckpt:\n",
    "    load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "# elif wandb_log:\n",
    "#     if wandb.run.resumed:\n",
    "#         load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj{s:02d}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58561ab9-dde4-47e4-9c31-44e60ecfc2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fc1a3f56810>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "498d6bf7-6ae6-4a34-aa79-943c89d4899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_txt_SC starting with epoch 0 / 150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829e7f55d3ba436dac24659510750331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307635/10536093.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=data_type):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bddac82df94491b7c5c5d8a406c9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1307635/10536093.py:140: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=data_type):\n",
      "/weka/proj-fmri/ckadirt/imagery_clean/MindEye_Imagery/src/vdvae.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  shapes=torch.load(\"vdvae/vdvae_shapes.pt\")\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/torch/autograd/graph.py:769: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1307635/10536093.py\", line 199, in <module>\n",
      "    loss_clip = utils.mixco_nce(\n",
      "  File \"/weka/proj-fmri/ckadirt/imagery_clean/MindEye_Imagery/src/utils.py\", line 189, in mixco_nce\n",
      "    brain_clip = (preds @ targs.T)/temp\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:111.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'MmBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 259\u001b[0m\n\u001b[1;32m    256\u001b[0m         blurry_pixcorr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pixcorr\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    258\u001b[0m utils\u001b[38;5;241m.\u001b[39mcheck_loss(loss)\n\u001b[0;32m--> 259\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    262\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/mei-env/lib/python3.11/site-packages/accelerate/accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/mei-env/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mei-env/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mei-env/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'MmBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "###### from tqdm.auto import tqdm\n",
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "test_caption = []\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    recon_cossim_txt = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    test_recon_cossim_txt = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    loss_prior_total_txt = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "    test_loss_prior_total_txt = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    caption_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    # print(f\"num_iterations_per_epoch: {num_iterations_per_epoch}, batch_size: {batch_size}, len(subj_list): {len(subj_list)}\")\n",
    "    for s, (cur_subj, train_dl) in enumerate(zip(subj_list, train_dls)):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            i = 0\n",
    "            while i < num_iterations_per_epoch:\n",
    "                # print(f\"restarting data loader at i={i} for s={s}\")\n",
    "                for data in train_dl:  \n",
    "                    if cur_subj < 9:\n",
    "                        behav0, past_behav0, future_behav0, old_behav0 = data\n",
    "                        \n",
    "                        # image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                        # image_sorted_idx = behav0[:,0,0].cpu().long().numpy()\n",
    "                        # image_sorted_idx = np.unique(np.sort(image_sorted_idx))\n",
    "                        \n",
    "                        # image0 = images[image_sorted_idx]\n",
    "                        # image0 = torch.tensor(image0, dtype=torch.float16, device=\"cpu\")  # Convert to tensor\n",
    "                        # while image0.shape[0] < batch_size:\n",
    "                        #     image0 = torch.cat((image0, image0[0].unsqueeze(0)), dim=0)\n",
    "                        image_idx = behav0[:,0,0].cpu().long().numpy()\n",
    "                        local_idx, image_sorted_idx = np.unique(image_idx, return_index=True)                \n",
    "                        # if len(image0) != len(image_idx): # hdf5 cant handle duplicate indexing\n",
    "                        #     continue\n",
    "                        image0 = torch.tensor(images[local_idx], dtype=data_type)\n",
    "                        image_iters[i, s*batch_size:s*batch_size+image0.shape[0]] = image0\n",
    "                        caption0 = captions[local_idx]\n",
    "                        caption_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = caption0\n",
    "                        # print(caption0.shape)\n",
    "                        # for caption in caption0:\n",
    "                        #     caption_iters.append(caption)\n",
    "                        \n",
    "                        # voxel_sorted_idx = behav0[:,0,5].cpu().long().numpy()\n",
    "                        # voxel_sorted_idx = np.unique(np.sort(voxel_sorted_idx))\n",
    "                        # voxel0 = voxels[f'subj{subj_list[s]:02d}'][voxel_sorted_idx]\n",
    "                        # voxel0 = torch.Tensor(voxel0)\n",
    "                        # while voxel0.shape[0] < batch_size:\n",
    "                        #     voxel0 = torch.cat((voxel0, voxel0[0].unsqueeze(0)), dim=0)\n",
    "                        voxel_idx = behav0[:,0,5].cpu().long().numpy()\n",
    "                        voxel_sorted_idx = voxel_idx[image_sorted_idx]\n",
    "                        voxel0 = voxels[f'subj0{subj_list[s]}'][voxel_sorted_idx]\n",
    "                        voxel0 = torch.Tensor(voxel0)#.unsqueeze(1)\n",
    "                    else:\n",
    "                        voxel0, image0 = data\n",
    "                        image0 = torch.nn.functional.interpolate(image0, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                        image_iters[i,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                    \n",
    "                    if seq_len==1:\n",
    "                        voxel0 = voxel0.unsqueeze(1)\n",
    "                    else:\n",
    "                        if seq_past>0:\n",
    "                            past_behavior = past_behav0[:,:(seq_past),5].cpu().long()\n",
    "                            past_voxel0 = voxels[f'subj{subj_list[s]:02d}'][past_behavior]\n",
    "                            past_voxel0[past_behavior==-1] = voxel0[torch.where(past_behavior==-1)[0]] # replace invalid past voxels \n",
    "                            past_voxel0 = torch.Tensor(past_voxel0)\n",
    "\n",
    "                            # if shared1000, then you need to mask it out \n",
    "                            for p in range(seq_past):\n",
    "                                mask = (past_behav0[:,p,-1] == 1) # [16,] bool\n",
    "                                index = torch.nonzero(mask.cpu()).squeeze()\n",
    "                                past_voxel0[index,p,:] = torch.zeros_like(past_voxel0[index,p,:])\n",
    "\n",
    "                        if seq_future>0:\n",
    "                            future_behavior = future_behav0[:,:(seq_future),5].cpu().long()\n",
    "                            future_voxel0 = voxels[f'subj{subj_list[s]:02d}'][future_behavior]\n",
    "                            future_voxel0[future_behavior==-1] = voxel0[torch.where(future_behavior==-1)[0]] # replace invalid past voxels \n",
    "                            future_voxel0 = torch.Tensor(future_voxel0)\n",
    "\n",
    "                            # if shared1000, then you need to mask it out \n",
    "                            for p in range(seq_future):\n",
    "                                mask = (future_behav0[:,p,-1] == 1) # [16,] bool\n",
    "                                index = torch.nonzero(mask.cpu()).squeeze()\n",
    "                                future_voxel0[index,p,:] = torch.zeros_like(future_voxel0[index,p,:])\n",
    "\n",
    "                        # concatenate current timepoint with past/future\n",
    "                        if seq_past > 0 and seq_future > 0:\n",
    "                            voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                            voxel0 = torch.cat((voxel0, future_voxel0), axis=1)\n",
    "                        elif seq_past > 0:\n",
    "                            voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                        else:\n",
    "                            voxel0 = torch.cat((voxel0.unsqueeze(1), future_voxel0), axis=1)\n",
    "\n",
    "                    if epoch < int(mixup_pct * num_epochs):\n",
    "                        voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                        perm_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = perm\n",
    "                        betas_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = betas\n",
    "                        select_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = select\n",
    "\n",
    "                    voxel_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = voxel0\n",
    "                    i +=1\n",
    "                    if (i >= num_iterations_per_epoch):\n",
    "                        # print(f\"breaking data loader at i={i} for s={s}\")\n",
    "                        break\n",
    "                    \n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    print(num_iterations_per_epoch)\n",
    "    for train_i in tqdm(range(num_iterations_per_epoch)):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj{s:02d}_iter{train_i}\"] for s in subj_list]\n",
    "            # print(f\"voxel_list {voxel_list}\")\n",
    "            image = image_iters[train_i].detach()\n",
    "            caption = np.concatenate([caption_iters[f\"subj{s:02d}_iter{train_i}\"] for s in subj_list])\n",
    "            image = image[0:len(caption)].to(device)   # fix the last batch problem\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "            clip_target_retrieval = clip_extractor.embed_image(image, hidden=True)\n",
    "            \n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "            if dual_guidance:\n",
    "                clip_target_txt = clip_extractor.embed_text(caption.tolist()).float()\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj{s:02d}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj{s:02d}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj{s:02d}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [model.ridge(voxel_list[si].detach().to(device), si) for si, s in enumerate(subj_list)]\n",
    "            voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            clip_voxels, backbone_txt, backbone, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "            \n",
    "            \n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target_retrieval.flatten(1), dim=-1)\n",
    "                if dual_guidance:\n",
    "                    clip_target_txt_norm = nn.functional.normalize(clip_target_txt.flatten(1), dim=-1)\n",
    "\n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                # print(f\"loss prior {loss_prior.item()}\")\n",
    "                loss += loss_prior\n",
    "                # for txt\n",
    "                if dual_guidance:\n",
    "                    loss_prior_txt, prior_out_txt = model.diffusion_prior_txt(text_embed=backbone_txt, image_embed=clip_target_txt)\n",
    "                    loss_prior_total_txt += loss_prior_txt.item()\n",
    "                    loss_prior_txt *= prior_scale\n",
    "                \n",
    "                # print(f\"loss prior {loss_prior.item()}\")\n",
    "                    loss += loss_prior_txt\n",
    "                    recon_cossim_txt += nn.functional.cosine_similarity(prior_out_txt, clip_target_txt).mean().item()\n",
    "                \n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                \n",
    "                loss += loss_clip\n",
    "\n",
    "            if blurry_recon:     \n",
    "                image_enc_pred = blurry_image_enc_\n",
    "\n",
    "                # image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                with torch.no_grad():\n",
    "                    image_enc = vdvae.embed_latent((image* 255).to(torch.uint8).to('cpu')).to('cuda').clone()\n",
    "                loss_blurry = mse(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                # if epoch < int(mixup_pct * num_epochs):\n",
    "                #     image_enc_shuf = image_enc[perm]\n",
    "                #     betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                #     image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                #         image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                loss += loss_blurry * blur_scale \n",
    "                # loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "\n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    image_enc_pred = blurry_image_enc_\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "\n",
    "                    blurry_recon_images = torch.zeros(len(random_samps),3,768,768)\n",
    "                    for temp_i, samp in enumerate(random_samps):\n",
    "                        c_recon = vdvae.reconstruct(image_enc_pred[0].unsqueeze(0))\n",
    "                        c_recon = torch.Tensor(np.array(c_recon)).permute(2,0,1)\n",
    "                        blurry_recon_images[temp_i] = c_recon\n",
    "                    # blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    # pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    # print(blurry_recon_images.shape, image[random_samps].to('cpu').shape)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps].to('cpu'), blurry_recon_images.clamp(0,1))\n",
    "                    \n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            caption = []\n",
    "            for test_i, data in enumerate(test_dl):  \n",
    "                if subj < 9:\n",
    "                    behav, past_behav, future_behav, old_behav = data\n",
    "                    # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                    assert len(behav) == num_test\n",
    "\n",
    "                    ## Average same-image repeats ##\n",
    "                    if test_image is None:\n",
    "                        voxel = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "                        \n",
    "                        if seq_len==1:\n",
    "                            voxel = voxel.unsqueeze(1)\n",
    "                        else:\n",
    "                            if seq_past>0:\n",
    "                                past_behavior = past_behav[:,:(seq_past),5].cpu().long()\n",
    "                                past_voxels = voxels[f'subj0{subj}'][past_behavior]\n",
    "                                if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                    past_voxels[torch.where(past_behavior==-1)[0]] = 0\n",
    "\n",
    "                            if seq_future>0:\n",
    "                                future_behavior = future_behav[:,:(seq_future),5].cpu().long()\n",
    "                                future_voxels = voxels[f'subj0{subj}'][future_behavior]                    \n",
    "                                if torch.any(future_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                    future_voxels[torch.where(future_behavior==-1)[0]] = 0\n",
    "                                \n",
    "                            if seq_past > 0 and seq_future > 0:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                                voxel = torch.cat((voxel, future_voxels), axis=1)\n",
    "                            elif seq_past > 0:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                            else:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), future_voxels), axis=1)\n",
    "\n",
    "                        image = behav[:,0,0].cpu().long()\n",
    "\n",
    "                        unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                        for im in unique_image:\n",
    "                            locs = torch.where(im == image)[0]\n",
    "                            if len(locs)==1:\n",
    "                                locs = locs.repeat(3)\n",
    "                            elif len(locs)==2:\n",
    "                                locs = locs.repeat(2)[:3]\n",
    "                            assert len(locs)==3\n",
    "                            if test_image is None:\n",
    "                                test_image = torch.tensor(images[im][None], dtype=torch.float16, device=\"cpu\")\n",
    "                                if dual_guidance:\n",
    "                                    test_caption = [captions[im]]\n",
    "                                test_voxel = voxel[locs][None]\n",
    "                            else:\n",
    "                                test_image = torch.vstack((test_image, torch.tensor(images[im][None], dtype=torch.float16, device=\"cpu\")))\n",
    "                                if dual_guidance:\n",
    "                                    test_caption.append(captions[im])\n",
    "                                test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "\n",
    "                    loss=0.\n",
    "                                \n",
    "                    test_indices = torch.arange(len(test_voxel))[:300]\n",
    "                    # print(test_image.shape,len(test_caption),test_indices.max())\n",
    "                    voxel = test_voxel[test_indices].to(device)\n",
    "                    image = test_image[test_indices].to(device)\n",
    "                    if dual_guidance:\n",
    "                        for idx in test_indices:\n",
    "                            # if isinstance(idx, torch.Tensor):\n",
    "                            #     idx = idx.item()\n",
    "                            caption.append(test_caption[idx])\n",
    "                        assert len(caption) == 300\n",
    "                    assert len(image) == 300\n",
    "                else:\n",
    "                    voxel, image = data\n",
    "                    voxel = voxel.unsqueeze(2).to(device)\n",
    "                    image = torch.nn.functional.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    image = image.to(device)\n",
    "\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "                clip_target_retrieval = clip_extractor.embed_image(image, hidden=True)\n",
    "\n",
    "                if dual_guidance:\n",
    "                    clip_target_txt = clip_extractor.embed_text(caption).float()\n",
    "                voxel = torch.mean(voxel, dim=1)\n",
    "                voxel_ridge = model.ridge(voxel,0)\n",
    "                clip_voxels, backbone_txt, backbone, blurry_image_enc_ = model.backbone(voxel_ridge) # Double check captions\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target_retrieval.flatten(1), dim=-1)\n",
    "                    # clip_voxels_txt_norm = nn.functional.normalize(clip_voxels_txt.flatten(1), dim=-1)\n",
    "                    if dual_guidance:\n",
    "                        clip_target_txt_norm = nn.functional.normalize(clip_target_txt.flatten(1), dim=-1)\n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                if use_prior:\n",
    "                    # image part\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    if dual_guidance:\n",
    "                        # txt part\n",
    "                        loss_prior_txt, contaminated_prior_out_txt = model.diffusion_prior_txt(text_embed=backbone_txt[random_samps], image_embed=clip_target_txt[random_samps])\n",
    "                        test_loss_prior_total_txt += loss_prior_txt.item()\n",
    "                        loss_prior_txt *= prior_scale\n",
    "                        loss += loss_prior_txt\n",
    "                    \n",
    "                    if visualize_prior:\n",
    "                        # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                        prior_out = model.diffusion_prior.p_sample_loop(backbone[random_samps].shape, \n",
    "                                        text_cond = dict(text_embed = backbone[random_samps]), \n",
    "                                        cond_scale = 1., timesteps = 20)\n",
    "                        if dual_guidance:\n",
    "                            prior_out_txt = model.diffusion_prior_txt.p_sample_loop(backbone_txt[random_samps].shape, \n",
    "                                            text_cond = dict(text_embed = backbone_txt[random_samps]), \n",
    "                                            cond_scale = 1., timesteps = 20)\n",
    "                            \n",
    "                            test_recon_cossim_txt += nn.functional.cosine_similarity(prior_out_txt, clip_target_txt[random_samps]).mean().item()\n",
    "                        \n",
    "                        test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target[random_samps]).mean().item()\n",
    "                        test_recon_mse += mse(prior_out, clip_target[random_samps]).item()\n",
    "                        \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                # if blurry_recon:\n",
    "                #     image_enc_pred = blurry_image_enc_\n",
    "                #     blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                #     pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                #     test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if blurry_recon:\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    image_enc_pred = blurry_image_enc_\n",
    "                    blurry_recon_images = torch.zeros(len(random_samps),3,768,768)\n",
    "                    for temp_i, samp in enumerate(random_samps):\n",
    "                        c_recon = vdvae.reconstruct(image_enc_pred[0].unsqueeze(0))\n",
    "                        c_recon = torch.Tensor(np.array(c_recon)).permute(2,0,1)\n",
    "                        blurry_recon_images[temp_i] = c_recon\n",
    "                    # blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    # pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    # print(blurry_recon_images.shape, image[random_samps].to('cpu').shape)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps].to('cpu'), blurry_recon_images.clamp(0,1).half())\n",
    "                    \n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            print(\"---\")\n",
    "\n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"train/recon_txt_cossim\": recon_cossim_txt / (train_i +1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"test/recon_txt_cossim\": test_recon_cossim_txt / (test_i +1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                \"train/loss_prior_txt\": loss_prior_total_txt / (train_i + 1),\n",
    "                \"test/loss_prior_txt\": test_loss_prior_total_txt / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    # image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    image_enc = vdvae.embed_latent((image[:4]* 255).to(torch.uint8).to('cpu')).to('cuda')\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        # axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].imshow(vdvae.reconstruct(latents = image_enc[j].unsqueeze(0)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        # axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].imshow(vdvae.reconstruct(latents = image_enc_pred[j].unsqueeze(0)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = np.random.randint(0, 3)\n",
    "                    print(f\"reconstructing... idx={idx}\")\n",
    "                    samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/orig\"] = wandb.Image(transforms.ToPILImage()(image[idx]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                        logs[f\"test/recons\"] = wandb.Image(transforms.ToPILImage()(samples[0]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                    if utils.is_interactive():\n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59807e4-d380-4257-af8b-468fc0e5f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_recon_images.clamp(0,1).half().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5aab29-5a95-487f-8b6c-be662f26dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixcorr = utils.pixcorr(image[random_samps].to('cpu'), image[random_samps].to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b34d5-7e47-4ded-8391-e457888eaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_enc_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a41a89-32c9-4f2b-95b5-d36242a88e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "codec = vdvae.embed_latent(image[10].unsqueeze(0).to('cpu'))\n",
    "recon_ = vdvae.reconstruct(latents = torch.rand(1,91168))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb0b23-c84d-4892-93e9-84ca6965f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(recon_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c1184-c40b-4aa0-bcfd-546f8754da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_enc_pred[0], image_enc_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5370df-96dd-4a22-b294-227e216b1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixcorr = utils.pixcorr(image[random_samps].to('cpu'), blurry_recon_images.clamp(0,1))\n",
    "pixcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49cdd7-b26c-4dca-aaeb-ff570780484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_blurry_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744b560-78ce-4232-ad65-0008f2b52ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(blurry_recon_images[2].to(torch.int8).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e29734-4351-40f2-87e4-24fe9f2e8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(blurry_recon_images[0].to(torch.int8).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a15f1a-fca7-4238-8758-1c6612d5d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_blurry = l1(image_enc_pred, image_enc)\n",
    "loss_blurry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746d8cd-cdd6-448a-b70c-709fa0dbd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_enc_pred[0,0:10], image_enc[0,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fd34d-7bde-41f6-bc9e-d94898bf5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_recon_images2 = F.interpolate(\n",
    "    blurry_recon_images, \n",
    "    size=(224, 224),  # Desired height and width\n",
    "    mode='bilinear',   # Interpolation mode\n",
    "    align_corners=False  # Recommended for bilinear interpolation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7a808-3a3e-4f17-99d3-c01485c26c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurry_recon_images2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdcf7d-bc83-41c0-9e87-c68641e80127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(image[10].permute(1,2,0).to('cpu').float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cff735-471a-4cab-88b5-bf3126663db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming 'image' is your tensor with shape (batch_size, channels, height, width)\n",
    "# Select the 11th image in the batch (index 10)\n",
    "selected_image = image[10]\n",
    "\n",
    "# Permute the dimensions to (height, width, channels)\n",
    "selected_image = selected_image.permute(1, 2, 0)\n",
    "\n",
    "# Move to CPU and convert to float\n",
    "selected_image = selected_image.to('cpu').float()\n",
    "\n",
    "# Convert to NumPy array\n",
    "numpy_array = selected_image.numpy()\n",
    "\n",
    "# If the tensor values are in [0, 1], scale to [0, 255]\n",
    "# This step is crucial for proper visualization\n",
    "numpy_array = (numpy_array * 255).astype(np.uint8)\n",
    "\n",
    "# Create the PIL Image\n",
    "mm = Image.fromarray(numpy_array)\n",
    "\n",
    "# Optionally, display the image\n",
    "mm.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024d7c3-6d4f-4a12-8735-bde976658c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image[10].permute(1,2,0).to('cpu').float().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6fd73-730b-4514-bf3f-1724b285f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_images = (image[10:12]* 255).to(torch.uint8).to('cpu')\n",
    "codec = vdvae.embed_latent(two_images)\n",
    "# codec = vdvae.embed_latent(image[10].to('cpu'))\n",
    "recon_ = vdvae.reconstruct(latents = codec[1].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f0a29-cea6-4f86-8687-189647356188",
   "metadata": {},
   "outputs": [],
   "source": [
    "codec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a260a1f-671d-4cf8-a55a-117371622b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87722b98-94fd-475b-bd30-a7025b152bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codec2 = vdvae.embed_latent(mm)\n",
    "recon_ = vdvae.reconstruct(latents = codec2[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ba132-be71-4672-9a58-e1e0fb938b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "image[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9387e-0f4f-4773-9e33-69c426d7d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "codec2.min(), codec.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a0b17-1fea-439f-a306-96d4640348b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "codec2.reconstruc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134918d-98ea-4220-9843-4c46cfdbe9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(backbone_txt.shape, clip_target_txt.shape, clip_target.shape, clip_voxels.shape, backbone.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785db93-003f-40f9-8c25-56c8d8c9e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_target_retrieval = clip_extractor.embed_image(image, hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f0eb6-d8c1-4738-80bf-6fe75b51687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blurry_image = vdvae.embed_latent(image.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5cd21-f844-4683-b3d9-c23782cd2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_enc_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add1dc1-112f-4688-abb9-9ff0264f5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconnnn = vdvae.reconstruct(image_enc_pred[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edff747-bc21-4f6d-8c0d-92130b12c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.pixcorr(image[0].to('cpu').unsqueeze(0), torch.Tensor(np.array(reconnnn)).permute(2,0,1).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abfe85-4a26-4271-86d5-ae70f5b82fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(image[0].unsqueeze(0).shape), torch.Tensor(np.array(reconnnn)).permute(2,0,1).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee734e-a64d-47c6-ba7f-d58c8804c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"train/recon_txt_cossim\": recon_cossim_txt / (train_i +1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"test/recon_txt_cossim\": test_recon_cossim_txt / (test_i +1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                \"train/loss_prior_txt\": loss_prior_total_txt / (train_i + 1),\n",
    "                \"test/loss_prior_txt\": test_loss_prior_total_txt / (test_i + 1),\n",
    "                }\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb752eeb-46d3-458f-a02b-df400ab2aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"train/recon_txt_cossim\": recon_cossim_txt / (train_i +1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"test/recon_txt_cossim\": test_recon_cossim_txt / (test_i +1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                \"train/loss_prior_txt\": loss_prior_total_txt / (train_i + 1),\n",
    "                \"test/loss_prior_txt\": test_loss_prior_total_txt / (test_i + 1),\n",
    "                }\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.plot(losses)\n",
    "# plt.show()\n",
    "# plt.plot(test_losses)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mei-env",
   "language": "python",
   "name": "mei-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
