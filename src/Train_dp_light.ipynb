{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from models import Clipper\n",
    "from versatile_diffusion import Reconstructor\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder # bigG embedder\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from sklearn.linear_model import Ridge\n",
    "import pickle\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd260e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 8\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]\n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3acf5160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 84693\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588554b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: subj01_40sess_hypatia_ridge_flat_dp_light\n",
      "--data_path=../dataset                     --cache_dir=../cache                     --model_name=subj01_40sess_hypatia_ridge_flat_dp_light --subj=1                     --no-multi_subject                     --mode imagery                     --use_prior                     --dual_guidance\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"subj01_40sess_hypatia_ridge_flat_dp_test\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=../dataset \\\n",
    "                    --cache_dir=../cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --no-multi_subject \\\n",
    "                    --mode imagery \\\n",
    "                    --use_prior \\\n",
    "                    --dual_guidance\"\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 1\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=1,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires much more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"mindeye_imagery\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=5e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ridge_weight_decay\",type=float,default=60000,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_weight_decay\",type=float,default=1e-2,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_imageryrf\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the ImageryRF dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_nsd\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Don't use the Natural Scenes Dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr_threshold\",type=float,default=-1.0,\n",
    "    help=\"Used for calculating SNR on a whole brain to narrow down voxels.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"all\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    if train_imageryrf:\n",
    "            # 9,10,11 is ImageryRF subjects\n",
    "        if no_nsd:\n",
    "            subj_list = np.arange(9,12)\n",
    "        else:\n",
    "            subj_list = np.arange(1,12)\n",
    "    else:\n",
    "        subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4743c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3964338",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d321316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 16 num_iterations_per_epoch = 46 num_samples_per_epoch = 750\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "num_devices = 1\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1 sessions\n",
      "../dataset/wds/subj01/train/{0..0}.tar\n",
      "torch.Size([1000, 15724]) (27000,)\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "../dataset/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n",
      "currently using 1 seq_len (chose 0 past behav and 0 future behav)\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    # If an NSD subject\n",
    "    if s < 9:\n",
    "        if multi_subject:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "        else:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "        print(train_url)\n",
    "        \n",
    "        train_data[f'subj{s:02d}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                            .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                            .decode(\"torch\")\\\n",
    "                            .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                            .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        betas = utils.create_snr_betas(subject=s, data_type=data_type, data_path=data_path, threshold = snr_threshold)\n",
    "        x_train, train_nsd_ids, x_test, test_nsd_ids = utils.load_nsd(subject=s, betas=betas, data_path=data_path)\n",
    "        print(x_test.shape, train_nsd_ids.shape)\n",
    "        num_voxels_list.append(x_test[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = x_test[0].shape[-1]\n",
    "        voxels[f'subj{s:02d}'] = x_train\n",
    "    elif s < 12:\n",
    "        train_url = \"\"\n",
    "        test_url = \"\"\n",
    "        betas, images, _, _ = utils.load_imageryrf(subject=int(s-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=False, split=True)\n",
    "        betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "        betas = betas.to(\"cpu\").to(data_type)\n",
    "        num_voxels_list.append(betas[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        num_nan_values = torch.sum(torch.isnan(betas))\n",
    "        print(\"Number of NaN values in betas:\", num_nan_values.item())\n",
    "        indices = torch.randperm(len(betas))\n",
    "        shuffled_betas = betas[indices]\n",
    "        shuffled_images = images[indices]\n",
    "        train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        \n",
    "        \n",
    "    # elif s < 15:\n",
    "    #     betas, images = utils.load_imageryrf(subject=int(s-11), mode=\"imagery\", mask=True, stimtype=\"object\", average=False, nest=False)\n",
    "    #     betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    #     betas = betas.to(\"cpu\").to(data_type)\n",
    "    #     num_voxels_list.append(betas[0].shape[-1])\n",
    "    #     num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        \n",
    "    #     indices = torch.randperm(len(betas))\n",
    "    #     shuffled_betas = betas[indices]\n",
    "    #     shuffled_images = images[indices]\n",
    "    #     train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "    #     train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    print(f\"num_voxels for subj{s:02d}: {num_voxels[f'subj{s:02d}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject (doesn't support ImageryRF)\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "if subj < 9:\n",
    "    test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "else:\n",
    "    _, _, betas, images = utils.load_imageryrf(subject=int(subj-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "    num_test = len(betas)\n",
    "    betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    betas = betas.to(\"cpu\").to(data_type)\n",
    "    num_nan_values = torch.sum(torch.isnan(betas))\n",
    "    print(\"Number of NaN values in test betas:\", num_nan_values.item())\n",
    "    test_data = torch.utils.data.TensorDataset(betas, images)\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")\n",
    "\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! (73000, 3, 224, 224)\n",
      "Loaded all 73k NSD captions to cpu! (73000,)\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)\n",
    "\n",
    "# Load 73k NSD captions\n",
    "captions = np.load(f'{data_path}/preprocessed_data/annots_73k.npy')\n",
    "print(\"Loaded all 73k NSD captions to cpu!\", captions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168051b",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f46cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    # Create a mapping from the old layer names to the new layer names\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a17b31",
   "metadata": {},
   "source": [
    "### VD/CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructor: Loading model... fp16: True\n",
      "\n",
      "#######################\n",
      "# Running in eps mode #\n",
      "#######################\n",
      "\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Load pth from ../cache/kl-f8.pth\n",
      "Load autoencoderkl with total 83653863 parameters,72921.759 parameter sum.\n",
      "Load optimus_bert_connector with total 109489920 parameters,19325.272 parameter sum.\n",
      "Load optimus_gpt2_connector with total 132109824 parameters,18600.700 parameter sum.\n",
      "Load pth from ../cache/optimus-vae.pth\n",
      "Load optimus_vae_next with total 241599744 parameters,-344611.688 parameter sum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load clip_image_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load clip_text_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load openai_unet_2d_next with total 859520964 parameters,99914.823 parameter sum.\n",
      "Load openai_unet_0d_next with total 1706797888 parameters,250071.939 parameter sum.\n",
      "Load vd_v2_0 with total 3746805485 parameters,206311.852 parameter sum.\n"
     ]
    }
   ],
   "source": [
    "clip_emb_dim = 768\n",
    "clip_seq_dim = 257\n",
    "clip_text_seq_dim=77\n",
    "clip_extractor = Reconstructor(device=device, cache_dir=cache_dir)\n",
    "clip_variant = \"ViT-L-14_2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1931e63",
   "metadata": {},
   "source": [
    "# Diffusion Prior Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ccd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    class MindEyeModule(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MindEyeModule, self).__init__()\n",
    "        def forward(self, x):\n",
    "            return x\n",
    "            \n",
    "    dp_model = MindEyeModule()\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 64\n",
    "    heads = clip_emb_dim//64 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = DiffusionPriorUNet(\n",
    "        cond_dim=out_dim, \n",
    "        dropout=0.1)\n",
    "    \n",
    "    dp_model.diffusion_prior = DiffusionPrior(\n",
    "        prior_network,\n",
    "        device=device,\n",
    "    )\n",
    "    if dual_guidance:\n",
    "        prior_network_txt = DiffusionPriorUNet(\n",
    "        cond_dim=out_dim, \n",
    "        dropout=0.1)\n",
    "    \n",
    "\n",
    "        dp_model.diffusion_prior_txt = DiffusionPrior(\n",
    "        prior_network,\n",
    "        device=device,\n",
    "    )\n",
    "    #     utils.count_params(dp_model.diffusion_prior_txt)\n",
    "    # utils.count_params(dp_model.diffusion_prior)\n",
    "    # num_params = utils.count_params(dp_model)\n",
    "    dp_model.to(device)\n",
    "    # for name, param in dp_model.named_parameters():\n",
    "        # print(\"DP Model Dtype:\", param.dtype)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2e145",
   "metadata": {},
   "source": [
    "# Creating block of CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb00346",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{data_path}/preprocessed_data/{clip_variant}_image_embeddings.pt\"\n",
    "emb_batch_size = 50\n",
    "if not os.path.exists(file_path):\n",
    "    # Generate CLIP Image embeddings\n",
    "    print(\"Generating CLIP Image embeddings!\")\n",
    "    clip_image = torch.zeros((len(images), clip_seq_dim * clip_emb_dim)).to(\"cpu\")\n",
    "    for i in tqdm(range(len(images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "        batch_images = images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "        batch_embeddings = clip_extractor.embed_image(torch.from_numpy(batch_images)).reshape(emb_batch_size, -1).detach().to(\"cpu\")\n",
    "        clip_image[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = batch_embeddings\n",
    "\n",
    "    torch.save(clip_image, file_path)\n",
    "else:\n",
    "    clip_image = torch.load(file_path)\n",
    "\n",
    "if dual_guidance:\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/{clip_variant}_text_embeddings.pt\"\n",
    "    if not os.path.exists(file_path_txt):\n",
    "        # Generate CLIP Text embeddings\n",
    "        print(\"Generating CLIP Text embeddings!\")\n",
    "        clip_text = torch.zeros((len(captions), clip_text_seq_dim * clip_emb_dim)).to(\"cpu\")\n",
    "        for i in tqdm(range(len(captions) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "            batch_captions = captions[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "            clip_text[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = clip_extractor.embed_text(batch_captions).reshape(emb_batch_size, -1).detach().to(\"cpu\")\n",
    "        torch.save(clip_text, file_path_txt)\n",
    "    else:\n",
    "        clip_text = torch.load(file_path_txt)\n",
    "\n",
    "if blurry_recon:\n",
    "    file_path = f\"{data_path}/preprocessed_data/autoenc_image_embeddings.pt\"\n",
    "    if not os.path.exists(file_path):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating VAE Image embeddings!\")\n",
    "        vae_image = torch.zeros((len(images), 3136)).to(\"cpu\")\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "\n",
    "            for i in tqdm(range(len(images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "                batch_images = images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "                batch_images = 2 * torch.from_numpy(batch_images).unsqueeze(0).detach().to(device=device, dtype=torch.float16) - 1\n",
    "                batch_embeddings = (autoenc.encode(batch_images).latent_dist.mode() * 0.18215).detach().to(\"cpu\").reshape(emb_batch_size, -1)\n",
    "                vae_image[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = batch_embeddings\n",
    "\n",
    "\n",
    "    else:\n",
    "        vae_image = torch.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e5daf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test images and captions for subj1! torch.Size([27000, 257, 768]) torch.Size([1000, 257, 768])\n"
     ]
    }
   ],
   "source": [
    "# Filter to only ones needed during trainin\n",
    "\n",
    "clip_image_train = torch.zeros((len(train_nsd_ids), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "clip_text_train = torch.zeros((len(train_nsd_ids), clip_text_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "vae_image_train = torch.zeros((len(train_nsd_ids), 3136)).to(\"cpu\")\n",
    "for i, idx in enumerate(train_nsd_ids):\n",
    "    clip_image_train[i] =  clip_image[idx].reshape(clip_seq_dim, clip_emb_dim)\n",
    "    clip_text_train[i] = clip_text[idx].reshape(clip_text_seq_dim, clip_emb_dim)\n",
    "    vae_image_train[i] = vae_image[idx]\n",
    "    \n",
    "clip_image_test = torch.zeros((len(test_nsd_ids), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "clip_text_test = torch.zeros((len(test_nsd_ids), clip_text_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "vae_image_test = torch.zeros((len(test_nsd_ids), 3136)).to(\"cpu\")\n",
    "for i, idx in enumerate(test_nsd_ids):\n",
    "    clip_image_test[i] =  clip_image[idx].reshape(clip_seq_dim, clip_emb_dim)\n",
    "    clip_text_test[i] = clip_text[idx].reshape(clip_text_seq_dim, clip_emb_dim)\n",
    "    vae_image_test[i] = vae_image[idx]\n",
    "print(f\"Loaded train/test images and captions for subj{subj}!\", clip_image_train.shape, clip_image_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991d756",
   "metadata": {},
   "source": [
    "# Train Ridge regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65570b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj01_40sess_hypatia_ridge_flat_dp_light model trained/loaded in 00:00:13\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model_path = f'{outdir}/ridge_image_weights.pkl'\n",
    "if not os.path.exists(model_path):\n",
    "    ridge_weights = np.zeros((clip_seq_dim * clip_emb_dim, num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "    ridge_biases = np.zeros((clip_seq_dim * clip_emb_dim)).astype(np.float32)\n",
    "    print(f\"Training Ridge CLIP Image model with alpha={ridge_weight_decay}\")\n",
    "    \n",
    "    model = Ridge(\n",
    "        alpha=ridge_weight_decay,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, clip_image_train.reshape(len(clip_image_train), -1))\n",
    "    ridge_weights = model.coef_\n",
    "    ridge_biases = model.intercept_\n",
    "    image_datadict = {\"coef\" : ridge_weights, \"intercept\" : ridge_biases}\n",
    "    # Save the regression weights\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(image_datadict, f)\n",
    "else:\n",
    "    with open(model_path, 'rb') as f:\n",
    "        image_datadict = pickle.load(f)\n",
    "    \n",
    "if dual_guidance:\n",
    "    model_path = f'{outdir}/ridge_text_weights.pkl'\n",
    "    if not os.path.exists(model_path):\n",
    "        ridge_weights_txt = np.zeros((clip_text_seq_dim * clip_emb_dim, num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "        ridge_biases_txt = np.zeros((clip_text_seq_dim * clip_emb_dim)).astype(np.float32)\n",
    "        print(f\"Training Ridge CLIP Text model with alpha={ridge_weight_decay}\")\n",
    "        model = Ridge(\n",
    "            alpha=ridge_weight_decay,\n",
    "            max_iter=50000,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        model.fit(x_train, clip_text_train.reshape(len(clip_text_train), -1))\n",
    "        ridge_weights_txt = model.coef_\n",
    "        ridge_biases_txt = model.intercept_\n",
    "        text_datadict = {\"coef\" : ridge_weights_txt, \"intercept\" : ridge_biases_txt}\n",
    "        # Save the regression weights\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(text_datadict, f)\n",
    "    else:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            text_datadict = pickle.load(f)\n",
    "            \n",
    "if blurry_recon:\n",
    "    model_path = f'{outdir}/ridge_blurry_weights.pkl'\n",
    "    if not os.path.exists(model_path):\n",
    "        ridge_weights_blurry = np.zeros((3136,num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "        ridge_biases_blurry = np.zeros((3136,)).astype(np.float32)\n",
    "        print(f\"Training Ridge Blurry recon model with alpha={ridge_weight_decay}\")\n",
    "        model = Ridge(\n",
    "            alpha=ridge_weight_decay,\n",
    "            max_iter=50000,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.fit(x_train, vae_image_train)\n",
    "        ridge_weights_blurry = model.coef_\n",
    "        ridge_biases_blurry = model.intercept_\n",
    "        blurry_datadict = {\"coef\" : ridge_weights_blurry, \"intercept\" : ridge_biases_blurry}\n",
    "        # Save the regression weights\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(blurry_datadict, f)\n",
    "    else:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            blurry_datadict = pickle.load(f)\n",
    "\n",
    "print(f\"{model_name} model trained/loaded in {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\")\n",
    "# If we arent going to train the diffusion prior, stop here:\n",
    "if not use_prior:\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a0b87",
   "metadata": {},
   "source": [
    "# Predict ridge variables for diffusion prior stage 2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08abb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training variables\n",
    "\n",
    "pred_clip_image_train = torch.zeros((len(clip_image_train), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "model = Ridge(\n",
    "    alpha=60000,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "model.coef_ = image_datadict[\"coef\"]\n",
    "model.intercept_ = image_datadict[\"intercept\"]\n",
    "pred_clip_image_train = torch.from_numpy(model.predict(x_train).reshape(-1, clip_seq_dim, clip_emb_dim))\n",
    "\n",
    "if dual_guidance:\n",
    "    pred_clip_text_train = torch.zeros((len(clip_text_train), clip_text_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = text_datadict[\"coef\"]\n",
    "    model.intercept_ = text_datadict[\"intercept\"]\n",
    "    pred_clip_text_train = torch.from_numpy(model.predict(x_train).reshape(-1, clip_text_seq_dim, clip_emb_dim))\n",
    "if blurry_recon:\n",
    "    pred_blurry_vae_train = torch.zeros((len(vae_image_train), 3136)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = blurry_datadict[\"coef\"]\n",
    "    model.intercept_ = blurry_datadict[\"intercept\"]\n",
    "    pred_blurry_vae_train = torch.from_numpy(model.predict(x_train).reshape(-1, 3136))\n",
    "    \n",
    "# normalizing preds\n",
    "# for sequence in range(clip_seq_dim):\n",
    "#     std_pred_clip_image_train = (pred_clip_image_train[:, sequence] - torch.mean(pred_clip_image_train[:, sequence],axis=0)) / torch.std(pred_clip_image_train[:, sequence],axis=0)\n",
    "#     pred_clip_image_train[:, sequence] = std_pred_clip_image_train * torch.std(clip_image_train[:, sequence],axis=0) + torch.mean(clip_image_train[:, sequence],axis=0)\n",
    "# if dual_guidance:\n",
    "#     for sequence in range(clip_text_seq_dim):\n",
    "#         std_pred_clip_text_train = (pred_clip_text_train[:, sequence] - torch.mean(pred_clip_text_train[:, sequence],axis=0)) / torch.std(pred_clip_text_train[:, sequence],axis=0)\n",
    "#         pred_clip_text_train[:, sequence] = std_pred_clip_text_train * torch.std(clip_text_train[:, sequence],axis=0) + torch.mean(clip_text_train[:, sequence],axis=0)\n",
    "# if blurry_recon:\n",
    "#     std_pred_blurry_vae_train = (pred_blurry_vae_train - torch.mean(pred_blurry_vae_train,axis=0)) / torch.std(pred_blurry_vae_train,axis=0)\n",
    "#     pred_blurry_vae_train = std_pred_blurry_vae_train * torch.std(vae_image_train,axis=0) + torch.mean(vae_image_train,axis=0)\n",
    "    \n",
    "    \n",
    "# Testing variables:\n",
    "pred_clip_image_test = torch.zeros((len(clip_image_test), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "model = Ridge(\n",
    "    alpha=60000,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "model.coef_ = image_datadict[\"coef\"]\n",
    "model.intercept_ = image_datadict[\"intercept\"]\n",
    "pred_clip_image_test = torch.from_numpy(model.predict(x_test).reshape(-1, clip_seq_dim, clip_emb_dim))\n",
    "\n",
    "if dual_guidance:\n",
    "    pred_clip_text_test = torch.zeros((len(clip_text_test), clip_text_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = text_datadict[\"coef\"]\n",
    "    model.intercept_ = text_datadict[\"intercept\"]\n",
    "    pred_clip_text_test = torch.from_numpy(model.predict(x_test).reshape(-1, clip_text_seq_dim, clip_emb_dim))\n",
    "if blurry_recon:\n",
    "    pred_blurry_vae_test = torch.zeros((len(vae_image_test), 3136)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = blurry_datadict[\"coef\"]\n",
    "    model.intercept_ = blurry_datadict[\"intercept\"]\n",
    "    pred_blurry_vae_test = torch.from_numpy(model.predict(x_test).reshape(-1, 3136))\n",
    "    \n",
    "# normalizing preds\n",
    "# for sequence in range(clip_seq_dim):\n",
    "#     std_pred_clip_image_test = (pred_clip_image_test[:, sequence] - torch.mean(pred_clip_image_test[:, sequence],axis=0)) / torch.std(pred_clip_image_test[:, sequence],axis=0)\n",
    "#     pred_clip_image_test[:, sequence] = std_pred_clip_image_test * torch.std(clip_image_train[:, sequence],axis=0) + torch.mean(clip_image_train[:, sequence],axis=0)\n",
    "# if dual_guidance:\n",
    "#     for sequence in range(clip_text_seq_dim):\n",
    "#         std_pred_clip_text_test = (pred_clip_text_test[:, sequence] - torch.mean(pred_clip_text_test[:, sequence],axis=0)) / torch.std(pred_clip_text_test[:, sequence],axis=0)\n",
    "#         pred_clip_text_test[:, sequence] = std_pred_clip_text_test * torch.std(clip_text_train[:, sequence],axis=0) + torch.mean(clip_text_train[:, sequence],axis=0)\n",
    "# if blurry_recon:\n",
    "#     std_pred_blurry_vae_test = (pred_blurry_vae_test - torch.mean(pred_blurry_vae_test,axis=0)) / torch.std(pred_blurry_vae_test,axis=0)\n",
    "#     pred_blurry_vae_test = std_pred_blurry_vae_test * torch.std(vae_image_train,axis=0) + torch.mean(vae_image_train,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40240f06",
   "metadata": {},
   "source": [
    "# Train Diffusion Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"use-prior\": use_prior,\n",
    "      \"blurry_recon\": blurry_recon,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "    #   \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"lr_scheduler_type\": lr_scheduler_type,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "      \"train_imageryrf\": train_imageryrf,\n",
    "      \"mode\": mode,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=None,\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84139cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, clip_pred=None, clip_target=None):\n",
    "        self.clip_pred = clip_pred\n",
    "        self.clip_target = clip_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clip_pred)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"clip_pred\": self.clip_pred[idx],\n",
    "            \"clip_target\": self.clip_target[idx]\n",
    "        }\n",
    "\n",
    "data_type=torch.float32\n",
    "image_dataset_train = EmbeddingDataset(\n",
    "        pred_clip_image_train.to(\"cpu\", data_type), \n",
    "        clip_image_train.to(\"cpu\", data_type))\n",
    "image_dataloader_train = torch.utils.data.DataLoader(image_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "image_dataset_test = EmbeddingDataset(\n",
    "        pred_clip_image_test.to(\"cpu\", data_type), \n",
    "        clip_image_test.to(\"cpu\", data_type))\n",
    "image_dataloader_test = torch.utils.data.DataLoader(image_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "if dual_guidance:\n",
    "    text_dataset_train = EmbeddingDataset(\n",
    "        pred_clip_text_train.to(\"cpu\", data_type),\n",
    "        clip_text_train.to(\"cpu\", data_type))\n",
    "    text_dataloader_train = torch.utils.data.DataLoader(text_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    text_dataset_test = EmbeddingDataset(\n",
    "        pred_clip_text_test.to(\"cpu\", data_type),\n",
    "        clip_text_test.to(\"cpu\", data_type))\n",
    "    text_dataloader_test = torch.utils.data.DataLoader(text_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "# opt_grouped_parameters = []\n",
    "# if use_prior:\n",
    "#     opt_grouped_parameters = [\n",
    "#         {'params': [p for n, p in dp_model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': prior_weight_decay},\n",
    "#         {'params': [p for n, p in dp_model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "#     ]\n",
    "#     if dual_guidance:\n",
    "#         opt_grouped_parameters.extend([\n",
    "#         {'params': [p for n, p in dp_model.diffusion_prior_txt.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': prior_weight_decay},\n",
    "#         {'params': [p for n, p in dp_model.diffusion_prior_txt.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#         ])\n",
    "        \n",
    "# optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "# if lr_scheduler_type == 'linear':\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#         optimizer,\n",
    "#         total_iters=int(np.floor(num_epochs*len(train_dataloader))),\n",
    "#         last_epoch=-1\n",
    "#     )\n",
    "# elif lr_scheduler_type == 'cycle':\n",
    "#     total_steps=int(np.floor(num_epochs*len(train_dataloader)))\n",
    "#     print(\"total_steps\", total_steps)\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#         optimizer, \n",
    "#         max_lr=max_lr,\n",
    "#         total_steps=total_steps,\n",
    "#         final_div_factor=1000,\n",
    "#         last_epoch=-1, pct_start=2/num_epochs\n",
    "#     )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': dp_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'test_losses': test_losses,\n",
    "        'lrs': lrs,\n",
    "        }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "# def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "#     print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "#     checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "#     state_dict = checkpoint['model_state_dict']\n",
    "#     if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "#         state_dict.pop('ridge.linears.0.weight',None)\n",
    "#     model.load_state_dict(state_dict, strict=strict)\n",
    "#     if load_epoch:\n",
    "#         globals()[\"epoch\"] = checkpoint['epoch']\n",
    "#         print(\"Epoch\",epoch)\n",
    "#     if load_optimizer:\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     if load_lr:\n",
    "#         lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "#     del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# with torch.cuda.amp.autocast(dtype=data_type):\n",
    "dp_model.diffusion_prior.train(image_dataloader_train, num_epochs=150, learning_rate=max_lr, wandb_log=wandb_log)\n",
    "dp_model.diffusion_prior_txt.train(text_dataloader_train, num_epochs=150, learning_rate=max_lr, wandb_log=wandb_log)\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a139f",
   "metadata": {},
   "source": [
    "# PASTING INFERENCE CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cedcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "reconstructor = clip_extractor\n",
    "save_raw = True\n",
    "# Load pretrained model ckpt\n",
    "tag='last'\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "try:\n",
    "    checkpoint = torch.load(f'{outdir}/{tag}.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    # dp_model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    dp_model.load_state_dict(state_dict, strict=True)\n",
    "    del checkpoint\n",
    "except: # probably ckpt is saved using deepspeed format\n",
    "    import deepspeed\n",
    "    print(\"load ckpt failed, loading deepspeed ckpt...\")\n",
    "    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "    dp_model.load_state_dict(state_dict, strict=False)\n",
    "    del state_dict\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd950bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_recons = None\n",
    "final_predcaptions = None\n",
    "final_clipvoxels = None\n",
    "final_blurryrecons = None\n",
    "raw_root = f\"/export/raid1/home/kneel027/Second-Sight/output/mental_imagery_paper_b3/{mode}/{model_name}/subject{subj}/\"\n",
    "print(\"raw_root:\", raw_root)\n",
    "recons_per_sample = 16\n",
    "data_type = torch.float16\n",
    "for rep in tqdm(range(gen_rep)):\n",
    "    seed = random.randint(0,10000000)\n",
    "    utils.seed_everything(seed = seed)\n",
    "    print(f\"seed = {seed}\")\n",
    "    # get all reconstructions    \n",
    "    # all_images = None\n",
    "    all_blurryrecons = None\n",
    "    all_recons = None\n",
    "    all_predcaptions = []\n",
    "    all_clipvoxels = None\n",
    "    \n",
    "    minibatch_size = 1\n",
    "    num_samples_per_image = 1\n",
    "    plotting = False\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "            \n",
    "            clip_voxels = pred_clip_image[idx].unsqueeze(0)\n",
    "            if dual_guidance:\n",
    "                clip_text_voxels = pred_clip_text[idx].unsqueeze(0)\n",
    "            else:\n",
    "                clip_text_voxels = None\n",
    "            print(f\"Ridge clip properties: shape {clip_voxels.shape}, {clip_text_voxels.shape}, type {clip_voxels.dtype}, {clip_text_voxels.dtype}, mean: {torch.mean(clip_voxels)}, {torch.mean(clip_text_voxels)}\")\n",
    "            # Save retrieval submodule outputs\n",
    "            if all_clipvoxels is None:\n",
    "                all_clipvoxels = clip_voxels.to('cpu')\n",
    "            else:\n",
    "                all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.to('cpu')))\n",
    "            \n",
    "            # Set defaults if diffusion prior is not enabled\n",
    "            prior_out = clip_voxels.reshape((-1, clip_seq_dim, clip_emb_dim)).to(device=device, dtype=data_type)\n",
    "            if dual_guidance:\n",
    "                prior_out_txt = clip_text_voxels.reshape((-1, clip_text_seq_dim, clip_emb_dim)).to(device=device, dtype=data_type)\n",
    "            else:\n",
    "                prior_out_txt = None\n",
    "            # Overwrite guidance variables if diffusion prior is enabled\n",
    "            if use_prior:\n",
    "                print(f\"Converted CLIP clip properties: shape {prior_out.shape}, {prior_out_txt.shape}, type {prior_out.dtype}, {prior_out_txt.dtype}, mean: {torch.mean(prior_out)}, {torch.mean(prior_out_txt)}, num_nans {torch.isnan(prior_out).sum()}, {torch.isnan(prior_out_txt).sum()}\")\n",
    "                # Feed voxels through versatile diffusion diffusion prior\n",
    "                prior_out = dp_model.diffusion_prior.generate(c_embeds=prior_out, num_inference_steps=50, guidance_scale=5.0)\n",
    "                if dual_guidance:\n",
    "                    prior_out_txt = dp_model.diffusion_prior_txt.generate(c_embeds=prior_out_txt, num_inference_steps=50, guidance_scale=5.0)\n",
    "                print(f\"Diffusion Prior clip properties: shape {prior_out.shape}, {prior_out_txt.shape}, type {prior_out.dtype}, {prior_out_txt.dtype}, mean: {torch.mean(prior_out)}, {torch.mean(prior_out_txt)}, num_nans {torch.isnan(prior_out).sum()}, {torch.isnan(prior_out_txt).sum()}\")\n",
    "            \n",
    "            if blurry_recon:\n",
    "                blurred_image = (autoenc.decode(pred_blurry_vae[idx].reshape((1,4,28,28)).half().to(device)/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                im = torch.Tensor(blurred_image)\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im.cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im.cpu()))\n",
    "                if plotting:\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(im))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "            \n",
    "            # Feed outputs through versatile diffusion\n",
    "            samples_multi = [reconstructor.reconstruct(\n",
    "                                image=transforms.ToPILImage()(torch.Tensor(blurred_image[0])),\n",
    "                                c_i=prior_out,\n",
    "                                c_t=prior_out_txt,\n",
    "                                n_samples=1,\n",
    "                                textstrength=0.4,\n",
    "                                strength=0.85,\n",
    "                                seed=seed) for _ in range(recons_per_sample)]\n",
    "            samples = utils.pick_best_recon(samples_multi, clip_voxels, clip_extractor)\n",
    "            if isinstance(samples, PIL.Image.Image):\n",
    "                samples = transforms.ToTensor()(samples)\n",
    "            samples = samples.unsqueeze(0)\n",
    "            \n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    \n",
    "            if plotting: \n",
    "                print(model_name)\n",
    "                err # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "            if save_raw:\n",
    "                # print(f\"Saving raw images to {raw_root}/{idx}/{rep}.png\")\n",
    "                os.makedirs(f\"{raw_root}/{idx}/\", exist_ok=True)\n",
    "                transforms.ToPILImage()(samples[0]).save(f\"{raw_root}/{idx}/{rep}.png\")\n",
    "                transforms.ToPILImage()(all_images[idx]).save(f\"{raw_root}/{idx}/ground_truth.png\")\n",
    "                if rep == 0:\n",
    "                    transforms.ToPILImage()(torch.Tensor(blurred_image[0]).cpu()).save(f\"{raw_root}/{idx}/low_level.png\")\n",
    "                    torch.save(clip_voxels, f\"{raw_root}/{idx}/clip_image_voxels.pt\")\n",
    "                    if dual_guidance:\n",
    "                        torch.save(clip_text_voxels, f\"{raw_root}/{idx}/clip_text_voxels.pt\")\n",
    "        # resize outputs before saving\n",
    "        imsize = 256\n",
    "        # saving\n",
    "        # print(all_recons.shape)\n",
    "        # torch.save(all_images,\"evals/all_images.pt\")\n",
    "        if final_recons is None:\n",
    "            final_recons = all_recons.unsqueeze(1)\n",
    "            # final_predcaptions = np.expand_dims(all_predcaptions, axis=1)\n",
    "            final_clipvoxels = all_clipvoxels.unsqueeze(1)\n",
    "            if blurry_recon:\n",
    "                final_blurryrecons = all_blurryrecons.unsqueeze(1)\n",
    "        else:\n",
    "            final_recons = torch.cat((final_recons, all_recons.unsqueeze(1)), dim=1)\n",
    "            # final_predcaptions = np.concatenate((final_predcaptions, np.expand_dims(all_predcaptions, axis=1)), axis=1)\n",
    "            final_clipvoxels = torch.cat((final_clipvoxels, all_clipvoxels.unsqueeze(1)), dim=1)\n",
    "            if blurry_recon:\n",
    "                final_blurryrecons = torch.cat((all_blurryrecons.unsqueeze(1),final_blurryrecons), dim = 1)\n",
    "        \n",
    "if blurry_recon:\n",
    "    torch.save(final_blurryrecons,f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}.pt\")\n",
    "torch.save(final_recons,f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\")\n",
    "# torch.save(final_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions_{mode}.pt\")\n",
    "torch.save(final_clipvoxels,f\"evals/{model_name}/{model_name}_all_clipvoxels_{mode}.pt\")\n",
    "print(f\"saved {model_name} mi outputs!\")\n",
    "\n",
    "# if not utils.is_interactive():\n",
    "#     sys.exit(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
