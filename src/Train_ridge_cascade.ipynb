{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from versatile_diffusion import Reconstructor\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from sklearn.linear_model import Ridge\n",
    "import pickle\n",
    "# custom functions #\n",
    "import utils\n",
    "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588554b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: subj01_40sess_hypatia_ridge_sc\n",
      "--data_path=../dataset/                     --cache_dir=../cache/                     --model_name=subj01_40sess_hypatia_ridge_sc                     --batch_size=64                     --no-multi_subject --subj=1 --num_sessions=40                     --dual_guidance\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"subj01_40sess_hypatia_ridge_sc\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path=../dataset/ \\\n",
    "                    --cache_dir=../cache/ \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --batch_size=64 \\\n",
    "                    --no-multi_subject --subj=1 --num_sessions=40 \\\n",
    "                    --dual_guidance\"\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 40\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=1,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires much more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",type=float,default=60000,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_imageryrf\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the ImageryRF dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_nsd\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Don't use the Natural Scenes Dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr_threshold\",type=float,default=-1.0,\n",
    "    help=\"Used for calculating SNR on a whole brain to narrow down voxels.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"all\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    if train_imageryrf:\n",
    "            # 9,10,11 is ImageryRF subjects\n",
    "        if no_nsd:\n",
    "            subj_list = np.arange(9,12)\n",
    "        else:\n",
    "            subj_list = np.arange(1,12)\n",
    "    else:\n",
    "        subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4743c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3964338",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d321316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 64 num_iterations_per_epoch = 468 num_samples_per_epoch = 30000\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "num_devices = 1\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 40 sessions\n",
      "../dataset//wds/subj01/train/{0..39}.tar\n",
      "torch.Size([27000, 15724]) (27000,)\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "../dataset//wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n",
      "currently using 1 seq_len (chose 0 past behav and 0 future behav)\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    # If an NSD subject\n",
    "    if s < 9:\n",
    "        if multi_subject:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "        else:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "        print(train_url)\n",
    "        \n",
    "        train_data[f'subj{s:02d}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                            .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                            .decode(\"torch\")\\\n",
    "                            .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                            .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        betas = utils.create_snr_betas(subject=s, data_type=torch.float16, data_path=data_path, threshold = snr_threshold)\n",
    "        x_train, valid_nsd_ids_train, x_test, test_nsd_ids = utils.load_nsd(subject=s, betas=betas, data_path=data_path)\n",
    "        print(x_train.shape, valid_nsd_ids_train.shape)\n",
    "        num_voxels_list.append(x_train[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = x_train[0].shape[-1]\n",
    "        voxels[f'subj{s:02d}'] = x_train\n",
    "    elif s < 12:\n",
    "        train_url = \"\"\n",
    "        test_url = \"\"\n",
    "        betas, images, _, _ = utils.load_imageryrf(subject=int(s-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=False, split=True)\n",
    "        betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "        betas = betas.to(\"cpu\").to(torch.float16)\n",
    "        num_voxels_list.append(betas[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        num_nan_values = torch.sum(torch.isnan(betas))\n",
    "        print(\"Number of NaN values in betas:\", num_nan_values.item())\n",
    "        indices = torch.randperm(len(betas))\n",
    "        shuffled_betas = betas[indices]\n",
    "        shuffled_images = images[indices]\n",
    "        train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        \n",
    "        \n",
    "    # elif s < 15:\n",
    "    #     betas, images = utils.load_imageryrf(subject=int(s-11), mode=\"imagery\", mask=True, stimtype=\"object\", average=False, nest=False)\n",
    "    #     betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    #     betas = betas.to(\"cpu\").to(data_type)\n",
    "    #     num_voxels_list.append(betas[0].shape[-1])\n",
    "    #     num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        \n",
    "    #     indices = torch.randperm(len(betas))\n",
    "    #     shuffled_betas = betas[indices]\n",
    "    #     shuffled_images = images[indices]\n",
    "    #     train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "    #     train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    print(f\"num_voxels for subj{s:02d}: {num_voxels[f'subj{s:02d}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject (doesn't support ImageryRF)\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "if subj < 9:\n",
    "    test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "else:\n",
    "    _, _, betas, images = utils.load_imageryrf(subject=int(subj-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "    num_test = len(betas)\n",
    "    betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    betas = betas.to(\"cpu\").to(torch.float16)\n",
    "    num_nan_values = torch.sum(torch.isnan(betas))\n",
    "    print(\"Number of NaN values in test betas:\", num_nan_values.item())\n",
    "    test_data = torch.utils.data.TensorDataset(betas, images)\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")\n",
    "\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! (73000, 3, 224, 224)\n",
      "Loaded all 73k NSD captions to cpu! (73000,)\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)\n",
    "\n",
    "# Load 73k NSD captions\n",
    "captions = np.load(f'{data_path}/preprocessed_data/annots_73k.npy')\n",
    "print(\"Loaded all 73k NSD captions to cpu!\", captions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168051b",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f46cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    # Create a mapping from the old layer names to the new layer names\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a17b31",
   "metadata": {},
   "source": [
    "### Feature extractor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6828ebb3b564c1f87ade118d3f49b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc_prior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", variant=\"bf16\", torch_dtype=torch.bfloat16, cache_dir=\"../cache\").to(device)\n",
    "image_embedding_variant = \"sc_latent\"\n",
    "clip_emb_dim = 768\n",
    "clip_seq_dim = 1\n",
    "text_embedding_variant = \"sc_text\"\n",
    "clip_text_seq_dim=1\n",
    "clip_text_emb_dim=1280\n",
    "text_embedding_variant2 = \"sc_text_pooled\"\n",
    "clip_text_seq_dim2=77\n",
    "clip_text_emb_dim2=1280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2e145",
   "metadata": {},
   "source": [
    "# Creating block of CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bb00346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Image embeddings!\n",
      "1 768\n",
      "torch.Size([73000, 1, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images...:   0%|          | 0/1460 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images...:   6%|▋         | 93/1460 [03:38<53:38,  2.35s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m emb_batch_size), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding images...\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     batch_images \u001b[38;5;241m=\u001b[39m images[i \u001b[38;5;241m*\u001b[39m emb_batch_size:i \u001b[38;5;241m*\u001b[39m emb_batch_size \u001b[38;5;241m+\u001b[39m emb_batch_size]\n\u001b[0;32m---> 11\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43msc_prior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# print(batch_embeddings.shape, batch_embeddings.unsqueeze(1).shape)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     clip_image[i \u001b[38;5;241m*\u001b[39m emb_batch_size:i \u001b[38;5;241m*\u001b[39m emb_batch_size \u001b[38;5;241m+\u001b[39m emb_batch_size] \u001b[38;5;241m=\u001b[39m batch_embeddings\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/site-packages/diffusers/pipelines/stable_cascade/pipeline_stable_cascade_prior.py:255\u001b[0m, in \u001b[0;36mStableCascadePriorPipeline.encode_image\u001b[0;34m(self, images, device, dtype, batch_size, num_images_per_prompt)\u001b[0m\n\u001b[1;32m    253\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m--> 255\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpixel_values\n\u001b[1;32m    256\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    257\u001b[0m     image_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_encoder(image)\u001b[38;5;241m.\u001b[39mimage_embeds\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/site-packages/transformers/image_processing_utils.py:552\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:286\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     images \u001b[38;5;241m=\u001b[39m [convert_to_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mto_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scaled_image(images[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m do_rescale:\n\u001b[1;32m    289\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:286\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    283\u001b[0m     images \u001b[38;5;241m=\u001b[39m [convert_to_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[43mto_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scaled_image(images[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m do_rescale:\n\u001b[1;32m    289\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/site-packages/transformers/image_utils.py:169\u001b[0m, in \u001b[0;36mto_numpy_array\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_numpy_array\u001b[39m(img) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_valid_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_vision_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/site-packages/transformers/image_utils.py:92\u001b[0m, in \u001b[0;36mis_valid_image\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_image\u001b[39m(img):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 92\u001b[0m         (\u001b[43mis_vision_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage))\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_torch_tensor(img)\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_tf_tensor(img)\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_jax_tensor(img)\n\u001b[1;32m     97\u001b[0m     )\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/site-packages/transformers/utils/import_utils.py:735\u001b[0m, in \u001b[0;36mis_vision_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pil_available:\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 735\u001b[0m         package_version \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPillow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/importlib/metadata/__init__.py:1009\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/importlib/metadata/__init__.py:982\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    977\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/importlib/metadata/__init__.py:563\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA distribution name is required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/importlib/metadata/__init__.py:915\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find metadata directories in paths heuristically.\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m prepared \u001b[38;5;241m=\u001b[39m Prepared(name)\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m--> 915\u001b[0m     \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(FastPath, paths)\n\u001b[1;32m    916\u001b[0m )\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/importlib/metadata/__init__.py:813\u001b[0m, in \u001b[0;36mFastPath.search\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtime\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msearch(name)\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/importlib/metadata/__init__.py:823\u001b[0m, in \u001b[0;36mFastPath.lookup\u001b[0;34m(self, mtime)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;129m@method_cache\u001b[39m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlookup\u001b[39m(\u001b[38;5;28mself\u001b[39m, mtime):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLookup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/importlib/metadata/__init__.py:833\u001b[0m, in \u001b[0;36mLookup.__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfos \u001b[38;5;241m=\u001b[39m FreezableDefaultDict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meggs \u001b[38;5;241m=\u001b[39m FreezableDefaultDict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m--> 833\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    834\u001b[0m     low \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m low\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.dist-info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.egg-info\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;66;03m# rpartition is faster than splitext and suitable for this purpose.\u001b[39;00m\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd2/lib/python3.11/importlib/metadata/__init__.py:800\u001b[0m, in \u001b[0;36mFastPath.children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 800\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip_children()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = f\"{data_path}/preprocessed_data/{image_embedding_variant}_image_embeddings.pt\"\n",
    "emb_batch_size = 50\n",
    "if not os.path.exists(file_path):\n",
    "    # Generate CLIP Image embeddings\n",
    "    print(\"Generating Image embeddings!\")\n",
    "    print(clip_seq_dim, clip_emb_dim)\n",
    "    clip_image = torch.zeros((len(images), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    print(clip_image.shape)\n",
    "    for i in tqdm(range(len(images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "        batch_images = images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "        batch_embeddings = sc_prior.encode_image(torch.from_numpy(batch_images),  device, torch.bfloat16, 1, 1)[0].detach().to(\"cpu\")\n",
    "        # print(batch_embeddings.shape, batch_embeddings.unsqueeze(1).shape)\n",
    "        clip_image[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = batch_embeddings.unsqueeze(2)\n",
    "\n",
    "    torch.save(clip_image, file_path)\n",
    "else:\n",
    "    clip_image = torch.load(file_path)\n",
    "clip_image_train = torch.zeros((len(valid_nsd_ids_train), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "for i, idx in enumerate(valid_nsd_ids_train):\n",
    "    clip_image_train[i] = clip_image[idx]\n",
    "file_path = f\"{data_path}/preprocessed_data/subject{subj}/{image_embedding_variant}_image_embeddings_train.pt\"\n",
    "if not os.path.exists(file_path):\n",
    "    torch.save(clip_image_train, file_path)\n",
    "        \n",
    "if dual_guidance:\n",
    "    emb_batch_size = 1\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/{text_embedding_variant}_text_embeddings.pt\"\n",
    "    file_path_txt2 = f\"{data_path}/preprocessed_data/{text_embedding_variant2}_text_embeddings.pt\"\n",
    "    if not os.path.exists(file_path_txt) or not os.path.exists(file_path_txt2):\n",
    "        # Generate CLIP Text embeddings\n",
    "        print(\"Generating Text embeddings!\")\n",
    "        clip_text = torch.zeros((len(captions), clip_text_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "        clip_text2 = torch.zeros((len(captions), clip_text_seq_dim2, clip_emb_dim2)).to(\"cpu\")\n",
    "        for i in tqdm(range(len(captions) // emb_batch_size), desc=\"Encoding captions...\"):\n",
    "            batch_captions = captions[i * emb_batch_size:i * emb_batch_size + emb_batch_size].tolist()\n",
    "            text_embed, pooled_text_embed, _, _ = sc_prior.encode_prompt(batch_captions[0], device=device, num_images_per_prompt=1, batch_size=1, do_classifier_free_guidance=False)\n",
    "            clip_text[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = text_embed.detach().to(\"cpu\")\n",
    "            clip_text2[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = pooled_text_embed.detach().to(\"cpu\")\n",
    "        torch.save(clip_text, file_path_txt)\n",
    "        torch.save(clip_text2, file_path_txt2)\n",
    "    else:\n",
    "        clip_text = torch.load(file_path_txt)\n",
    "        clip_text2 = torch.load(file_path_txt2)\n",
    "    clip_text_train = torch.zeros((len(valid_nsd_ids_train), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "    clip_text_train2 = torch.zeros((len(valid_nsd_ids_train), clip_text_seq_dim2, clip_text_emb_dim2)).to(\"cpu\")\n",
    "    for i, idx in enumerate(valid_nsd_ids_train):\n",
    "        clip_text_train[i] = clip_text[idx]\n",
    "        clip_text_train2[i] = clip_text2[idx]\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{text_embedding_variant}_text_embeddings_train.pt\"\n",
    "    file_path_txt2 = f\"{data_path}/preprocessed_data/subject{subj}/{text_embedding_variant2}_text_embeddings_train.pt\"\n",
    "    if not os.path.exists(file_path_txt) or not os.path.exists(file_path_txt2):\n",
    "        torch.save(clip_text_train, file_path_txt)\n",
    "        torch.save(clip_text_train2, file_path_txt2)\n",
    "\n",
    "if blurry_recon:\n",
    "    file_path = f\"{data_path}/preprocessed_data/autoenc_image_embeddings.pt\"\n",
    "    if not os.path.exists(file_path):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating VAE Image embeddings!\")\n",
    "        vae_image = torch.zeros((len(images), 3136)).to(\"cpu\")\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "\n",
    "            for i in tqdm(range(len(images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "                batch_images = images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "                batch_images = 2 * torch.from_numpy(batch_images).unsqueeze(0).detach().to(device=device, dtype=torch.float16) - 1\n",
    "                batch_embeddings = (autoenc.encode(batch_images).latent_dist.mode() * 0.18215).detach().to(\"cpu\").reshape(emb_batch_size, -1)\n",
    "                vae_image[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = batch_embeddings\n",
    "\n",
    "\n",
    "    else:\n",
    "        vae_image = torch.load(file_path)\n",
    "    vae_image_train = torch.zeros((len(valid_nsd_ids_train), 3136)).to(\"cpu\")\n",
    "    for i, idx in enumerate(valid_nsd_ids_train):\n",
    "        vae_image_train[i] = vae_image[idx]\n",
    "    file_path_vae = f\"{data_path}/preprocessed_data/subject{subj}/autoenc_image_embeddings_train.pt\"\n",
    "    if not os.path.exists(file_path_vae):\n",
    "        torch.save(vae_image_train, file_path_vae)\n",
    "print(f\"Loaded train image clip and text clip for subj{subj}!\", clip_image_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991d756",
   "metadata": {},
   "source": [
    "# Train Ridge regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65570b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge CLIP Text model with alpha=60000\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ridge_weights = np.zeros((clip_seq_dim * clip_emb_dim, num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "ridge_biases = np.zeros((clip_seq_dim * clip_emb_dim)).astype(np.float32)\n",
    "print(f\"Training Ridge Image model with alpha={weight_decay}\")\n",
    "model = Ridge(\n",
    "    alpha=weight_decay,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.fit(x_train, clip_image_train.reshape(len(clip_image_train), -1))\n",
    "ridge_weights = model.coef_\n",
    "ridge_biases = model.intercept_\n",
    "datadict = {\"coef\" : ridge_weights, \"intercept\" : ridge_biases}\n",
    "# Save the regression weights\n",
    "with open(f'{outdir}/ridge_image_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(datadict, f)\n",
    "    \n",
    "if dual_guidance:\n",
    "    ridge_weights_txt = np.zeros((clip_text_seq_dim * clip_text_emb_dim, num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "    ridge_biases_txt = np.zeros((clip_text_seq_dim * clip_text_emb_dim)).astype(np.float32)\n",
    "    print(f\"Training Ridge Text model with alpha={weight_decay}\")\n",
    "    model = Ridge(\n",
    "        alpha=weight_decay,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, clip_text_train.reshape(len(clip_text_train), -1))\n",
    "    ridge_weights_txt = model.coef_\n",
    "    ridge_biases_txt = model.intercept_\n",
    "    datadict = {\"coef\" : ridge_weights_txt, \"intercept\" : ridge_biases_txt}\n",
    "    # Save the regression weights\n",
    "    with open(f'{outdir}/ridge_text_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(datadict, f)\n",
    "        \n",
    "    ridge_weights_txt2 = np.zeros((clip_text_seq_dim2 * clip_emb_dim, num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "    ridge_biases_txt2 = np.zeros((clip_text_seq_dim2 * clip_text_emb_di2m)).astype(np.float32)\n",
    "    print(f\"Training Ridge Text2 model with alpha={weight_decay}\")\n",
    "    model = Ridge(\n",
    "        alpha=weight_decay,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(x_train, clip_text_train2.reshape(len(clip_text_train), -1))\n",
    "    ridge_weights_txt2 = model.coef_\n",
    "    ridge_biases_txt2 = model.intercept_\n",
    "    datadict = {\"coef\" : ridge_weights_txt2, \"intercept\" : ridge_biases_txt2}\n",
    "    # Save the regression weights\n",
    "    with open(f'{outdir}/ridge_text2_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(datadict, f)\n",
    "            \n",
    "if blurry_recon:\n",
    "    ridge_weights_blurry = np.zeros((3136,num_voxels[f'subj{s:02d}'])).astype(np.float32)\n",
    "    ridge_biases_blurry = np.zeros((3136,)).astype(np.float32)\n",
    "    print(f\"Training Ridge Blurry recon model with alpha={weight_decay}\")\n",
    "    model = Ridge(\n",
    "        alpha=weight_decay,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(x_train, vae_image_train)\n",
    "    ridge_weights_blurry = model.coef_\n",
    "    ridge_biases_blurry = model.intercept_\n",
    "    datadict = {\"coef\" : ridge_weights_blurry, \"intercept\" : ridge_biases_blurry}\n",
    "    # Save the regression weights\n",
    "    with open(f'{outdir}/ridge_blurry_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(datadict, f)\n",
    "\n",
    "\n",
    "print(f\"Elapsed training time for {model_name}: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
