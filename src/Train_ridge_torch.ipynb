{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from models import Clipper\n",
    "from versatile_diffusion import Reconstructor\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder # bigG embedder\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "from sklearn.linear_model import Ridge\n",
    "import pickle\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd260e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 8\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]\n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588554b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: testing\n",
      "--data_path=../dataset/                     --cache_dir=../cache/                     --model_name=testing                     --batch_size=64                     --no-multi_subject --subj=1 --num_sessions=40                     --hidden_dim=1024 --clip_scale=1.                     --no-blurry_recon --blur_scale=.5                      --seq_past=0 --seq_future=0                     --no-use_prior --prior_scale=30                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=150 --no-use_image_aug                     --ckpt_interval=1 --ckpt_saving\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"testing\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path=../dataset/ \\\n",
    "                    --cache_dir=../cache/ \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --batch_size=64 \\\n",
    "                    --no-multi_subject --subj=1 --num_sessions=40 \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --no-blurry_recon --blur_scale=.5  \\\n",
    "                    --seq_past=0 --seq_future=0 \\\n",
    "                    --no-use_prior --prior_scale=30 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=150 --no-use_image_aug \\\n",
    "                    --ckpt_interval=1 --ckpt_saving\"\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 40\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=40,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires much more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"mindeye_imagery\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",type=float,default=1e-2,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_imageryrf\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the ImageryRF dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_nsd\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Don't use the Natural Scenes Dataset for pretraining\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr_threshold\",type=float,default=-1.0,\n",
    "    help=\"Used for calculating SNR on a whole brain to narrow down voxels.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"all\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    if train_imageryrf:\n",
    "            # 9,10,11 is ImageryRF subjects\n",
    "        if no_nsd:\n",
    "            subj_list = np.arange(9,12)\n",
    "        else:\n",
    "            subj_list = np.arange(1,12)\n",
    "    else:\n",
    "        subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4743c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3964338",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d321316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 64 num_iterations_per_epoch = 468 num_samples_per_epoch = 30000\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "num_devices = 1\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 40 sessions\n",
      "../dataset//wds/subj01/train/{0..39}.tar\n",
      "torch.Size([27000, 15724]) (27000,)\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "../dataset//wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n",
      "currently using 1 seq_len (chose 0 past behav and 0 future behav)\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    # If an NSD subject\n",
    "    if s < 9:\n",
    "        if multi_subject:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "        else:\n",
    "            train_url = f\"{data_path}/wds/subj{s:02d}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "        print(train_url)\n",
    "        \n",
    "        train_data[f'subj{s:02d}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                            .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                            .decode(\"torch\")\\\n",
    "                            .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                            .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "        betas = utils.create_snr_betas(subject=s, data_type=data_type, data_path=data_path, threshold = snr_threshold)\n",
    "        num_voxels_list.append(betas[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        voxels[f'subj{s:02d}'] = betas\n",
    "    elif s < 12:\n",
    "        train_url = \"\"\n",
    "        test_url = \"\"\n",
    "        betas, images, _, _ = utils.load_imageryrf(subject=int(s-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=False, split=True)\n",
    "        betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "        betas = betas.to(\"cpu\").to(data_type)\n",
    "        num_voxels_list.append(betas[0].shape[-1])\n",
    "        num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        num_nan_values = torch.sum(torch.isnan(betas))\n",
    "        print(\"Number of NaN values in betas:\", num_nan_values.item())\n",
    "        indices = torch.randperm(len(betas))\n",
    "        shuffled_betas = betas[indices]\n",
    "        shuffled_images = images[indices]\n",
    "        train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "        train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "        \n",
    "        \n",
    "    # elif s < 15:\n",
    "    #     betas, images = utils.load_imageryrf(subject=int(s-11), mode=\"imagery\", mask=True, stimtype=\"object\", average=False, nest=False)\n",
    "    #     betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    #     betas = betas.to(\"cpu\").to(data_type)\n",
    "    #     num_voxels_list.append(betas[0].shape[-1])\n",
    "    #     num_voxels[f'subj{s:02d}'] = betas[0].shape[-1]\n",
    "        \n",
    "    #     indices = torch.randperm(len(betas))\n",
    "    #     shuffled_betas = betas[indices]\n",
    "    #     shuffled_images = images[indices]\n",
    "    #     train_data[f'subj{s:02d}'] = torch.utils.data.TensorDataset(shuffled_betas, shuffled_images)\n",
    "    #     train_dl[f'subj{s:02d}'] = torch.utils.data.DataLoader(train_data[f'subj{s:02d}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    print(f\"num_voxels for subj{s:02d}: {num_voxels[f'subj{s:02d}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject (doesn't support ImageryRF)\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "if subj < 9:\n",
    "    test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "else:\n",
    "    _, _, betas, images = utils.load_imageryrf(subject=int(subj-8), mode=mode, mask=True, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "    num_test = len(betas)\n",
    "    betas = torch.where(torch.isnan(betas), torch.zeros_like(betas), betas)\n",
    "    betas = betas.to(\"cpu\").to(data_type)\n",
    "    num_nan_values = torch.sum(torch.isnan(betas))\n",
    "    print(\"Number of NaN values in test betas:\", num_nan_values.item())\n",
    "    test_data = torch.utils.data.TensorDataset(betas, images)\n",
    "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")\n",
    "\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! (73000, 3, 224, 224)\n",
      "27000\n",
      "Loaded train images for subj1! torch.Size([27000, 3, 224, 224])\n",
      "Loaded all 73k NSD captions to cpu! (73000,)\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'] # if you go OOM you can remove the [:] so it isnt preloaded to cpu! (will require a few edits elsewhere tho)\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)\n",
    "\n",
    "# Load 73k NSD captions\n",
    "captions = np.load(f'{data_path}/preprocessed_data/annots_73k.npy')\n",
    "print(\"Loaded all 73k NSD captions to cpu!\", captions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168051b",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f46cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    # Create a mapping from the old layer names to the new layer names\n",
    "    layer_mapping = {\n",
    "        \"encoder.mid_block.attentions.0.to_q.weight\": \"encoder.mid_block.attentions.0.query.weight\",\n",
    "        \"encoder.mid_block.attentions.0.to_q.bias\": \"encoder.mid_block.attentions.0.query.bias\",\n",
    "        \"encoder.mid_block.attentions.0.to_k.weight\": \"encoder.mid_block.attentions.0.key.weight\",\n",
    "        \"encoder.mid_block.attentions.0.to_k.bias\": \"encoder.mid_block.attentions.0.key.bias\",\n",
    "        \"encoder.mid_block.attentions.0.to_v.weight\": \"encoder.mid_block.attentions.0.value.weight\",\n",
    "        \"encoder.mid_block.attentions.0.to_v.bias\": \"encoder.mid_block.attentions.0.value.bias\",\n",
    "        \"encoder.mid_block.attentions.0.to_out.0.weight\": \"encoder.mid_block.attentions.0.proj_attn.weight\",\n",
    "        \"encoder.mid_block.attentions.0.to_out.0.bias\": \"encoder.mid_block.attentions.0.proj_attn.bias\",\n",
    "        \"decoder.mid_block.attentions.0.to_q.weight\": \"decoder.mid_block.attentions.0.query.weight\",\n",
    "        \"decoder.mid_block.attentions.0.to_q.bias\": \"decoder.mid_block.attentions.0.query.bias\",\n",
    "        \"decoder.mid_block.attentions.0.to_k.weight\": \"decoder.mid_block.attentions.0.key.weight\",\n",
    "        \"decoder.mid_block.attentions.0.to_k.bias\": \"decoder.mid_block.attentions.0.key.bias\",\n",
    "        \"decoder.mid_block.attentions.0.to_v.weight\": \"decoder.mid_block.attentions.0.value.weight\",\n",
    "        \"decoder.mid_block.attentions.0.to_v.bias\": \"decoder.mid_block.attentions.0.value.bias\",\n",
    "        \"decoder.mid_block.attentions.0.to_out.0.weight\": \"decoder.mid_block.attentions.0.proj_attn.weight\",\n",
    "        \"decoder.mid_block.attentions.0.to_out.0.bias\": \"decoder.mid_block.attentions.0.proj_attn.bias\"\n",
    "    }\n",
    "\n",
    "    # Create a new state dictionary with the renamed layers\n",
    "    new_ckpt = {}\n",
    "    for old_key, value in ckpt.items():\n",
    "        new_key = layer_mapping.get(old_key, old_key)  # Get the new key, or use the old key if not in mapping\n",
    "        new_ckpt[new_key] = value\n",
    "    autoenc.load_state_dict(new_ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a17b31",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Clipper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_turbo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clip_emb_dim = 768\n",
    "clip_seq_dim = 257\n",
    "clip_text_seq_dim=77\n",
    "clip_extractor = Reconstructor(device=device, cache_dir=cache_dir)\n",
    "clip_variant = \"ViT-L-14\"\n",
    "output_dims = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2e145",
   "metadata": {},
   "source": [
    "# Creating block of CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb00346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [03:52<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"{data_path}/preprocessed_data/{clip_variant}_image_embeddings.pt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # Generate CLIP Image embeddings\n",
    "    print(\"Generating CLIP Image embeddings!\")\n",
    "    clip_image = torch.zeros((len(images), clip_seq_dim * clip_emb_dim)).to(\"cpu\")\n",
    "    for i in tqdm(range(0, len(images)), desc=\"Encoding images...\"):\n",
    "        clip_image[i] = clip_extractor.embed_image(torch.from_numpy(images[i]).unsqueeze(0)).flatten().detach().to(\"cpu\")\n",
    "    torch.save(clip_image, file_path)\n",
    "else:\n",
    "    clip_image = torch.load(file_path)\n",
    "output_dims[\"image\"] = int(clip_image.shape[-1])\n",
    "    \n",
    "if dual_guidance:\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/{clip_variant}_text_embeddings.pt\"\n",
    "    if not os.path.exists(file_path_txt):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating CLIP Text embeddings!\")\n",
    "        clip_text = torch.zeros((len(captions), clip_text_seq_dim * clip_emb_dim)).to(\"cpu\")\n",
    "        for i in tqdm(range(0, len(captions)), desc=\"Encoding captions...\"):\n",
    "            clip_text[i] = clip_extractor.embed_text(str(captions[i])).flatten().detach().to(\"cpu\")\n",
    "        torch.save(clip_text, file_path_txt)\n",
    "    else:\n",
    "        clip_text = torch.load(file_path_txt)\n",
    "    output_dims[\"text\"] = int(clip_text.shape[-1])\n",
    "    \n",
    "if blurry_recon:\n",
    "    file_path = f\"{data_path}/preprocessed_data/autoenc_image_embeddings.pt\"\n",
    "    if not os.path.exists(file_path):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating VAE Image embeddings!\")\n",
    "        vae_image = torch.zeros((len(images), 3136)).to(\"cpu\")\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            for i in tqdm(range(0, len(images)), desc=\"Encoding images...\"):\n",
    "                vae_image[i] = (autoenc.encode(2*torch.from_numpy(images[i]).unsqueeze(0).detach().to(device=device, dtype=torch.float16)-1).latent_dist.mode() * 0.18215).detach().to(\"cpu\").flatten()\n",
    "            torch.save(vae_image, file_path)\n",
    "    else:\n",
    "        vae_image = torch.load(file_path)\n",
    "    output_dims[\"blurry\"] = int(vae_image.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f62b9a",
   "metadata": {},
   "source": [
    "# Initialize model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6132d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MindEye_Imagery_Ridge(nn.Module):\n",
    "    def __init__(self, input_dim, output_dims, hidden_dim=1024, dual_guidance=False, blurry_recon=False):\n",
    "        super(MindEye_Imagery_Ridge, self).__init__()\n",
    "        self.dual_guidance = dual_guidance\n",
    "        self.blurry_recon = blurry_recon\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        # Image linear layer\n",
    "        self.image_layer = nn.Linear(hidden_dim, output_dims['image'], bias=True)\n",
    "\n",
    "        # Text linear layer (if enabled)\n",
    "        if self.dual_guidance:\n",
    "            self.text_layer = nn.Linear(hidden_dim, output_dims['text'], bias=True)\n",
    "\n",
    "        # Blurry recon linear layer (if enabled)\n",
    "        if self.blurry_recon:\n",
    "            self.blurry_layer = nn.Linear(hidden_dim, output_dims['blurry'], bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        outputs = {'image': self.image_layer(x)}\n",
    "        if self.dual_guidance:\n",
    "            outputs['text'] = self.text_layer(x)\n",
    "        if self.blurry_recon:\n",
    "            outputs['blurry'] = self.blurry_layer(x)\n",
    "        return outputs\n",
    "\n",
    "model = MindEye_Imagery_Ridge(num_voxels[f'subj{subj:02d}'], output_dims, hidden_dim, dual_guidance, blurry_recon)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991d756",
   "metadata": {},
   "source": [
    "# Prepare models and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65570b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27000, 15724])\n",
      "Number of NaN values in betas: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_turbo/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer with weight decay (L2 regularization)\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [{'params': [p for n, p in model.named_parameters()], 'weight_decay': weight_decay}]\n",
    "    \n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "    last_epoch=-1\n",
    ")\n",
    "\n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536cc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"use-prior\": use_prior,\n",
    "      \"blurry_recon\": blurry_recon,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "      \"train_imageryrf\": train_imageryrf,\n",
    "      \"mode\": mode,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=None,\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd0f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dls = [train_dl[f'subj{s:02d}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, *train_dls = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3493130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "test_caption = []\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    recon_cossim_txt = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    test_recon_cossim_txt = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_image_clip_total = 0.\n",
    "    loss_text_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    \n",
    "    test_loss_image_clip_total = 0.\n",
    "    test_loss_text_clip_total = 0.\n",
    "    test_loss_blurry_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    loss_prior_total_txt = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "    test_loss_prior_total_txt = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_blurry_cont_total = 0.\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    image_clip_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), clip_seq_dim * clip_emb_dim).float()\n",
    "    if dual_guidance:\n",
    "        text_clip_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), clip_text_seq_dim * clip_emb_dim).float()\n",
    "    if blurry_recon:\n",
    "        vae_image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3136).float()\n",
    "\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    # print(f\"num_iterations_per_epoch: {num_iterations_per_epoch}, batch_size: {batch_size}, len(subj_list): {len(subj_list)}\")\n",
    "    for s, (cur_subj, train_dl) in enumerate(zip(subj_list, train_dls)):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            i = 0\n",
    "            while i < num_iterations_per_epoch:\n",
    "                # print(f\"restarting data loader at i={i} for s={s}\")\n",
    "                for data in train_dl:  \n",
    "                    if cur_subj < 9:\n",
    "                        behav0, past_behav0, future_behav0, old_behav0 = data\n",
    "                        \n",
    "                        # image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                        # image_sorted_idx = behav0[:,0,0].cpu().long().numpy()\n",
    "                        # image_sorted_idx = np.unique(np.sort(image_sorted_idx))\n",
    "                        \n",
    "                        # image0 = images[image_sorted_idx]\n",
    "                        # image0 = torch.tensor(image0, dtype=torch.float16, device=\"cpu\")  # Convert to tensor\n",
    "                        # while image0.shape[0] < batch_size:\n",
    "                        #     image0 = torch.cat((image0, image0[0].unsqueeze(0)), dim=0)\n",
    "                        image_idx = behav0[:,0,0].cpu().long().numpy()\n",
    "                        local_idx, image_sorted_idx = np.unique(image_idx, return_index=True)                \n",
    "                        # if len(image0) != len(image_idx): # hdf5 cant handle duplicate indexing\n",
    "                        #     continue\n",
    "                        image0 = torch.tensor(images[local_idx], dtype=data_type)\n",
    "                        image_iters[i, s*batch_size:s*batch_size+image0.shape[0]] = image0\n",
    "                        image_clip0 = clip_image[local_idx]\n",
    "                        image_clip_iters[i, s*batch_size:s*batch_size+image_clip0.shape[0]]\n",
    "                        \n",
    "                        if dual_guidance:\n",
    "                            text_clip0 = clip_text[local_idx]\n",
    "                            text_clip_iters[i, s*batch_size:s*batch_size+text_clip0.shape[0]]\n",
    "                        if blurry_recon:\n",
    "                            vae_image0 = vae_image[local_idx]\n",
    "                            vae_image_iters[i, s*batch_size:s*batch_size+vae_image0.shape[0]]\n",
    "                        \n",
    "                        voxel_idx = behav0[:,0,5].cpu().long().numpy()\n",
    "                        voxel_sorted_idx = voxel_idx[image_sorted_idx]\n",
    "                        voxel0 = voxels[f'subj0{subj_list[s]}'][voxel_sorted_idx]\n",
    "                        voxel0 = torch.Tensor(voxel0)#.unsqueeze(1)\n",
    "                    else:\n",
    "                        voxel0, image0 = data\n",
    "                        image0 = torch.nn.functional.interpolate(image0, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                        image_iters[i,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                    \n",
    "                    if seq_len==1:\n",
    "                        voxel0 = voxel0.unsqueeze(1)\n",
    "                    else:\n",
    "                        if seq_past>0:\n",
    "                            past_behavior = past_behav0[:,:(seq_past),5].cpu().long()\n",
    "                            past_voxel0 = voxels[f'subj{subj_list[s]:02d}'][past_behavior]\n",
    "                            past_voxel0[past_behavior==-1] = voxel0[torch.where(past_behavior==-1)[0]] # replace invalid past voxels \n",
    "                            past_voxel0 = torch.Tensor(past_voxel0)\n",
    "\n",
    "                            # if shared1000, then you need to mask it out \n",
    "                            for p in range(seq_past):\n",
    "                                mask = (past_behav0[:,p,-1] == 1) # [16,] bool\n",
    "                                index = torch.nonzero(mask.cpu()).squeeze()\n",
    "                                past_voxel0[index,p,:] = torch.zeros_like(past_voxel0[index,p,:])\n",
    "\n",
    "                        if seq_future>0:\n",
    "                            future_behavior = future_behav0[:,:(seq_future),5].cpu().long()\n",
    "                            future_voxel0 = voxels[f'subj{subj_list[s]:02d}'][future_behavior]\n",
    "                            future_voxel0[future_behavior==-1] = voxel0[torch.where(future_behavior==-1)[0]] # replace invalid past voxels \n",
    "                            future_voxel0 = torch.Tensor(future_voxel0)\n",
    "\n",
    "                            # if shared1000, then you need to mask it out \n",
    "                            for p in range(seq_future):\n",
    "                                mask = (future_behav0[:,p,-1] == 1) # [16,] bool\n",
    "                                index = torch.nonzero(mask.cpu()).squeeze()\n",
    "                                future_voxel0[index,p,:] = torch.zeros_like(future_voxel0[index,p,:])\n",
    "\n",
    "                        # concatenate current timepoint with past/future\n",
    "                        if seq_past > 0 and seq_future > 0:\n",
    "                            voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                            voxel0 = torch.cat((voxel0, future_voxel0), axis=1)\n",
    "                        elif seq_past > 0:\n",
    "                            voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                        else:\n",
    "                            voxel0 = torch.cat((voxel0.unsqueeze(1), future_voxel0), axis=1)\n",
    "\n",
    "                    if epoch < int(mixup_pct * num_epochs):\n",
    "                        voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                        perm_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = perm\n",
    "                        betas_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = betas\n",
    "                        select_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = select\n",
    "\n",
    "                    voxel_iters[f\"subj{subj_list[s]:02d}_iter{i}\"] = voxel0\n",
    "                    i +=1\n",
    "                    if ~(i<num_iterations_per_epoch):\n",
    "                        # print(f\"breaking data loader at i={i} for s={s}\")\n",
    "                        break\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj{s:02d}_iter{train_i}\"] for s in subj_list]\n",
    "            train_voxels = voxel_list[0].detach().to(device)\n",
    "            # print(f\"voxel_list {voxel_list}\")\n",
    "            image = image_iters[train_i][:train_voxels.shape[0]].detach()\n",
    "            image_clip_target = image_clip_iters[train_i][:train_voxels.shape[0]].detach().to(device)\n",
    "            image_clip_target_norm = nn.functional.normalize(image_clip_target.flatten(1), dim=-1)\n",
    "            if dual_guidance:\n",
    "                text_clip_target = text_clip_iters[train_i][:train_voxels.shape[0]].detach().to(device)\n",
    "                text_clip_target_norm = nn.functional.normalize(text_clip_target.flatten(1), dim=-1)\n",
    "            if blurry_recon:\n",
    "                vae_image_target = vae_image_iters[train_i][:train_voxels.shape[0]].detach().to(device)\n",
    "                vae_image_target_norm = nn.functional.normalize(vae_image_target.flatten(1), dim=-1)\n",
    "\n",
    "            outputs = model(train_voxels)\n",
    "            image_clip_voxels_norm = nn.functional.normalize(outputs[\"image\"].flatten(1), dim=-1)\n",
    "            image_clip_loss = mse(image_clip_voxels_norm, image_clip_target_norm.unsqueeze(1))\n",
    "            loss += image_clip_loss\n",
    "            loss_image_clip_total += image_clip_loss.item()\n",
    "            if dual_guidance:\n",
    "                text_clip_voxels_norm = nn.functional.normalize(outputs[\"text\"].flatten(1), dim=-1)\n",
    "                text_clip_loss = mse(text_clip_voxels_norm, text_clip_target_norm.unsqueeze(1))\n",
    "                loss += text_clip_loss\n",
    "                loss_text_clip_total += text_clip_loss.item()\n",
    "            if blurry_recon:\n",
    "                blurry_image_enc_norm = nn.functional.normalize(outputs[\"blurry\"].flatten(1), dim=-1)\n",
    "                blurry_image_loss = mse(blurry_image_enc_norm, vae_image_target_norm.unsqueeze(1))\n",
    "                loss += blurry_image_loss\n",
    "                loss_blurry_total += blurry_image_loss.item()\n",
    "                \n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(image_clip_voxels_norm)).to(image_clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(image_clip_voxels_norm, image_clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(image_clip_target_norm, image_clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    try:\n",
    "                        blurry_recon_images = (autoenc.decode(blurry_image_enc[random_samps].reshape((-1, 4, 28, 28))/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                        pixcorr = utils.pixcorr(image[random_samps].cpu().float(), blurry_recon_images.cpu().float())\n",
    "                    except:\n",
    "                        print(random_samps, blurry_image_enc.shape)\n",
    "                        print(blurry_image_enc[random_samps].reshape((-1, 4, 28, 28)).shape, image[random_samps].shape)\n",
    "                        print(blurry_recon_images.shape)\n",
    "                        sys.exit()\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            caption = []\n",
    "            for test_i, data in enumerate(test_dl):  \n",
    "                if subj < 9:\n",
    "                    behav, past_behav, future_behav, old_behav = data\n",
    "                    # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                    assert len(behav) == num_test\n",
    "\n",
    "                    ## Average same-image repeats ##\n",
    "                    if test_image is None:\n",
    "                        voxel = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long().numpy()]\n",
    "                        \n",
    "                        if seq_len==1:\n",
    "                            voxel = voxel.unsqueeze(1)\n",
    "                        else:\n",
    "                            if seq_past>0:\n",
    "                                past_behavior = past_behav[:,:(seq_past),5].cpu().long()\n",
    "                                past_voxels = voxels[f'subj0{subj}'][past_behavior]\n",
    "                                if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                    past_voxels[torch.where(past_behavior==-1)[0]] = 0\n",
    "\n",
    "                            if seq_future>0:\n",
    "                                future_behavior = future_behav[:,:(seq_future),5].cpu().long()\n",
    "                                future_voxels = voxels[f'subj0{subj}'][future_behavior]                    \n",
    "                                if torch.any(future_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                    future_voxels[torch.where(future_behavior==-1)[0]] = 0\n",
    "                                \n",
    "                            if seq_past > 0 and seq_future > 0:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                                voxel = torch.cat((voxel, future_voxels), axis=1)\n",
    "                            elif seq_past > 0:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                            else:\n",
    "                                voxel = torch.cat((voxel.unsqueeze(1), future_voxels), axis=1)\n",
    "\n",
    "                        image = behav[:,0,0].cpu().long()\n",
    "\n",
    "                        unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                        for im in unique_image:\n",
    "                            locs = torch.where(im == image)[0]\n",
    "                            if len(locs)==1:\n",
    "                                locs = locs.repeat(3)\n",
    "                            elif len(locs)==2:\n",
    "                                locs = locs.repeat(2)[:3]\n",
    "                            assert len(locs)==3\n",
    "                            if test_image is None:\n",
    "                                test_image = torch.tensor(images[im][None], dtype=torch.float16, device=\"cpu\")\n",
    "                                test_image_clip = clip_image[im][None]\n",
    "                                if dual_guidance:\n",
    "                                    test_text_clip = clip_text[im][None]\n",
    "                                if blurry_recon:\n",
    "                                    test_vae_image = vae_image[im][None]\n",
    "                                test_voxel = voxel[locs][None]\n",
    "                            else:\n",
    "                                test_image = torch.vstack((test_image, torch.tensor(images[im][None], dtype=torch.float16, device=\"cpu\")))\n",
    "                                test_image_clip = torch.vstack((test_image_clip, clip_image[im][None]))\n",
    "                                if dual_guidance:\n",
    "                                    test_text_clip = torch.vstack((test_text_clip, clip_text[im][None]))\n",
    "                                if blurry_recon:\n",
    "                                    test_vae_image = torch.vstack((test_vae_image, vae_image[im][None]))\n",
    "                                test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "                                \n",
    "                    test_indices = torch.arange(len(test_voxel))[:300]\n",
    "                    # print(test_image.shape,len(test_caption),test_indices.max())\n",
    "                    voxel = test_voxel[test_indices].to(device)\n",
    "                    image = test_image[test_indices].to(device)\n",
    "                    image_clip_target = test_image_clip[test_indices].to(device)\n",
    "                    image_clip_target_norm = nn.functional.normalize(image_clip_target.flatten(1), dim=-1)\n",
    "                    if dual_guidance:\n",
    "                        text_clip_target = test_text_clip[test_indices].to(device)\n",
    "                        text_clip_target_norm = nn.functional.normalize(text_clip_target.flatten(1), dim=-1)\n",
    "                    if blurry_recon:\n",
    "                        vae_image_target = test_vae_image[test_indices].to(device)\n",
    "                        vae_image_target_norm = nn.functional.normalize(vae_image_target.flatten(1), dim=-1)\n",
    "                else:\n",
    "                    voxel, image = data\n",
    "                    voxel = voxel.unsqueeze(2).to(device)\n",
    "                    image = torch.nn.functional.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "                    image = image.to(device)\n",
    "                \n",
    "                loss=0.\n",
    "                voxel = torch.mean(voxel, dim=1).to(device)\n",
    "                \n",
    "                outputs = model(voxel)\n",
    "                image_clip_voxels = outputs[\"image\"]\n",
    "                image_clip_voxels_norm = nn.functional.normalize(image_clip_voxels.flatten(1), dim=-1)\n",
    "                image_clip_loss = mse(image_clip_voxels_norm, image_clip_target_norm.unsqueeze(1))\n",
    "                loss += image_clip_loss\n",
    "                test_loss_image_clip_total += image_clip_loss.item()\n",
    "                if dual_guidance:\n",
    "                    text_clip_voxels = outputs[\"text\"]\n",
    "                    text_clip_voxels_norm = nn.functional.normalize(text_clip_voxels.flatten(1), dim=-1)\n",
    "                    text_clip_loss = mse(text_clip_voxels_norm, text_clip_target_norm.unsqueeze(1))\n",
    "                    loss += text_clip_loss\n",
    "                    test_loss_text_clip_total += text_clip_loss.item()\n",
    "                if blurry_recon:\n",
    "                    blurry_image_enc = outputs[\"blurry\"]\n",
    "                    blurry_image_enc_norm = nn.functional.normalize(blurry_image_enc.flatten(1), dim=-1)\n",
    "                    blurry_image_loss = mse(blurry_image_enc_norm, vae_image_target_norm.unsqueeze(1))\n",
    "                    loss += blurry_image_loss\n",
    "                    test_loss_blurry_total += blurry_image_loss.item()\n",
    "                    \n",
    "                if clip_scale>0:\n",
    "                    image_clip_voxels_norm = nn.functional.normalize(image_clip_voxels.flatten(1), dim=-1)\n",
    "                    image_clip_target_norm = nn.functional.normalize(image_clip_target.flatten(1), dim=-1)\n",
    "\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(image_clip_voxels_norm)).to(image_clip_voxels_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(image_clip_voxels_norm, image_clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(image_clip_target_norm, image_clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "                if blurry_recon:\n",
    "                    with torch.no_grad():\n",
    "                        # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                        random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                        blurry_recon_images = (autoenc.decode(blurry_image_enc.reshape((-1, 4, 28, 28))[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                        pixcorr = utils.pixcorr(image[random_samps].cpu().float(), blurry_recon_images.cpu().float())\n",
    "                        test_blurry_pixcorr += pixcorr.item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            print(\"---\")\n",
    "\n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_image_clip_total\": loss_image_clip_total / (train_i + 1),\n",
    "                \"train/loss_text_clip_total\": loss_text_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_image_clip_total\": test_loss_image_clip_total / (test_i + 1),\n",
    "                \"test/loss_text_clip_total\": test_loss_text_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"train/recon_txt_cossim\": recon_cossim_txt / (train_i +1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"test/recon_txt_cossim\": test_recon_cossim_txt / (test_i +1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                \"train/loss_prior_txt\": loss_prior_total_txt / (train_i + 1),\n",
    "                \"test/loss_prior_txt\": test_loss_prior_total_txt / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(blurry_image_enc[[j]].reshape((-1, 4, 28, 28))/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = np.random.randint(0, 3)\n",
    "                    print(f\"reconstructing... idx={idx}\")\n",
    "                    samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/orig\"] = wandb.Image(transforms.ToPILImage()(image[idx]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                        logs[f\"test/recons\"] = wandb.Image(transforms.ToPILImage()(samples[0]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                    if utils.is_interactive():\n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
