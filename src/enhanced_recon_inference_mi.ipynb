{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.linear_model import Ridge\n",
    "from flux_reconstructor import Flux_Reconstructor\n",
    "from sd35_reconstructor import SD35_Reconstructor\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: subj01_40sess_hypatia_ridge_flux_ip\n",
      "--data_path=../dataset                     --cache_dir=../cache                     --model_name=subj01_40sess_hypatia_ridge_flux_ip --subj=1                     --mode vision                     --dual_guidance\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # model_name = \"final_subj01_pretrained_40sess_24bs\"\n",
    "    model_name = \"subj01_40sess_hypatia_ridge_sc_flux_enhanced\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=../dataset \\\n",
    "                    --cache_dir=../cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --mode vision \\\n",
    "                    --dual_guidance\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gen_rep\",type=int,default=10,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr\",type=float,default=-1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--normalize_preds\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_raw\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--raw_path\",type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--sd35\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "\n",
    "if seed > 0 and gen_rep == 1:\n",
    "    # seed all random functions, but only if doing 1 rep\n",
    "    utils.seed_everything(seed)\n",
    "    \n",
    "all_base_recons = torch.load(f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\")\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "model_name = model_name + \"_enhanced\"\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459b128",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 1, 15724]) torch.Size([18, 3, 425, 425])\n"
     ]
    }
   ],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, all_images = utils.load_nsd_synthetic(subject=subj, average=False, nest=True)\n",
    "elif subj > 8:\n",
    "    _, _, voxels, all_images = utils.load_imageryrf(subject=subj-8, mode=mode, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "else:\n",
    "    voxels, all_images = utils.load_nsd_mental_imagery(subject=subj, mode=mode, stimtype=\"all\", snr=snr, average=True, nest=False)\n",
    "num_voxels = voxels.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3602057",
   "metadata": {},
   "source": [
    "# Load pretrained models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a83acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sd35:\n",
    "    reconstructor = SD35_Reconstructor(embedder_only=False, device=device)\n",
    "    text_embedding_variant = \"sd35_t5\"\n",
    "    clip_text_seq_dim=154\n",
    "    clip_text_emb_dim=4096\n",
    "    text_embedding_variant2 = \"sd35_clip\"\n",
    "    clip_text_seq_dim2=1\n",
    "    clip_text_emb_dim2=2048\n",
    "else:\n",
    "    reconstructor = Flux_Reconstructor(embedder_only=False, device=device)\n",
    "    text_embedding_variant = \"flux_clip\"\n",
    "    clip_text_seq_dim=1\n",
    "    clip_text_emb_dim=768\n",
    "    text_embedding_variant2 = \"flux_t5\"\n",
    "    clip_text_seq_dim2=64\n",
    "    clip_text_emb_dim2=4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491a12d",
   "metadata": {},
   "source": [
    "### Compute ground truth embeddings for training data (for feature normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc750b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize_preds:\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{text_embedding_variant}_text_embeddings_train.pt\"\n",
    "    clip_text_train = torch.load(file_path_txt)\n",
    "    \n",
    "    file_path_txt2 = f\"{data_path}/preprocessed_data/subject{subj}/{text_embedding_variant2}_text_embeddings_train.pt\"\n",
    "    clip_text_train2 = torch.load(file_path_txt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dbfdb",
   "metadata": {},
   "source": [
    "# Predicting latent vectors for reconstruction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{outdir}/ridge_text_weights.pkl', 'rb') as f:\n",
    "    text_datadict = pickle.load(f)\n",
    "pred_clip_text = torch.zeros((len(all_images), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "model = Ridge(\n",
    "    alpha=60000,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "model.coef_ = text_datadict[\"coef\"]\n",
    "model.intercept_ = text_datadict[\"intercept\"]\n",
    "pred_clip_text = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_text_seq_dim, clip_text_emb_dim))\n",
    "\n",
    "with open(f'{outdir}/ridge_text2_weights.pkl', 'rb') as f:\n",
    "    text_datadict2 = pickle.load(f)\n",
    "pred_clip_text2 = torch.zeros((len(all_images), clip_text_seq_dim2, clip_text_emb_dim2)).to(\"cpu\")\n",
    "model = Ridge(\n",
    "    alpha=60000,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "model.coef_ = text_datadict2[\"coef\"]\n",
    "model.intercept_ = text_datadict2[\"intercept\"]\n",
    "pred_clip_text2 = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_text_seq_dim2, clip_text_emb_dim2))\n",
    "\n",
    "print(\"pred_clip_text shape:\", pred_clip_text.shape)\n",
    "print(\"Number of NaN values in pred_clip_text:\", torch.isnan(pred_clip_text).sum().item())\n",
    "\n",
    "print(\"pred_clip_text2 shape:\", pred_clip_text2.shape)\n",
    "print(\"Number of NaN values in pred_clip_text2:\", torch.isnan(pred_clip_text2).sum().item())\n",
    "if normalize_preds:\n",
    "    for sequence in range(clip_text_seq_dim):\n",
    "        std_pred_clip_text = (pred_clip_text[:, sequence] - torch.mean(pred_clip_text[:, sequence],axis=0)) / (torch.std(pred_clip_text[:, sequence],axis=0) + 1e-6)\n",
    "        pred_clip_text[:, sequence] = std_pred_clip_text * torch.std(clip_text_train[:, sequence],axis=0) + torch.mean(clip_text_train[:, sequence],axis=0)\n",
    "    for sequence in range(clip_text_seq_dim2):\n",
    "        std_pred_clip_text2 = (pred_clip_text2[:, sequence] - torch.mean(pred_clip_text2[:, sequence],axis=0)) / (torch.std(pred_clip_text2[:, sequence],axis=0) + 1e-6)\n",
    "        pred_clip_text2[:, sequence] = std_pred_clip_text2 * torch.std(clip_text_train2[:, sequence],axis=0) + torch.mean(clip_text_train2[:, sequence],axis=0)\n",
    "print(\"Normalized pred_clip_text shape:\", pred_clip_text.shape)\n",
    "print(\"Normalized Number of NaN values in pred_clip_text:\", torch.isnan(pred_clip_text).sum().item())\n",
    "\n",
    "print(\"Normalized pred_clip_text2 shape:\", pred_clip_text2.shape)\n",
    "print(\"Normalized Number of NaN values in pred_clip_text2:\", torch.isnan(pred_clip_text2).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c8363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/raid1/home/kneel027/Second-Sight/output/mental_imagery_paper_b3/vision/subj01_40sess_hypatia_ridge_flux_ip/subject1/\n"
     ]
    }
   ],
   "source": [
    "raw_root = f\"/export/raid1/home/kneel027/Second-Sight/output/mental_imagery_paper_b3/{mode}/{model_name}/subject{subj}/\"\n",
    "os.makedirs(raw_root,exist_ok=True)\n",
    "torch.save(pred_clip_text, f\"{raw_root}/{text_embedding_variant}_text_voxels.pt\")\n",
    "torch.save(pred_clip_text2, f\"{raw_root}/{text_embedding_variant2}_text_voxels.pt\")\n",
    "print(raw_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_root: /export/raid1/home/kneel027/Second-Sight/output/mental_imagery_paper_b3/vision/subj01_40sess_hypatia_ridge_flux_ip/subject1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 8678759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:   0%|          | 0/18 [00:03<?, ?it/s]\n",
      "  0%|          | 0/10 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reconstructor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Feed outputs through versatile diffusion\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m samples_multi \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mreconstructor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToPILImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblurred_image\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mc_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_voxels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mc_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_text_voxels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtextstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrecons_per_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m samples \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mpick_best_recon(samples_multi, clip_voxels, clip_extractor)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(samples, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Feed outputs through versatile diffusion\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m samples_multi \u001b[38;5;241m=\u001b[39m [\u001b[43mreconstructor\u001b[49m\u001b[38;5;241m.\u001b[39mreconstruct(\n\u001b[1;32m     53\u001b[0m                     image\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToPILImage()(torch\u001b[38;5;241m.\u001b[39mTensor(blurred_image[\u001b[38;5;241m0\u001b[39m])),\n\u001b[1;32m     54\u001b[0m                     c_i\u001b[38;5;241m=\u001b[39mclip_voxels,\n\u001b[1;32m     55\u001b[0m                     c_t\u001b[38;5;241m=\u001b[39mclip_text_voxels,\n\u001b[1;32m     56\u001b[0m                     n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     57\u001b[0m                     textstrength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m,\n\u001b[1;32m     58\u001b[0m                     strength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m,\n\u001b[1;32m     59\u001b[0m                     seed\u001b[38;5;241m=\u001b[39mseed) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(recons_per_sample)]\n\u001b[1;32m     60\u001b[0m samples \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mpick_best_recon(samples_multi, clip_voxels, clip_extractor)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(samples, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reconstructor' is not defined"
     ]
    }
   ],
   "source": [
    "final_recons = None\n",
    "final_predcaptions = None\n",
    "final_clipvoxels = None\n",
    "final_blurryrecons = None\n",
    "\n",
    "raw_root = f\"{raw_path}/{mode}/{model_name}/subject{subj}/\"\n",
    "print(\"raw_root:\", raw_root)\n",
    "os.makedirs(raw_root,exist_ok=True)\n",
    "recons_per_sample = 16\n",
    "\n",
    "for rep in tqdm(range(gen_rep)):\n",
    "    seed = random.randint(0,10000000)\n",
    "    utils.seed_everything(seed = seed)\n",
    "    print(f\"seed = {seed}\")\n",
    "    # get all reconstructions    \n",
    "    all_recons = None\n",
    "    all_clipvoxels = None\n",
    "    \n",
    "    minibatch_size = 1\n",
    "    num_samples_per_image = 1\n",
    "    plotting = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "            \n",
    "            clip_text_voxels = pred_clip_text[idx]\n",
    "            clip_text_voxels2 = pred_clip_text2[idx]\n",
    "            \n",
    "            # Feed outputs through versatile diffusion\n",
    "            samples = reconstructor.reconstruct(\n",
    "                                image=transforms.ToPILImage()(all_base_recons[idx][rep]),\n",
    "                                c_t=clip_text_voxels,\n",
    "                                t5=clip_text_voxels2,\n",
    "                                n_samples=1,\n",
    "                                strength=0.8,\n",
    "                                seed=seed)\n",
    "            if isinstance(samples, PIL.Image.Image):\n",
    "                samples = transforms.ToTensor()(samples)\n",
    "            samples = samples.unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    \n",
    "            if plotting: \n",
    "                print(model_name)\n",
    "                err # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "            if save_raw:\n",
    "                # print(f\"Saving raw images to {raw_root}/{idx}/{rep}.png\")\n",
    "                os.makedirs(f\"{raw_root}/{idx}/\", exist_ok=True)\n",
    "                transforms.ToPILImage()(samples[0]).save(f\"{raw_root}/{idx}/{rep}.png\")\n",
    "                if rep == 0:\n",
    "                    transforms.ToPILImage()(all_base_recons[idx][rep]).save(f\"{raw_root}/{idx}/low_level.png\")\n",
    "                    transforms.ToPILImage()(all_images[idx]).save(f\"{raw_root}/{idx}/ground_truth.png\")\n",
    "        # resize outputs before saving\n",
    "        imsize = 256\n",
    "        # saving\n",
    "        # print(all_recons.shape)\n",
    "        # torch.save(all_images,\"evals/all_images.pt\")\n",
    "        if final_recons is None:\n",
    "            final_recons = all_recons.unsqueeze(1)\n",
    "        else:\n",
    "            final_recons = torch.cat((final_recons, all_recons.unsqueeze(1)), dim=1)\n",
    "\n",
    "torch.save(final_recons,f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\")\n",
    "print(f\"saved {model_name} mi outputs!\")\n",
    "\n",
    "# if not utils.is_interactive():\n",
    "#     sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye_imagery_vd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
