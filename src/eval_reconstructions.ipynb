{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5f265e-407a-40bd-92fb-a652091fd7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "# # SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb36c22b-db86-4e8b-83a3-a00ac5656c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "[2023-12-31 13:45:03,075] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Setting batch_size to 16\n",
      "[2023-12-31 13:45:07,016] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-31 13:45:07,017] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "# ## UNCOMMENT BELOW SECTION AND COMMENT OUT DEEPSPEED SECTION TO AVOID USING DEEPSPEED ###\n",
    "# use_deepspeed = False\n",
    "# accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "# global_batch_size = batch_size = 16\n",
    "\n",
    "### DEEPSPEED INITIALIZATION ###\n",
    "use_deepspeed = True\n",
    "import deepspeed\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "if num_devices <= 1 and utils.is_interactive():\n",
    "    global_batch_size = batch_size = 16\n",
    "    print(f\"Setting batch_size to {batch_size}\")\n",
    "    # can emulate a distributed environment for deepspeed to work in jupyter notebook\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = str(np.random.randint(10000)+9000)\n",
    "    os.environ[\"RANK\"] = \"0\"\n",
    "    os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "    os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "    os.environ[\"GLOBAL_BATCH_SIZE\"] = str(global_batch_size) # set this to your batch size!\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]    \n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices\n",
    "    if num_devices <= 1:\n",
    "        os.environ[\"RANK\"] = \"0\"\n",
    "        os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "# alter the deepspeed config according to your global and local batch size\n",
    "if local_rank == 0:\n",
    "    with open('deepspeed_config_stage2_cpuoffload.json', 'r') as file:\n",
    "        config = json.load(file)\n",
    "    config['train_batch_size'] = int(os.environ[\"GLOBAL_BATCH_SIZE\"])\n",
    "    config['train_micro_batch_size_per_gpu'] = batch_size\n",
    "    config['bf16'] = {'enabled': False}\n",
    "    config['fp16'] = {'enabled': True}\n",
    "    with open('deepspeed_config_stage2_cpuoffload.json', 'w') as file:\n",
    "        json.dump(config, file)\n",
    "else:\n",
    "    # give some time for the local_rank=0 gpu to prep new deepspeed config file\n",
    "    time.sleep(10)\n",
    "deepspeed_plugin = DeepSpeedPlugin(\"deepspeed_config_stage2_cpuoffload.json\")\n",
    "accelerator = Accelerator(split_batches=False, deepspeed_plugin=deepspeed_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b420ad13-30d0-4caa-94ce-04941bbc770e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 3702695\n",
      "device: cuda:0\n",
      "Distributed environment: DistributedType.DEEPSPEED  Backend: nccl\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'zero_optimization': {'stage': 2, 'contiguous_gradients': True, 'stage3_gather_16bit_weights_on_model_save': True, 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_prefetch_bucket_size': 10000000.0, 'stage3_param_persistence_threshold': 100000.0, 'reduce_bucket_size': 10000000.0, 'sub_group_size': 1000000000.0, 'offload_optimizer': {'device': 'cpu', 'nvme_path': '/scratch', 'pin_memory': True}, 'offload_param': {'device': 'none', 'nvme_path': '/scratch', 'buffer_size': 4000000000.0, 'pin_memory': True}}, 'aio': {'block_size': 26214400, 'queue_depth': 32, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'train_batch_size': 16, 'train_micro_batch_size_per_gpu': 16, 'wall_clock_breakdown': False, 'zero_allow_untested_optimizer': True}\n",
      "\n",
      "distributed = True num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffb659a-8154-4536-ab27-2d976da1bf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: unCLIP_8gpu_noblur_nodepth\n",
      "--data_path=/fsx/proj-fmri/shared/mindeyev2_dataset                     --model_name=unCLIP_8gpu_noblur_nodepth                     --subj=1 --subj_list=1 --num_sessions=37                     --hidden_dim=2048 --seq_len=1\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"unCLIP_8gpu_noblur_nodepth\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/fsx/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --subj=1 --subj_list=1 --num_sessions=37 \\\n",
    "                    --hidden_dim=2048 --seq_len=1\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8120cd-f226-4e2c-a6c5-3cd8ef6e9bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 37\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/fsx/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (zero = all sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj_list\", type=int, nargs='+', default=[2,3,4,5,6,7,8],\n",
    "    help=\"number of subjects\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=4096,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32bf9351-e1d3-43b6-a2f7-736308385929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_voxels for subj01: 15724\n",
      "/fsx/proj-fmri/shared/mindeyev2_dataset/wds/subj01/test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "\n",
    "voxels = {}\n",
    "# Load hdf5 data for betas\n",
    "f = h5py.File(f'{data_path}/betas_all_subj0{subj}_fp32.hdf5', 'r')\n",
    "betas = f['betas'][:]\n",
    "betas = torch.Tensor(betas).to(\"cpu\")\n",
    "num_voxels = betas[0].shape[-1]\n",
    "voxels[f'subj0{subj}'] = betas\n",
    "print(f\"num_voxels for subj0{subj}: {num_voxels}\")\n",
    "\n",
    "if num_sessions==37: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "else: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "    \n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d4b24f-7f0f-4fc1-9252-271c9a8045ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2770 2770 982\n"
     ]
    }
   ],
   "source": [
    "# Prep images but don't load them all to memory\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images']\n",
    "\n",
    "# Prep test voxels and indices of test images\n",
    "test_images_idx = []\n",
    "for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):\n",
    "    test_voxels = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "    test_images_idx = np.append(test_images_idx, behav[:,0,0].cpu().numpy())\n",
    "test_images_idx = test_images_idx.astype(int)\n",
    "\n",
    "assert (test_i+1) * num_test == len(test_voxels) == len(test_images_idx)\n",
    "print(test_i, len(test_voxels), len(test_images_idx), len(np.unique(test_images_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4880021-253b-485c-8272-ef5d0e313ab4",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7832f1bd-e5a6-4405-af0c-140f9ee446af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48bbf01c-0f95-42e0-a55d-23c49d799da8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "32,204,802 total\n",
      "32,204,802 trainable\n",
      "param counts:\n",
      "917,472,144 total\n",
      "917,472,144 trainable\n",
      "param counts:\n",
      "949,676,946 total\n",
      "949,676,946 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "949676946"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "        self.temp = nn.Parameter(torch.Tensor([5.3]))\n",
    "        self.bias = nn.Parameter(torch.Tensor([-2.]))\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim, seq_len=seq_len)\n",
    "\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, out_dim=768, in_dim=15724, seq_len=2, h=4096, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768, text_clip_size=768, text_out_dim=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        self.text_clip_size = text_clip_size\n",
    "        \n",
    "        # Mixer Blocks\n",
    "        self.mixer_ln1 = nn.ModuleList([\n",
    "            self.ln(h) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mlp(seq_len, seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_ln2 = nn.ModuleList([\n",
    "            self.ln(h) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mlp(h, h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.clin1 = nn.Linear(h * seq_len, out_dim, bias=True)\n",
    "        self.clip_proj = self.projector(clip_size, clip_size)\n",
    "            \n",
    "    def projector(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def ln(self, dim):\n",
    "        return nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors for blur and depth outputs\n",
    "        t,b,d = torch.Tensor([0.]), torch.Tensor([0.]), torch.Tensor([0.])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x.permute(0,2,1)\n",
    "        residual2 = x\n",
    "        for ln1, block1, ln2, block2 in zip(self.mixer_ln1, self.mixer_blocks1, self.mixer_ln2, self.mixer_blocks2):\n",
    "            # Layer norm before transpose\n",
    "            x = ln1(x)\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            # Channel mixing\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            # Embedding mixing\n",
    "            x = ln2(x)\n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.clin1(x).reshape(len(x), -1, self.clip_size)\n",
    "        c = self.clip_proj(backbone)\n",
    "        \n",
    "        return backbone, c\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20b30b-d68c-4ba3-a86b-b52658ffaf82",
   "metadata": {},
   "source": [
    "## Diffusion Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab0c2582-05c7-45c2-8fb5-8c435d0391ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "1,209,542,162 total\n",
      "1,209,542,146 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1209542146"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = VersatileDiffusionPriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    "    voxel2clip=None,\n",
    ")\n",
    "utils.count_params(model.diffusion_prior)\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76b32e-bd38-4457-8936-d232991ccd50",
   "metadata": {},
   "source": [
    "## Prep unCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03523df6-33da-4368-ad2e-4fa5d02c966b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "vector_suffix torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                       denoiser_config=denoiser_config,\n",
    "                       first_stage_config=first_stage_config,\n",
    "                       conditioner_config=conditioner_config,\n",
    "                       sampler_config=sampler_config,\n",
    "                       scale_factor=scale_factor,\n",
    "                       disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "\n",
    "ckpt_path = '/fsx/proj-fmri/shared/mindeyev2_dataset/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "image = torch.Tensor(images[:1]).to(device)\n",
    "batch={\"jpg\": image,\n",
    "      \"original_size_as_tuple\": torch.ones(image.shape[0], 2).to(device) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(image.shape[0], 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3db1b-565b-48b6-be10-673d068fbaf5",
   "metadata": {},
   "source": [
    "# Load pretrained model ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8011898d-9ff9-4a05-8e4a-4130124799e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---loading /fsx/proj-fmri/paulscotti/MindEyeV2/train_logs/unCLIP_8gpu_noblur_nodepth/last.pth ckpt---\n",
      "\n",
      "Processing zero checkpoint '/fsx/proj-fmri/paulscotti/MindEyeV2/train_logs/unCLIP_8gpu_noblur_nodepth/last'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.gradients, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.12.2\n",
      "Reconstructed Frozen fp32 state dict with 1 params 16 elements\n",
      "Reconstructed fp32 state dict with 132 params 1209542146 elements\n",
      "ckpt loaded!\n"
     ]
    }
   ],
   "source": [
    "tag='last'\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "if use_deepspeed:\n",
    "    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "    try:\n",
    "        model.module.load_state_dict(state_dict, strict=True)\n",
    "    except:\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "    del state_dict\n",
    "else:\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    try:\n",
    "        model.module.load_state_dict(state_dict, strict=True)\n",
    "    except:\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "    del checkpoint\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5bdd94-4b71-431b-ac3e-e78f9259ca02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule(\n",
       "  (ridge): RidgeRegression(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=15724, out_features=2048, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (backbone): BrainNetwork(\n",
       "    (mixer_ln1): ModuleList(\n",
       "      (0-3): 4 x LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (mixer_blocks1): ModuleList(\n",
       "      (0-3): 4 x Sequential(\n",
       "        (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.15, inplace=False)\n",
       "        (3): Linear(in_features=1, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (mixer_ln2): ModuleList(\n",
       "      (0-3): 4 x LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (mixer_blocks2): ModuleList(\n",
       "      (0-3): 4 x Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.15, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (clin1): Linear(in_features=2048, out_features=425984, bias=True)\n",
       "    (clip_proj): Sequential(\n",
       "      (0): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1664, out_features=2048, bias=True)\n",
       "      (3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (4): GELU(approximate='none')\n",
       "      (5): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (6): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (7): GELU(approximate='none')\n",
       "      (8): Linear(in_features=2048, out_features=1664, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (diffusion_prior): BrainDiffusionPrior(\n",
       "    (noise_scheduler): NoiseScheduler()\n",
       "    (net): VersatileDiffusionPriorNetwork(\n",
       "      (to_time_embeds): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SinusoidalPosEmb()\n",
       "          (1): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "                (1): SiLU()\n",
       "                (2): Identity()\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Linear(in_features=3328, out_features=3328, bias=True)\n",
       "                (1): SiLU()\n",
       "                (2): Identity()\n",
       "              )\n",
       "              (2): Linear(in_features=3328, out_features=1664, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Rearrange('b (n d) -> b n d', n=1)\n",
       "      )\n",
       "      (causal_transformer): FlaggedCausalTransformer(\n",
       "        (init_norm): Identity()\n",
       "        (rel_pos_bias): RelPosBias(\n",
       "          (relative_attention_bias): Embedding(32, 32)\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x ModuleList(\n",
       "            (0): Attention(\n",
       "              (norm): LayerNorm()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_q): Linear(in_features=1664, out_features=1664, bias=False)\n",
       "              (to_kv): Linear(in_features=1664, out_features=104, bias=False)\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1664, out_features=1664, bias=False)\n",
       "                (1): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): LayerNorm()\n",
       "              (1): Linear(in_features=1664, out_features=13312, bias=False)\n",
       "              (2): SwiGLU()\n",
       "              (3): Identity()\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "              (5): Linear(in_features=6656, out_features=1664, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm()\n",
       "        (project_out): Linear(in_features=1664, out_features=1664, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model to inference and print full model\n",
    "model.eval().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb7b91-6de0-4b3c-aff2-cfb0c89037e0",
   "metadata": {},
   "source": [
    "# Quantitative evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6fd0d-e8c8-42da-91a4-3dded031a1c3",
   "metadata": {},
   "source": [
    "## No upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd726881-c711-47dc-9c5f-030da75801c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                         | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusion prior...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c0eedf66044b2ab9bda1f216c4e2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████████▋                                                                                                                                                       | 1/8 [24:27<2:51:12, 1467.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusion prior...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb20f1987af4d3c9e904423061f9603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████████████████████████▎                                                                                                                                 | 2/8 [48:46<2:26:15, 1462.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusion prior...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35336029d4204b8ca93d16798b66a6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_images = None\n",
    "all_recons = None\n",
    "minibatch_size = 128\n",
    "num_samples_per_image = 1\n",
    "plotting = False\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "    for batch in tqdm(range(0,len(np.unique(test_images_idx)),minibatch_size)):\n",
    "        uniq_imgs = np.unique(test_images_idx)[batch:batch+minibatch_size]\n",
    "        voxel = None\n",
    "        for uniq_img in uniq_imgs:\n",
    "            locs = np.where(test_images_idx==uniq_img)[0]\n",
    "            if len(locs)==1:\n",
    "                locs = locs.repeat(3)\n",
    "            elif len(locs)==2:\n",
    "                locs = locs.repeat(2)[:3]\n",
    "            assert len(locs)==3\n",
    "            if voxel is None:\n",
    "                voxel = test_voxels[None,locs] # 1, num_image_repetitions, num_voxels\n",
    "            else:\n",
    "                voxel = torch.vstack((voxel, test_voxels[None,locs]))\n",
    "        image = images[uniq_imgs] # 1, 3, 224, 224\n",
    "        \n",
    "        if plotting:\n",
    "            print(\"original images:\")\n",
    "            for i in range(len(image)):\n",
    "                im = torch.Tensor(image[i])\n",
    "                plt.figure(figsize=(2,2))\n",
    "                plt.imshow(transforms.ToPILImage()(im))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                if all_images is None:\n",
    "                    all_images = im[None]\n",
    "                else:\n",
    "                    all_images = torch.vstack((all_images, im[None]))\n",
    "\n",
    "        # feed voxels through ridge + backbone (each image repetition separately fed through)\n",
    "        for rep in range(3):\n",
    "            voxel_ridge = model.ridge(voxel[:,rep,None].float(),0) # 0th index of subj_list\n",
    "            backbone0, clip_voxels0 = model.backbone(voxel_ridge)\n",
    "            if rep==0:\n",
    "                clip_voxels = clip_voxels0\n",
    "                backbone = backbone0\n",
    "            else:\n",
    "                clip_voxels += clip_voxels0\n",
    "                backbone += backbone0\n",
    "        clip_voxels /= 3\n",
    "        backbone /= 3\n",
    "\n",
    "        # feed voxels through diffusion prior\n",
    "        print(\"diffusion prior...\")\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)\n",
    "        \n",
    "        # feed diffusion prior output through unCLIP\n",
    "        if plotting:\n",
    "            print(\"reconstructions:\")\n",
    "            for i in range(len(image)):\n",
    "                samples = utils.unclip_recon(prior_out[[i]],\n",
    "                                 diffusion_engine,\n",
    "                                 vector_suffix,\n",
    "                                 num_samples=num_samples_per_image)\n",
    "                if all_recons is None:\n",
    "                    all_recons = samples\n",
    "                else:\n",
    "                    all_recons = torch.vstack((all_recons, samples))\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "        \n",
    "print(all_images.shape, all_recons.shape)\n",
    "torch.save(all_images,\"all_images.pt\")\n",
    "torch.save(all_recons,f\"{model_name}_all_recons.pt\")\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d66b33-b327-4895-a861-ecc6ccc51296",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42009e9-f910-4f02-8db6-d46778aa6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = torch.load(\"all_images.pt\")\n",
    "all_recons = torch.load(f\"{model_name}_all_recons.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f482da88-3f4c-4615-998e-96e712368530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15724])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imsize = 256\n",
    "all_images = transforms.Resize((imsize,imsize))(all_images)\n",
    "all_recons = transforms.Resize((imsize,imsize))(all_recons)\n",
    "\n",
    "np.random.seed(0)\n",
    "ind = np.flip(np.array([112,119,101,44,159,22,173,174,175,189,981,243,249,255,265]))\n",
    "\n",
    "all_interleaved = torch.zeros(len(ind)*2,3,imsize,imsize)\n",
    "icount = 0\n",
    "for t in ind:\n",
    "    all_interleaved[icount] = all_images[t]\n",
    "    all_interleaved[icount+1] = all_recons[t]\n",
    "    icount += 2\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(imgs,figsize):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    \n",
    "grid = make_grid(all_interleaved, nrow=10, padding=2)\n",
    "show(grid,figsize=(20,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26e124-2444-434d-a399-d03c2c90cc08",
   "metadata": {},
   "source": [
    "## 2-way identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1778ff-5d6a-4087-b59f-0f44b9e0eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "@torch.no_grad()\n",
    "def two_way_identification(all_recons, all_images, model, preprocess, feature_layer=None, return_avg=True):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "\n",
    "    r = np.corrcoef(reals, preds)\n",
    "    r = r[:len(all_images), len(all_images):]\n",
    "    congruents = np.diag(r)\n",
    "\n",
    "    success = r < congruents\n",
    "    success_cnt = np.sum(success, 0)\n",
    "\n",
    "    if return_avg:\n",
    "        perf = np.mean(success_cnt) / (len(all_images)-1)\n",
    "        return perf\n",
    "    else:\n",
    "        return success_cnt, len(all_images)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6be966-52ef-4cf6-8078-8d2d9617564b",
   "metadata": {},
   "source": [
    "## PixCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17ea38-a254-4e90-a910-711734fdd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "# Flatten images while keeping the batch dimension\n",
    "all_images_flattened = preprocess(all_images).reshape(len(all_images), -1).cpu()\n",
    "all_recons_flattened = preprocess(all_recons).view(len(all_recons), -1).cpu()\n",
    "\n",
    "print(all_images_flattened.shape)\n",
    "print(all_recons_flattened.shape)\n",
    "\n",
    "corrsum = 0\n",
    "for i in tqdm(range(982)):\n",
    "    corrsum += np.corrcoef(all_images_flattened[i], all_recons_flattened[i])[0][1]\n",
    "corrmean = corrsum / 982\n",
    "\n",
    "pixcorr = corrmean\n",
    "print(pixcorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a556d5b-33a2-44aa-b48d-4b168316bbdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326fc4c-1248-4d0f-9176-218c6460f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/zijin-gu/meshconv-decoding/issues/3\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR), \n",
    "])\n",
    "\n",
    "# convert image to grayscale with rgb2grey\n",
    "img_gray = rgb2gray(preprocess(all_images).permute((0,2,3,1)).cpu())\n",
    "recon_gray = rgb2gray(preprocess(all_recons).permute((0,2,3,1)).cpu())\n",
    "print(\"converted, now calculating ssim...\")\n",
    "\n",
    "ssim_score=[]\n",
    "for im,rec in tqdm(zip(img_gray,recon_gray),total=len(all_images)):\n",
    "    ssim_score.append(ssim(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "\n",
    "ssim = np.mean(ssim_score)\n",
    "print(ssim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35138520-ec00-48a6-90dc-249a32a783d2",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45cc6c-ab80-43e2-b446-c8fcb4fc54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "\n",
    "alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4','features.11']).to(device)\n",
    "alex_model.eval().requires_grad_(False)\n",
    "\n",
    "# see alex_weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "layer = 'early, AlexNet(2)'\n",
    "print(f\"\\n---{layer}---\")\n",
    "all_per_correct = two_way_identification(all_recons.to(device).float(), all_images, \n",
    "                                                          alex_model, preprocess, 'features.4')\n",
    "alexnet2 = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {alexnet2:.4f}\")\n",
    "\n",
    "layer = 'mid, AlexNet(5)'\n",
    "print(f\"\\n---{layer}---\")\n",
    "all_per_correct = two_way_identification(all_recons.to(device).float(), all_images, \n",
    "                                                          alex_model, preprocess, 'features.11')\n",
    "alexnet5 = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {alexnet5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296bab2-d106-469e-b997-b32d21a2cf01",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c1b2b-af2a-476d-a1ac-32ee915ac2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "inception_model = create_feature_extractor(inception_v3(weights=weights), \n",
    "                                           return_nodes=['avgpool']).to(device)\n",
    "inception_model.eval().requires_grad_(False)\n",
    "\n",
    "# see weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "all_per_correct = two_way_identification(all_recons, all_images,\n",
    "                                        inception_model, preprocess, 'avgpool')\n",
    "        \n",
    "inception = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {inception:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a25f7f-8298-4413-b512-8a1173413e07",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbf7ce-8793-4988-a328-a632acd88aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "all_per_correct = two_way_identification(all_recons, all_images,\n",
    "                                        clip_model.encode_image, preprocess, None) # final layer\n",
    "clip_ = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {clip_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fed9f8-ef1a-4c6d-a83f-2a934b6e87fd",
   "metadata": {},
   "source": [
    "## Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14143c0f-1b32-43ef-98d8-8ed458df4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "weights = EfficientNet_B1_Weights.DEFAULT\n",
    "eff_model = create_feature_extractor(efficientnet_b1(weights=weights), \n",
    "                                    return_nodes=['avgpool']).to(device)\n",
    "eff_model.eval().requires_grad_(False)\n",
    "\n",
    "# see weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "gt = eff_model(preprocess(all_images))['avgpool']\n",
    "gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "fake = eff_model(preprocess(all_recons))['avgpool']\n",
    "fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "\n",
    "effnet = np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean()\n",
    "print(\"Distance:\",effnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f669d-cab7-4c75-90cd-651283f65a9e",
   "metadata": {},
   "source": [
    "## SwAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60b0c4-79fe-4cff-95e9-99733c821e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "swav_model = create_feature_extractor(swav_model, \n",
    "                                    return_nodes=['avgpool']).to(device)\n",
    "swav_model.eval().requires_grad_(False)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "gt = swav_model(preprocess(all_images))['avgpool']\n",
    "gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "fake = swav_model(preprocess(all_recons))['avgpool']\n",
    "fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "\n",
    "swav = np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean()\n",
    "print(\"Distance:\",swav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c9a27-a09b-4726-bf7e-4f4bce3ffc67",
   "metadata": {},
   "source": [
    "## Display in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb3896-ae60-4933-9c64-79ff0ae3a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store variable names and their corresponding values\n",
    "data = {\n",
    "    \"Metric\": [\"PixCorr\", \"SSIM\", \"AlexNet(2)\", \"AlexNet(5)\", \"InceptionV3\", \"CLIP\", \"EffNet-B\", \"SwAV\"],\n",
    "    \"Value\": [pixcorr, ssim, alexnet2, alexnet5, inception, clip_, effnet, swav],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# save table to txt file\n",
    "os.makedirs('tables/',exist_ok=True)\n",
    "df.to_csv(f'tables/{recon_path[:-3]}.csv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "mindeye"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
