{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f265e-407a-40bd-92fb-a652091fd7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchmetrics import PearsonCorrCoef\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from models import GNet8_Encoder\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb659a-8154-4536-ab27-2d976da1bf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # model_name = \"git_30ep_4block_hid4096_bs48\"\n",
    "    model_name = \"testing\"\n",
    "    subj = 1\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    jupyter_args = f\"--data_path=../dataset --model_name={model_name} --subj={subj} --num_sessions 37\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8120cd-f226-4e2c-a6c5-3cd8ef6e9bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/fsx/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Evaluate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (zero = all sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d66b33-b327-4895-a861-ecc6ccc51296",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66f9c9-f25a-48d9-9e9a-272ab33d20ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_images = torch.load(f\"evals/subj0{subj}_all_images.pt\")\n",
    "all_recons = torch.load(f\"evals/{model_name}/{model_name}_all_recons.pt\")\n",
    "# all_blurryrecons = torch.load(f\"evals/{model_name}/{model_name}_all_blurryrecons.pt\")\n",
    "# all_enhancedrecons = torch.load(f\"evals/{model_name}/{model_name}_all_enhancedrecons.pt\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42009e9-f910-4f02-8db6-d46778aa6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 256\n",
    "if all_images.shape[-1] != imsize:\n",
    "    all_images = transforms.Resize((imsize,imsize))(all_images).float()\n",
    "if all_recons.shape[-1] != imsize:\n",
    "    all_recons = transforms.Resize((imsize,imsize))(all_recons).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba0b041-042f-46e7-8f59-4055355785bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(9, 4))\n",
    "jj=-1; kk=0;\n",
    "for j in [0,1,2,3]:\n",
    "    jj+=1\n",
    "    axes[kk][jj].imshow(utils.torch_to_Image(all_images[j]))\n",
    "    axes[kk][jj].axis('off')\n",
    "    jj+=1\n",
    "    axes[kk][jj].imshow(utils.torch_to_Image(all_recons[j]))\n",
    "    axes[kk][jj].axis('off')\n",
    "    if jj==3: kk+=1; jj=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4deb53-4d85-4292-92c5-bb59077523cf",
   "metadata": {},
   "source": [
    "# Retrieval eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49d57a-9b65-490c-a1e0-61bd05682171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "percent_correct_fwds, percent_correct_bwds = [], []\n",
    "percent_correct_fwd, percent_correct_bwd = None, None\n",
    "\n",
    "with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "    for test_i, loop in enumerate(tqdm(range(30))):\n",
    "        random_samps = np.random.choice(np.arange(len(all_images)), size=300, replace=False)\n",
    "        emb = clip_img_embedder(all_images[random_samps].to(device)).float() # CLIP-Image\n",
    "\n",
    "        emb_ = all_clipvoxels[random_samps] # CLIP-Brain\n",
    "\n",
    "        # flatten if necessary\n",
    "        emb = emb.reshape(len(emb),-1)\n",
    "        emb_ = emb_.reshape(len(emb_),-1)\n",
    "\n",
    "        # l2norm \n",
    "        emb = nn.functional.normalize(emb,dim=-1)\n",
    "        emb_ = nn.functional.normalize(emb_,dim=-1)\n",
    "\n",
    "        labels = torch.arange(len(emb)).to(device)\n",
    "        bwd_sim = utils.batchwise_cosine_similarity(emb,emb_)  # clip, brain\n",
    "        fwd_sim = utils.batchwise_cosine_similarity(emb_,emb)  # brain, clip\n",
    "\n",
    "        assert len(bwd_sim) == 300\n",
    "\n",
    "        percent_correct_fwds = np.append(percent_correct_fwds, utils.topk(fwd_sim, labels,k=1).item())\n",
    "        percent_correct_bwds = np.append(percent_correct_bwds, utils.topk(bwd_sim, labels,k=1).item())\n",
    "\n",
    "        if test_i==0:\n",
    "            print(\"Loop 0:\",percent_correct_fwds, percent_correct_bwds)\n",
    "            \n",
    "percent_correct_fwd = np.mean(percent_correct_fwds)\n",
    "fwd_sd = np.std(percent_correct_fwds) / np.sqrt(len(percent_correct_fwds))\n",
    "fwd_ci = stats.norm.interval(0.95, loc=percent_correct_fwd, scale=fwd_sd)\n",
    "\n",
    "percent_correct_bwd = np.mean(percent_correct_bwds)\n",
    "bwd_sd = np.std(percent_correct_bwds) / np.sqrt(len(percent_correct_bwds))\n",
    "bwd_ci = stats.norm.interval(0.95, loc=percent_correct_bwd, scale=bwd_sd)\n",
    "\n",
    "print(f\"fwd percent_correct: {percent_correct_fwd:.4f} 95% CI: [{fwd_ci[0]:.4f},{fwd_ci[1]:.4f}]\")\n",
    "print(f\"bwd percent_correct: {percent_correct_bwd:.4f} 95% CI: [{bwd_ci[0]:.4f},{bwd_ci[1]:.4f}]\")\n",
    "\n",
    "fwd_sim = np.array(fwd_sim.cpu())\n",
    "bwd_sim = np.array(bwd_sim.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4a5df-ea7c-44bd-879a-dc97ba05b524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Given Brain embedding, find correct Image embedding\")\n",
    "fig, ax = plt.subplots(nrows=4, ncols=6, figsize=(11,12))\n",
    "for trial in range(4):\n",
    "    ax[trial, 0].imshow(utils.torch_to_Image(all_image[random_samps][trial]))\n",
    "    ax[trial, 0].set_title(\"original\\nimage\")\n",
    "    ax[trial, 0].axis(\"off\")\n",
    "    for attempt in range(5):\n",
    "        which = np.flip(np.argsort(fwd_sim[trial]))[attempt]\n",
    "        ax[trial, attempt+1].imshow(utils.torch_to_Image(all_image[random_samps][which]))\n",
    "        ax[trial, attempt+1].set_title(f\"Top {attempt+1}\")\n",
    "        ax[trial, attempt+1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26e124-2444-434d-a399-d03c2c90cc08",
   "metadata": {},
   "source": [
    "## 2-way identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1778ff-5d6a-4087-b59f-0f44b9e0eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "@torch.no_grad()\n",
    "def two_way_identification(all_recons, all_images, model, preprocess, feature_layer=None, return_avg=True):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "\n",
    "    r = np.corrcoef(reals, preds)\n",
    "    r = r[:len(all_images), len(all_images):]\n",
    "    congruents = np.diag(r)\n",
    "\n",
    "    success = r < congruents\n",
    "    success_cnt = np.sum(success, 0)\n",
    "\n",
    "    if return_avg:\n",
    "        perf = np.mean(success_cnt) / (len(all_images)-1)\n",
    "        return perf\n",
    "    else:\n",
    "        return success_cnt, len(all_images)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6be966-52ef-4cf6-8078-8d2d9617564b",
   "metadata": {},
   "source": [
    "## PixCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17ea38-a254-4e90-a910-711734fdd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "# Flatten images while keeping the batch dimension\n",
    "all_images_flattened = preprocess(all_images).reshape(len(all_images), -1).cpu()\n",
    "all_recons_flattened = preprocess(all_recons).view(len(all_recons), -1).cpu()\n",
    "\n",
    "print(all_images_flattened.shape)\n",
    "print(all_recons_flattened.shape)\n",
    "\n",
    "corrsum = 0\n",
    "for i in tqdm(range(len(all_images))):\n",
    "    corrsum += np.corrcoef(all_images_flattened[i], all_recons_flattened[i])[0][1]\n",
    "corrmean = corrsum / len(all_images)\n",
    "\n",
    "pixcorr = corrmean\n",
    "print(pixcorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a556d5b-33a2-44aa-b48d-4b168316bbdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326fc4c-1248-4d0f-9176-218c6460f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/zijin-gu/meshconv-decoding/issues/3\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR), \n",
    "])\n",
    "\n",
    "# convert image to grayscale with rgb2grey\n",
    "img_gray = rgb2gray(preprocess(all_images).permute((0,2,3,1)).cpu())\n",
    "recon_gray = rgb2gray(preprocess(all_recons).permute((0,2,3,1)).cpu())\n",
    "print(\"converted, now calculating ssim...\")\n",
    "\n",
    "ssim_score=[]\n",
    "for im,rec in tqdm(zip(img_gray,recon_gray),total=len(all_images)):\n",
    "    ssim_score.append(ssim(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "\n",
    "ssim = np.mean(ssim_score)\n",
    "print(ssim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35138520-ec00-48a6-90dc-249a32a783d2",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45cc6c-ab80-43e2-b446-c8fcb4fc54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "\n",
    "alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4','features.11']).to(device)\n",
    "alex_model.eval().requires_grad_(False)\n",
    "\n",
    "# see alex_weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "layer = 'early, AlexNet(2)'\n",
    "print(f\"\\n---{layer}---\")\n",
    "all_per_correct = two_way_identification(all_recons.to(device).float(), all_images, \n",
    "                                                          alex_model, preprocess, 'features.4')\n",
    "alexnet2 = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {alexnet2:.4f}\")\n",
    "\n",
    "layer = 'mid, AlexNet(5)'\n",
    "print(f\"\\n---{layer}---\")\n",
    "all_per_correct = two_way_identification(all_recons.to(device).float(), all_images, \n",
    "                                                          alex_model, preprocess, 'features.11')\n",
    "alexnet5 = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {alexnet5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296bab2-d106-469e-b997-b32d21a2cf01",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c1b2b-af2a-476d-a1ac-32ee915ac2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "inception_model = create_feature_extractor(inception_v3(weights=weights), \n",
    "                                           return_nodes=['avgpool']).to(device)\n",
    "inception_model.eval().requires_grad_(False)\n",
    "\n",
    "# see weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "all_per_correct = two_way_identification(all_recons, all_images,\n",
    "                                        inception_model, preprocess, 'avgpool')\n",
    "        \n",
    "inception = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {inception:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a25f7f-8298-4413-b512-8a1173413e07",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbf7ce-8793-4988-a328-a632acd88aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "all_per_correct = two_way_identification(all_recons, all_images,\n",
    "                                        clip_model.encode_image, preprocess, None) # final layer\n",
    "clip_ = np.mean(all_per_correct)\n",
    "print(f\"2-way Percent Correct: {clip_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fed9f8-ef1a-4c6d-a83f-2a934b6e87fd",
   "metadata": {},
   "source": [
    "## Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14143c0f-1b32-43ef-98d8-8ed458df4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "weights = EfficientNet_B1_Weights.DEFAULT\n",
    "eff_model = create_feature_extractor(efficientnet_b1(weights=weights), \n",
    "                                    return_nodes=['avgpool'])\n",
    "eff_model.eval().requires_grad_(False)\n",
    "\n",
    "# see weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "gt = eff_model(preprocess(all_images))['avgpool']\n",
    "gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "fake = eff_model(preprocess(all_recons))['avgpool']\n",
    "fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "\n",
    "effnet = np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean()\n",
    "print(\"Distance:\",effnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f669d-cab7-4c75-90cd-651283f65a9e",
   "metadata": {},
   "source": [
    "## SwAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60b0c4-79fe-4cff-95e9-99733c821e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "swav_model = create_feature_extractor(swav_model, \n",
    "                                    return_nodes=['avgpool'])\n",
    "swav_model.eval().requires_grad_(False)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "gt = swav_model(preprocess(all_images))['avgpool']\n",
    "gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "fake = swav_model(preprocess(all_recons))['avgpool']\n",
    "fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "\n",
    "swav = np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean()\n",
    "print(\"Distance:\",swav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822eddb",
   "metadata": {},
   "source": [
    "# Brain Correlation\n",
    "### Load brain data, brain masks, image lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brain data as ground truth\n",
    "def my_split_by_node(urls): return urls\n",
    "\n",
    "voxels = {}\n",
    "# Load hdf5 data for betas\n",
    "f = h5py.File(f'{data_path}/betas_all_subj0{subj}_fp32.hdf5', 'r')\n",
    "betas = f['betas'][:]\n",
    "betas = torch.Tensor(betas).to(\"cpu\")\n",
    "num_voxels = betas[0].shape[-1]\n",
    "voxels[f'subj0{subj}'] = betas\n",
    "print(f\"num_voxels for subj0{subj}: {num_voxels}\")\n",
    "print(num_sessions)\n",
    "if num_sessions==37: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "else: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "    \n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep test voxels and indices of test images\n",
    "test_images_idx = []\n",
    "for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):\n",
    "    test_voxels = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "    test_images_idx = np.append(test_images_idx, behav[:,0,0].cpu().numpy())\n",
    "test_images_idx = test_images_idx.astype(int)\n",
    "\n",
    "assert (test_i+1) * num_test == len(test_voxels) == len(test_images_idx)\n",
    "print(test_i, len(test_voxels), len(test_images_idx), len(np.unique(test_images_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brain region masks\n",
    "brain_region_masks = {}\n",
    "with h5py.File(\"evals/brain_region_masks.hdf5\", \"r\") as file:\n",
    "    # Iterate over each subject\n",
    "    for subject in file.keys():\n",
    "        subject_group = file[subject]\n",
    "        # Load the masks data for each subject\n",
    "        subject_masks = {\"nsd_general\" : subject_group[\"nsd_general\"][:],\n",
    "                         \"V1\" : subject_group[\"V1\"][:], \n",
    "                         \"V2\" : subject_group[\"V2\"][:], \n",
    "                         \"V3\" : subject_group[\"V3\"][:], \n",
    "                         \"V4\" : subject_group[\"V4\"][:],\n",
    "                         \"higher_vis\" : subject_group[\"higher_vis\"][:]}\n",
    "        brain_region_masks[subject] = subject_masks\n",
    "subject_masks = brain_region_masks[f\"subj0{subj}\"]\n",
    "\n",
    "# Average voxels of test set across trial repetitions for brain correlation scores\n",
    "test_voxels_averaged = torch.zeros((len(np.unique(test_images_idx)),num_voxels))\n",
    "# i = 0\n",
    "uniq_imgs = np.unique(test_images_idx)\n",
    "for i, uniq_img in enumerate(uniq_imgs):\n",
    "    locs = np.where(test_images_idx==uniq_img)[0]\n",
    "    if len(locs)==1:\n",
    "        locs = locs.repeat(3)\n",
    "    elif len(locs)==2:\n",
    "        locs = locs.repeat(2)[:3]\n",
    "    assert len(locs)==3\n",
    "    test_voxels_averaged[i] = torch.mean(test_voxels[None,locs], dim=1)\n",
    "\n",
    "# Prepare image list for input to GNet\n",
    "recon_list = []\n",
    "for i in range(all_recons.shape[0]):\n",
    "    img = all_recons[i].detach()\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    recon_list.append(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba131d01",
   "metadata": {},
   "source": [
    "### Calculate Brain Correlation scores for each brain area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNet = GNet8_Encoder(device=device,subject=subj)\n",
    "PeC = PearsonCorrCoef(num_outputs=len(recon_list))\n",
    "beta_primes = GNet.predict(recon_list)\n",
    "\n",
    "region_brain_correlations = {}\n",
    "for region, mask in subject_masks.items():\n",
    "    score = PeC(test_voxels_averaged[:,mask].moveaxis(0,1), beta_primes[:,mask].moveaxis(0,1))\n",
    "    region_brain_correlations[region] = float(torch.mean(score))\n",
    "print(region_brain_correlations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c9a27-a09b-4726-bf7e-4f4bce3ffc67",
   "metadata": {},
   "source": [
    "## Display in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb3896-ae60-4933-9c64-79ff0ae3a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store variable names and their corresponding values\n",
    "import pandas as pd\n",
    "data = {\n",
    "    \"Metric\": [\"PixCorr\", \"SSIM\", \"AlexNet(2)\", \"AlexNet(5)\", \"InceptionV3\", \"CLIP\", \"EffNet-B\", \"SwAV\", \"FwdRetrieval\", \"BwdRetrieval\",\n",
    "               \"Brain Corr. nsd_general\", \"Brain Corr. V1\", \"Brain Corr. V2\", \"Brain Corr. V3\", \"Brain Corr. V4\",  \"Brain Corr. higher_vis\"],\n",
    "    \"Value\": [pixcorr, ssim, alexnet2, alexnet5, inception, clip_, effnet, swav, percent_correct_fwd, percent_correct_bwd, \n",
    "              region_brain_correlations[\"nsd_general\"], region_brain_correlations[\"V1\"], region_brain_correlations[\"V2\"], region_brain_correlations[\"V3\"], region_brain_correlations[\"V4\"], region_brain_correlations[\"higher_vis\"]]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(model_name)\n",
    "print(df.to_string(index=False))\n",
    "# print(df[\"Value\"].to_string(index=False))\n",
    "\n",
    "# save table to txt file\n",
    "os.makedirs('tables/',exist_ok=True)\n",
    "df[\"Value\"].to_csv(f'tables/{model_name}.csv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
