{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5f265e-407a-40bd-92fb-a652091fd7ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "PID of this process = 3303008\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from einops.layers.torch import Rearrange\n",
    "from transformers import CLIPModel, AutoTokenizer, AutoProcessor\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.metrics import structural_similarity\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "import clip\n",
    "import scipy as sp\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "import utils\n",
    "from models import GNet8_Encoder\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffb659a-8154-4536-ab27-2d976da1bf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: p_trained_subj01_40sess_hypatia_new_vd_dual_proj\n",
      "--model_name=p_trained_subj01_40sess_hypatia_new_vd_dual_proj --subj=1 --data_path=/weka/proj-medarc/shared/mindeyev2_dataset --cache_dir=/weka/proj-medarc/shared/cache --all_recons_path=evals/p_trained_subj01_40sess_hypatia_new_vd_dual_proj/p_trained_subj01_40sess_hypatia_new_vd_dual_proj_all_recons_vision.pt --mode vision                     --criteria=all --imagery_data_path=/weka/proj-medarc/shared/umn-imagery\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"p_trained_subj01_40sess_hypatia_new_vd_dual_proj\"\n",
    "    # model_name = \"pretest_pretrained_subj01_40sess_hypatia_pg_sessions40\"\n",
    "    mode = \"vision\"\n",
    "    # all_recons_path = f\"evals/{model_name}/{model_name}_all_enhancedrecons_{mode}.pt\"\n",
    "    all_recons_path = f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\"\n",
    "    subj = 1\n",
    "    \n",
    "    cache_dir = \"/weka/proj-medarc/shared/cache\"\n",
    "    data_path = \"/weka/proj-medarc/shared/mindeyev2_dataset\"\n",
    "    \n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    jupyter_args = f\"--model_name={model_name} --subj={subj} --data_path={data_path} --cache_dir={cache_dir} --all_recons_path={all_recons_path} --mode {mode} \\\n",
    "                    --criteria=all --imagery_data_path=/weka/proj-medarc/shared/umn-imagery\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb8120cd-f226-4e2c-a6c5-3cd8ef6e9bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--all_recons_path\", type=str,\n",
    "    help=\"Path to where all_recons.pt is stored\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Evaluate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--criteria\",type=str, default=\"all\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--imagery_data_path\",type=str, default=None\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "\n",
    "if not imagery_data_path:\n",
    "    imagery_data_path = data_path\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d66b33-b327-4895-a861-ecc6ccc51296",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997f9672-b74d-4dcf-b4d7-a593fdce9cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mode == \"synthetic\":\n",
    "    all_images = torch.zeros((284, 3, 714, 1360))\n",
    "    all_images[:220] = torch.load(f\"{imagery_data_path}/nsddata_stimuli/stimuli/nsdsynthetic/nsd_synthetic_stim_part1.pt\")\n",
    "    #The last 64 stimuli are slightly different for each subject, so we load these separately for each subject\n",
    "    all_images[220:] = torch.load(f\"{imagery_data_path}/nsddata_stimuli/stimuli/nsdsynthetic/nsd_synthetic_stim_part2_sub{subj}.pt\")\n",
    "else:\n",
    "    all_images = torch.load(f\"{imagery_data_path}/nsddata_stimuli/stimuli/imagery_stimuli_18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be66f9c9-f25a-48d9-9e9a-272ab33d20ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_recons_path: evals/p_trained_subj01_40sess_hypatia_new_vd_dual_proj/p_trained_subj01_40sess_hypatia_new_vd_dual_proj_all_recons_vision.pt\n",
      "all_recons_mult.shape: torch.Size([18, 1, 3, 512, 512])\n",
      "all_clipvoxels_mult.shape: torch.Size([18, 257, 768])\n",
      "p_trained_subj01_40sess_hypatia_new_vd_dual_proj_all_recons_vision\n",
      "torch.Size([18, 3, 425, 425]) torch.Size([18, 1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"all_recons_path:\", all_recons_path)\n",
    "print(\"all_recons_path:\", all_recons_path)\n",
    "\n",
    "# Determine the target image dimension\n",
    "target_dim = 512\n",
    "final_recons = torch.load(all_recons_path)\n",
    "# Resize the images if necessary\n",
    "if final_recons.shape[-1] != target_dim:\n",
    "    resize_transform = transforms.Resize((target_dim, target_dim))\n",
    "    final_recons_resized = torch.zeros((18, 10, 3, target_dim, target_dim))\n",
    "    for sample in range(18):\n",
    "        for frame in range(10):\n",
    "            final_recons_resized[sample, frame] = resize_transform(final_recons[sample, frame])\n",
    "    final_recons = final_recons_resized\n",
    "    \n",
    "\n",
    "print(\"final_recons.shape:\", final_recons.shape)\n",
    "\n",
    "# Residual submodule\n",
    "try:\n",
    "    all_clipvoxels_mult = torch.load(f\"evals/{model_name}/{model_name}_all_clipvoxels_{mode}.pt\").reshape((18, 257, 768))\n",
    "    print(\"all_clipvoxels_mult.shape:\", all_clipvoxels_mult.shape)\n",
    "    clip_enabled = True\n",
    "except:\n",
    "    clip_enabled = False\n",
    "# Low-level submodule\n",
    "if blurry_recon:\n",
    "    all_blurryrecons_mult = torch.load(f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}.pt\")\n",
    "\n",
    "# model name\n",
    "model_name_plus_suffix = f\"{model_name}_all_recons_{mode}\"\n",
    "print(model_name_plus_suffix)\n",
    "print(all_images.shape, final_recons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9a60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truths, if using NSD-Imagery, we load only the first 12 because the last 6 are conceptual stimuli, for which there was no \"ground truth image\" to calculate statistics against\n",
    "if mode != \"synthetic\":\n",
    "    all_images = all_images[:12]\n",
    "    final_recons = final_recons[:12]\n",
    "    if clip_enabled:\n",
    "        all_clipvoxels = all_clipvoxels[:12]\n",
    "    if blurry_recon:\n",
    "        all_blurryrecons = all_blurryrecons[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26e124-2444-434d-a399-d03c2c90cc08",
   "metadata": {},
   "source": [
    "## 2-way identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e1778ff-5d6a-4087-b59f-0f44b9e0eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def two_way_identification(all_recons, all_images, model, preprocess, feature_layer=None, return_avg=False):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    # Each row: features of an image\n",
    "    # Transpose to have variables as columns\n",
    "    reals_T = reals.T\n",
    "    preds_T = preds.T\n",
    "    r = np.corrcoef(reals_T, preds_T, rowvar=False)\n",
    "    \n",
    "    # Extract correlations between reals and preds\n",
    "    N = len(all_images)\n",
    "    r = r[:N, N:]  # Shape (N, N)\n",
    "    \n",
    "    # Get congruent correlations (diagonal elements)\n",
    "    congruents = np.diag(r)\n",
    "    \n",
    "    # For each reconstructed image, compare its correlation with the correct original image\n",
    "    # vs. other original images\n",
    "    success_counts = []\n",
    "    total_comparisons = N - 1  # Exclude self-comparison\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Correlations of reconstructed image i with all original images\n",
    "        correlations = r[:, i]\n",
    "        # Correlation with the correct original image\n",
    "        congruent = congruents[i]\n",
    "        # Count how many times the correlation with other images is less than the congruent correlation\n",
    "        successes = np.sum(correlations < congruent) - 1  # Subtract 1 to exclude the self-comparison\n",
    "        success_rate = successes / total_comparisons\n",
    "        success_counts.append(success_rate)\n",
    "    \n",
    "    if return_avg:\n",
    "        # Return the average success rate\n",
    "        return np.mean(success_counts)\n",
    "    else:\n",
    "        # Return the list of success rates per reconstructed image\n",
    "        return success_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6be966-52ef-4cf6-8078-8d2d9617564b",
   "metadata": {},
   "source": [
    "## PixCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e17ea38-a254-4e90-a910-711734fdd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pixcorr = transforms.Compose([\n",
    "    transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "def get_pix_corr(all_images, all_recons, return_avg=False):\n",
    "\n",
    "    \n",
    "    # Flatten images while keeping the batch dimension\n",
    "    all_images_flattened = preprocess_pixcorr(all_images).reshape(len(all_images), -1).cpu()\n",
    "    all_recons_flattened = preprocess_pixcorr(all_recons).view(len(all_recons), -1).cpu()\n",
    "    \n",
    "    correlations = []\n",
    "    for i in range(len(all_images)):\n",
    "        correlations.append(np.corrcoef(all_images_flattened[i], all_recons_flattened[i])[0][1])\n",
    "    if return_avg:\n",
    "        return np.mean(correlations)\n",
    "    else:\n",
    "        return correlations\n",
    "    \n",
    "preprocess_ssim = transforms.Compose([\n",
    "    transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a556d5b-33a2-44aa-b48d-4b168316bbdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2326fc4c-1248-4d0f-9176-218c6460f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_ssim = transforms.Compose([\n",
    "    transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR), \n",
    "])\n",
    "\n",
    "def get_ssim(all_images, all_recons, return_avg=False):\n",
    "\n",
    "    \n",
    "    # convert image to grayscale with rgb2grey\n",
    "    img_gray = rgb2gray(preprocess_ssim(all_images).permute((0,2,3,1)).cpu())\n",
    "    recon_gray = rgb2gray(preprocess_ssim(all_recons).permute((0,2,3,1)).cpu())\n",
    "    \n",
    "    ssim_score=[]\n",
    "    for im,rec in zip(img_gray,recon_gray):\n",
    "        ssim_score.append(structural_similarity(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "    if return_avg:\n",
    "        return np.mean(ssim_score)\n",
    "    else:\n",
    "        return ssim_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35138520-ec00-48a6-90dc-249a32a783d2",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b45cc6c-ab80-43e2-b446-c8fcb4fc54e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "\n",
    "alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4','features.11']).to(device)\n",
    "alex_model.eval().requires_grad_(False)\n",
    "preprocess_alexnet = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def get_alexnet(all_images, all_recons, return_avg=False):\n",
    "    #AlexNet(2)\n",
    "    alexnet2 = two_way_identification(all_recons.to(device).float(), all_images, \n",
    "                                                            alex_model, preprocess_alexnet, 'features.4', return_avg=return_avg)\n",
    "    \n",
    "    #AlexNet(5)\n",
    "    alexnet5 = two_way_identification(all_recons.to(device).float(), all_images, \n",
    "                                                            alex_model, preprocess_alexnet, 'features.11', return_avg=return_avg)\n",
    "    return alexnet2, alexnet5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296bab2-d106-469e-b997-b32d21a2cf01",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a9c1b2b-af2a-476d-a1ac-32ee915ac2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torchvision/models/feature_extraction.py:174: UserWarning: NOTE: The nodes obtained by tracing the model in eval mode are a subsequence of those obtained in train mode. When choosing nodes for feature extraction, you may need to specify output nodes for train and eval mode separately.\n",
      "  warnings.warn(msg + suggestion_msg)\n"
     ]
    }
   ],
   "source": [
    "weights = Inception_V3_Weights.DEFAULT\n",
    "inception_model = create_feature_extractor(inception_v3(weights=weights), \n",
    "                                        return_nodes=['avgpool']).to(device)\n",
    "inception_model.eval().requires_grad_(False)\n",
    "preprocess_inception = transforms.Compose([\n",
    "    transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def get_inceptionv3(all_images, all_recons, return_avg=False):\n",
    "    \n",
    "    inception = two_way_identification(all_recons.float(), all_images.float(),\n",
    "                                            inception_model, preprocess_inception, 'avgpool', return_avg=return_avg)\n",
    "            \n",
    "    return inception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a25f7f-8298-4413-b512-8a1173413e07",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6afbf7ce-8793-4988-a328-a632acd88aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "preprocess_clip = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                        std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "def get_clip(all_images, all_recons, return_avg=False):\n",
    "    clip_2way = two_way_identification(all_recons, all_images,\n",
    "                                            clip_model.encode_image, preprocess_clip, None, return_avg=return_avg) # final layer\n",
    "    return clip_2way\n",
    "\n",
    "def get_clip_cosine(final_embeds, gt_embeds):\n",
    "    # Get the cosine similarity between the clip embeddings\n",
    "    # of the final recons and the ground truth images\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    cos_sim = [float(value) for value in cos(final_embeds, gt_embeds)]\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fed9f8-ef1a-4c6d-a83f-2a934b6e87fd",
   "metadata": {},
   "source": [
    "## Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14143c0f-1b32-43ef-98d8-8ed458df4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = EfficientNet_B1_Weights.DEFAULT\n",
    "eff_model = create_feature_extractor(efficientnet_b1(weights=weights), \n",
    "                                    return_nodes=['avgpool'])\n",
    "eff_model.eval().requires_grad_(False)\n",
    "preprocess_efficientnet = transforms.Compose([\n",
    "    transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def get_efficientnet(all_images, all_recons, return_avg=False):\n",
    "    # see weights.transforms()\n",
    "\n",
    "    \n",
    "    gt = eff_model(preprocess_efficientnet(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "    fake = eff_model(preprocess_efficientnet(all_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "    \n",
    "    effnet = [sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]\n",
    "    if return_avg:\n",
    "        return np.mean(effnet)\n",
    "    else:\n",
    "        return effnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f669d-cab7-4c75-90cd-651283f65a9e",
   "metadata": {},
   "source": [
    "## SwAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c60b0c4-79fe-4cff-95e9-99733c821e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /admin/home-ckadirt/.cache/torch/hub/facebookresearch_swav_main\n",
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "\n",
    "swav_model = create_feature_extractor(swav_model, \n",
    "                                    return_nodes=['avgpool'])\n",
    "swav_model.eval().requires_grad_(False)\n",
    "preprocess_swav = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def get_swav(all_images, all_recons, return_avg=False):\n",
    "    gt = swav_model(preprocess_swav(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "    fake = swav_model(preprocess_swav(all_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "    \n",
    "    swav = [sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]\n",
    "    if return_avg:\n",
    "        return np.mean(swav)\n",
    "    else:\n",
    "        return swav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f644d-5e4e-46ec-890c-f703b62f0c3b",
   "metadata": {},
   "source": [
    "# Brain Correlation\n",
    "### Load brain data, brain masks, image lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36c14558-fb89-426d-af2f-1f555f8025d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 1, 15724]) torch.Size([18, 3, 425, 425])\n"
     ]
    }
   ],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, stimulus = utils.load_nsd_synthetic(subject=subj, average=False, nest=True, data_root=imagery_data_path)\n",
    "else:\n",
    "    voxels, _ = utils.load_nsd_mental_imagery(subject=subj, mode=mode, stimtype=\"all\", average=True, nest=False, data_root=imagery_data_path)\n",
    "    voxels = voxels[:12]\n",
    "num_voxels = voxels.shape[-1]\n",
    "num_test = voxels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8df5ebf-014d-46bb-9fb4-c0577e544e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brain region masks\n",
    "try:\n",
    "    brain_region_masks = {}\n",
    "    with h5py.File(f\"{cache_dir}/brain_region_masks.hdf5\", \"r\") as file:\n",
    "        # Iterate over each subject\n",
    "        for subject in file.keys():\n",
    "            subject_group = file[subject]\n",
    "            # Load the masks data for each subject\n",
    "            subject_masks = {\"nsd_general\" : subject_group[\"nsd_general\"][:],\n",
    "                             \"V1\" : subject_group[\"V1\"][:], \n",
    "                             \"V2\" : subject_group[\"V2\"][:], \n",
    "                             \"V3\" : subject_group[\"V3\"][:], \n",
    "                             \"V4\" : subject_group[\"V4\"][:],\n",
    "                             \"higher_vis\" : subject_group[\"higher_vis\"][:]}\n",
    "            brain_region_masks[subject] = subject_masks\n",
    "    subject_masks = brain_region_masks[f\"subj0{subj}\"]\n",
    "except: \n",
    "    brain_region_masks = {}\n",
    "    with h5py.File(f\"{data_path}/brain_region_masks.hdf5\", \"r\") as file:\n",
    "        # Iterate over each subject\n",
    "        for subject in file.keys():\n",
    "            subject_group = file[subject]\n",
    "            # Load the masks data for each subject\n",
    "            subject_masks = {\"nsd_general\" : subject_group[\"nsd_general\"][:],\n",
    "                             \"V1\" : subject_group[\"V1\"][:], \n",
    "                             \"V2\" : subject_group[\"V2\"][:], \n",
    "                             \"V3\" : subject_group[\"V3\"][:], \n",
    "                             \"V4\" : subject_group[\"V4\"][:],\n",
    "                             \"higher_vis\" : subject_group[\"higher_vis\"][:]}\n",
    "            brain_region_masks[subject] = subject_masks\n",
    "    subject_masks = brain_region_masks[f\"subj0{subj}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f27d3-5e45-496b-b666-b437e916c7f1",
   "metadata": {},
   "source": [
    "### Calculate Brain Correlation scores for each brain area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "528db098-6977-4dbd-9bc6-dfd6a96e0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import PearsonCorrCoef\n",
    "\n",
    "try:\n",
    "    GNet = GNet8_Encoder(device=device,subject=subj,model_path=f\"{cache_dir}/gnet_multisubject.pt\")\n",
    "except:\n",
    "    GNet = GNet8_Encoder(device=device,subject=subj,model_path=f\"{data_path}/gnet_multisubject.pt\")\n",
    "    \n",
    "\n",
    "def get_brain_correlation(subject_masks, all_recons, return_avg=False):\n",
    "\n",
    "    # Prepare image list for input to GNet\n",
    "    recon_list = []\n",
    "    for i in range(all_recons.shape[0]):\n",
    "        img = all_recons[i].detach()\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        recon_list.append(img)\n",
    "        \n",
    "    PeC = PearsonCorrCoef(num_outputs=len(recon_list))\n",
    "    beta_primes = GNet.predict(recon_list)\n",
    "    \n",
    "    region_brain_correlations = {}\n",
    "    for region, mask in subject_masks.items():\n",
    "        score = PeC(voxels[:,0,mask].moveaxis(0,1), beta_primes[:,mask].moveaxis(0,1))\n",
    "        score = score.tolist()\n",
    "        if return_avg:\n",
    "            region_brain_correlations[region] = float(torch.mean(score))\n",
    "        else:\n",
    "            region_brain_correlations[region] = score\n",
    "    return region_brain_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb03a824-6e32-4998-9b4b-6468d5cc99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.22s/it]\n"
     ]
    }
   ],
   "source": [
    "metrics_data = {\n",
    "            \"index_sample\": [],\n",
    "            \"index_image\": [],\n",
    "            \"PixCorr\": [],\n",
    "            \"SSIM\": [],\n",
    "            \"AlexNet(2)\": [],\n",
    "            \"AlexNet(5)\": [],\n",
    "            \"InceptionV3\": [],\n",
    "            \"CLIP\": [],\n",
    "            \"EffNet-B\": [],\n",
    "            \"SwAV\": [],\n",
    "            \"Brain Corr. nsd_general\": [],\n",
    "            \"Brain Corr. V1\": [],\n",
    "            \"Brain Corr. V2\": [],\n",
    "            \"Brain Corr. V3\": [],\n",
    "            \"Brain Corr. V4\": [],\n",
    "            \"Brain Corr. higher_vis\": [],\n",
    "        }\n",
    "\n",
    "# Iterate over each sample and compute metrics with tqdm and suppressed output\n",
    "for repetition in tqdm(range(final_recons.shape[1]), desc=\"Processing samples\", file=sys.stdout):\n",
    "    with open(os.devnull, 'w') as fnull, contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
    "        rep_recons = final_recons[:, repetition]\n",
    "\n",
    "        pixcorr = get_pix_corr(all_images, rep_recons)\n",
    "        ssim = get_ssim(all_images, rep_recons)\n",
    "        alexnet2, alexnet5 = get_alexnet(all_images, rep_recons)\n",
    "        inception = get_inceptionv3(all_images, rep_recons)\n",
    "        clip = get_clip(all_images, rep_recons)\n",
    "        effnet = get_efficientnet(all_images, rep_recons)\n",
    "        swav = get_swav(all_images, rep_recons)\n",
    "        region_brain_correlations = get_brain_correlation(subject_masks, rep_recons)\n",
    "\n",
    "        # Append each result to its corresponding list, and store the image index\n",
    "        \n",
    "        metrics_data[\"index_sample\"].extend(list(range(final_recons.shape[0])))\n",
    "        metrics_data[\"index_image\"].extend([repetition for _ in range(final_recons.shape[0])])\n",
    "        metrics_data[\"PixCorr\"].extend(pixcorr)\n",
    "        metrics_data[\"SSIM\"].extend(ssim)\n",
    "        metrics_data[\"AlexNet(2)\"].extend(alexnet2)\n",
    "        metrics_data[\"AlexNet(5)\"].extend(alexnet5)\n",
    "        metrics_data[\"InceptionV3\"].extend(inception)\n",
    "        metrics_data[\"CLIP\"].extend(clip)\n",
    "        metrics_data[\"EffNet-B\"].extend(effnet)\n",
    "        metrics_data[\"SwAV\"].extend(swav)\n",
    "        metrics_data[\"Brain Corr. nsd_general\"].extend(region_brain_correlations[\"nsd_general\"])\n",
    "        metrics_data[\"Brain Corr. V1\"].extend(region_brain_correlations[\"V1\"])\n",
    "        metrics_data[\"Brain Corr. V2\"].extend(region_brain_correlations[\"V2\"])\n",
    "        metrics_data[\"Brain Corr. V3\"].extend(region_brain_correlations[\"V3\"])\n",
    "        metrics_data[\"Brain Corr. V4\"].extend(region_brain_correlations[\"V4\"])\n",
    "        metrics_data[\"Brain Corr. higher_vis\"].extend(region_brain_correlations[\"higher_vis\"])\n",
    "\n",
    "# Check that all lists have the same length before creating DataFrame\n",
    "lengths = [len(values) for values in metrics_data.values()]\n",
    "if len(set(lengths)) != 1:\n",
    "    print(\"Error: Not all metric lists have the same length\")\n",
    "    for metric, values in metrics_data.items():\n",
    "        print(f\"{metric}: {len(values)} items\")\n",
    "else:\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    # Save the table to a CSV file\n",
    "    os.makedirs('tables/', exist_ok=True)\n",
    "    df.to_csv(f'tables/{model_name_plus_suffix}.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6ae46-3de9-4abb-ae2d-c8998d83d6f2",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye_imagery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
