{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5f265e-407a-40bd-92fb-a652091fd7ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "PID of this process = 14903\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import contextlib\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import CLIPModel, AutoTokenizer, AutoProcessor\n",
    "# import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from models import GNet8_Encoder\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffb659a-8154-4536-ab27-2d976da1bf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: test_pretrained_subj01_40sess_hypatia_pg_sessions40\n",
      "--model_name=test_pretrained_subj01_40sess_hypatia_pg_sessions40 --subj=1 --data_path=../dataset --cache_dir=../cache/ --all_recons_path=evals/test_pretrained_subj01_40sess_hypatia_pg_sessions40/test_pretrained_subj01_40sess_hypatia_pg_sessions40_all_recons_imagery.pt --mode imagery                     --criteria=all\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"test_pretrained_subj01_40sess_hypatia_pg_sessions40\"\n",
    "    # model_name = \"pretest_pretrained_subj01_40sess_hypatia_pg_sessions40\"\n",
    "    mode = \"imagery\"\n",
    "    # all_recons_path = f\"evals/{model_name}/{model_name}_all_enhancedrecons_{mode}.pt\"\n",
    "    all_recons_path = f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\"\n",
    "    subj = 1\n",
    "    \n",
    "    cache_dir = \"/weka/proj-medarc/shared/cache\"\n",
    "    data_path = \"/weka/proj-medarc/shared/mindeyev2_dataset\"\n",
    "    \n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    jupyter_args = f\"--model_name={model_name} --subj={subj} --data_path={data_path} --cache_dir={cache_dir} --all_recons_path={all_recons_path} --mode {mode} \\\n",
    "                    --criteria=all\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb8120cd-f226-4e2c-a6c5-3cd8ef6e9bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--all_recons_path\", type=str,\n",
    "    help=\"Path to where all_recons.pt is stored\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Evaluate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--criteria\",type=str, default=\"all\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d66b33-b327-4895-a861-ecc6ccc51296",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997f9672-b74d-4dcf-b4d7-a593fdce9cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mode == \"synthetic\":\n",
    "    all_images = torch.zeros((284, 3, 714, 1360))\n",
    "    all_images[:220] = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/nsdsynthetic/nsd_synthetic_stim_part1.pt\")\n",
    "    #The last 64 stimuli are slightly different for each subject, so we load these separately for each subject\n",
    "    all_images[220:] = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/nsdsynthetic/nsd_synthetic_stim_part2_sub{subj}.pt\")\n",
    "else:\n",
    "    all_images = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/imagery_stimuli_18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be66f9c9-f25a-48d9-9e9a-272ab33d20ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_recons_path: evals/test_pretrained_subj01_40sess_hypatia_pg_sessions40/test_pretrained_subj01_40sess_hypatia_pg_sessions40_all_recons_imagery.pt\n",
      "test_pretrained_subj01_40sess_hypatia_pg_sessions40_all_recons_imagery\n",
      "torch.Size([18, 3, 425, 425]) torch.Size([18, 10, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"all_recons_path:\", all_recons_path)\n",
    "print(\"all_recons_path:\", all_recons_path)\n",
    "\n",
    "# Determine the target image dimension\n",
    "target_dim = 512\n",
    "all_recons_mult = torch.load(all_recons_path)\n",
    "# Resize the images if necessary\n",
    "if all_recons_mult.shape[-1] != target_dim:\n",
    "    resize_transform = transforms.Resize((target_dim, target_dim))\n",
    "    all_recons_mult_resized = torch.zeros((18, 10, 3, target_dim, target_dim))\n",
    "    for sample in range(18):\n",
    "        for frame in range(10):\n",
    "            all_recons_mult_resized[sample, frame] = resize_transform(all_recons_mult[sample, frame])\n",
    "    all_recons_mult = all_recons_mult_resized\n",
    "    \n",
    "\n",
    "print(\"all_recons_mult.shape:\", all_recons_mult.shape)\n",
    "\n",
    "# Residual submodule\n",
    "try:\n",
    "    all_clipvoxels_mult = torch.load(f\"evals/{model_name}/{model_name}_all_clipvoxels_{mode}.pt\").reshape((18, 257, 768))\n",
    "    print(\"all_clipvoxels_mult.shape:\", all_clipvoxels_mult.shape)\n",
    "    clip_enabled = True\n",
    "except:\n",
    "    clip_enabled = False\n",
    "# Low-level submodule\n",
    "if blurry_recon:\n",
    "    all_blurryrecons_mult = torch.load(f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}.pt\")\n",
    "\n",
    "# model name\n",
    "model_name_plus_suffix = f\"{model_name}_all_recons_{mode}\"\n",
    "print(model_name_plus_suffix)\n",
    "print(all_images.shape, all_recons_mult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e783a0-6d8f-4116-9fe7-b3dac7e354c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create full grid of recon comparisons\n",
    "# from PIL import Image\n",
    "\n",
    "# imsize = 150\n",
    "# if all_images.shape[-1] != imsize:\n",
    "#     all_images = transforms.Resize((imsize,imsize))(transforms.CenterCrop(all_images.shape[2])(all_images)).float()\n",
    "# if all_recons.shape[-1] != imsize:\n",
    "#     all_recons = transforms.Resize((imsize,imsize))(transforms.CenterCrop(all_images.shape[2])(all_recons)).float()\n",
    "\n",
    "# num_images = all_recons.shape[0]\n",
    "# num_rows = (2 * num_images + 11) // 12\n",
    "\n",
    "# # Interleave tensors\n",
    "# merged = torch.stack([val for pair in zip(all_images, all_recons) for val in pair], dim=0)\n",
    "\n",
    "# # Calculate grid size\n",
    "# grid = torch.zeros((num_rows * 12, 3, all_recons.shape[-1], all_recons.shape[-1]))\n",
    "\n",
    "# # Populate the grid\n",
    "# grid[:2*num_images] = merged\n",
    "# grid_images = [transforms.functional.to_pil_image(grid[i]) for i in range(num_rows * 12)]\n",
    "\n",
    "# # Create the grid image\n",
    "# grid_image = Image.new('RGB', (all_recons.shape[-1]*12, all_recons.shape[-1] * num_rows))  # 10 images wide\n",
    "\n",
    "# # Paste images into the grid\n",
    "# for i, img in enumerate(grid_images):\n",
    "#     grid_image.paste(img, (all_recons.shape[-1] * (i % 12), all_recons.shape[-1] * (i // 12)))\n",
    "\n",
    "# grid_image.save(f\"../figs/{model_name_plus_suffix}_{len(all_recons)}recons.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9a60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ground truths, if using NSD-Imagery, we load only the first 12 because the last 6 are conceptual stimuli, for which there was no \"ground truth image\" to calculate statistics against\n",
    "# if mode != \"synthetic\":\n",
    "#     all_images = all_images[:12]\n",
    "#     all_recons = all_recons[:12]\n",
    "#     all_clipvoxels = all_clipvoxels[:12]\n",
    "#     if blurry_recon:\n",
    "#         all_blurryrecons = all_blurryrecons[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42009e9-f910-4f02-8db6-d46778aa6595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imsize = 256\n",
    "# if all_images.shape[-1] != imsize:\n",
    "#     all_images = transforms.Resize((imsize,imsize))(all_images).float()\n",
    "# if all_recons.shape[-1] != imsize:\n",
    "#     all_recons = transforms.Resize((imsize,imsize))(all_recons).float()\n",
    "# if blurry_recon:\n",
    "#     if all_blurryrecons.shape[-1] != imsize:\n",
    "#         all_blurryrecons = transforms.Resize((imsize,imsize))(all_blurryrecons).float()\n",
    "    \n",
    "# if \"enhanced\" in model_name_plus_suffix and blurry_recon:\n",
    "#     print(\"weighted averaging to improve low-level evals\")\n",
    "#     all_recons = all_recons*.75 + all_blurryrecons*.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "434b33b5-c799-4054-889c-ac74663d31ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 2 / 117 / 231 / 164 / 619 / 791\n",
    "# import textwrap\n",
    "# def wrap_title(title, wrap_width):\n",
    "#     return \"\\n\".join(textwrap.wrap(title, wrap_width))\n",
    "\n",
    "# fig, axes = plt.subplots(4,6, figsize=(12,8))\n",
    "# index = 0\n",
    "# for j in range(4):\n",
    "#     for k in range(6):\n",
    "#         if k%2==0:\n",
    "#             axes[j][k].imshow(utils.torch_to_Image(all_images[index]))\n",
    "#             axes[j][k].axis('off')\n",
    "#         else:\n",
    "#             axes[j][k].imshow(utils.torch_to_Image(all_recons[index]))\n",
    "#             axes[j][k].axis('off')\n",
    "#             index +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4deb53-4d85-4292-92c5-bb59077523cf",
   "metadata": {},
   "source": [
    "# Retrieval eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f862220-1df3-49b0-a36c-0aa2f168315d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load embedding model\n",
    "# clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "#     arch=\"ViT-bigG-14\",\n",
    "#     version=\"laion2b_s39b_b160k\",\n",
    "#     output_tokens=True,\n",
    "#     only_tokens=True,\n",
    "# )\n",
    "# clip_img_embedder.to(device)\n",
    "\n",
    "# clip_seq_dim = 256\n",
    "# clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e49d57a-9b65-490c-a1e0-61bd05682171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# def get_retrieval_eval(all_images_o, all_clipvoxels_o, plot = False):\n",
    "#     all_clipvoxels = all_clipvoxels_o.detach().cpu()\n",
    "#     all_images = all_images_o.detach().cpu()\n",
    "#     percent_correct_fwds, percent_correct_bwds = [], []\n",
    "#     percent_correct_fwd, percent_correct_bwd = None, None\n",
    "    \n",
    "#     with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "#         for test_i, loop in enumerate(tqdm(range(30))):\n",
    "#             random_samps = np.random.choice(np.arange(len(all_images)), size=4, replace=False)\n",
    "#             emb = clip_img_embedder.embed_image(all_images[random_samps].to(device)).float() # CLIP-Image\n",
    "#             emb_ = all_clipvoxels[random_samps].to(device).float() # CLIP-Brain\n",
    "    \n",
    "#             # flatten if necessary\n",
    "#             emb = emb.reshape(len(emb),-1)\n",
    "#             emb_ = emb_.reshape(len(emb_),-1)\n",
    "    \n",
    "#             # l2norm \n",
    "#             emb = nn.functional.normalize(emb,dim=-1)\n",
    "#             emb_ = nn.functional.normalize(emb_,dim=-1)\n",
    "    \n",
    "#             labels = torch.arange(len(emb)).to(device)\n",
    "#             print(emb.shape, emb_.shape)\n",
    "#             bwd_sim = utils.batchwise_cosine_similarity(emb,emb_)  # clip, brain\n",
    "#             fwd_sim = utils.batchwise_cosine_similarity(emb_,emb)  # brain, clip\n",
    "    \n",
    "#             assert len(bwd_sim) == 4\n",
    "    \n",
    "#             percent_correct_fwds = np.append(percent_correct_fwds, utils.topk(fwd_sim, labels,k=1).item())\n",
    "#             percent_correct_bwds = np.append(percent_correct_bwds, utils.topk(bwd_sim, labels,k=1).item())\n",
    "    \n",
    "#             if test_i==0:\n",
    "#                 print(\"Loop 0:\",percent_correct_fwds, percent_correct_bwds)\n",
    "                \n",
    "#     percent_correct_fwd = np.mean(percent_correct_fwds)\n",
    "#     fwd_sd = np.std(percent_correct_fwds) / np.sqrt(len(percent_correct_fwds))\n",
    "#     fwd_ci = stats.norm.interval(0.95, loc=percent_correct_fwd, scale=fwd_sd)\n",
    "    \n",
    "#     percent_correct_bwd = np.mean(percent_correct_bwds)\n",
    "#     bwd_sd = np.std(percent_correct_bwds) / np.sqrt(len(percent_correct_bwds))\n",
    "#     bwd_ci = stats.norm.interval(0.95, loc=percent_correct_bwd, scale=bwd_sd)\n",
    "    \n",
    "#     print(f\"fwd percent_correct: {percent_correct_fwd:.4f} 95% CI: [{fwd_ci[0]:.4f},{fwd_ci[1]:.4f}]\")\n",
    "#     print(f\"bwd percent_correct: {percent_correct_bwd:.4f} 95% CI: [{bwd_ci[0]:.4f},{bwd_ci[1]:.4f}]\")\n",
    "    \n",
    "#     fwd_sim = np.array(fwd_sim.cpu())\n",
    "#     bwd_sim = np.array(bwd_sim.cpu())\n",
    "\n",
    "#     if plot:\n",
    "#         print(\"Given Brain embedding, find correct Image embedding\")\n",
    "#         fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(11,12))\n",
    "#         for trial in range(4):\n",
    "#             ax[trial, 0].imshow(utils.torch_to_Image(all_images[random_samps][trial]))\n",
    "#             ax[trial, 0].set_title(\"original\\nimage\")\n",
    "#             ax[trial, 0].axis(\"off\")\n",
    "#             for attempt in range(3):\n",
    "#                 which = np.flip(np.argsort(fwd_sim[trial]))[attempt]\n",
    "#                 ax[trial, attempt+1].imshow(utils.torch_to_Image(all_images[random_samps][which]))\n",
    "#                 ax[trial, attempt+1].set_title(f\"Top {attempt+1}\")\n",
    "#                 ax[trial, attempt+1].axis(\"off\")\n",
    "#         fig.tight_layout()\n",
    "#         plt.show()\n",
    "#     return fwd_sim, bwd_sim, percent_correct_fwd, percent_correct_bwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26e124-2444-434d-a399-d03c2c90cc08",
   "metadata": {},
   "source": [
    "## 2-way identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e1778ff-5d6a-4087-b59f-0f44b9e0eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "@torch.no_grad()\n",
    "def two_way_identification(all_recons, all_images, model, preprocess, feature_layer=None, return_avg=True):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "\n",
    "    r = np.corrcoef(reals, preds)\n",
    "    r = r[:len(all_images), len(all_images):]\n",
    "    congruents = np.diag(r)\n",
    "\n",
    "    success = r < congruents\n",
    "    success_cnt = np.sum(success, 0)\n",
    "\n",
    "    if return_avg:\n",
    "        perf = np.mean(success_cnt) / (len(all_images)-1)\n",
    "        return perf\n",
    "    else:\n",
    "        return success_cnt, len(all_images)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6be966-52ef-4cf6-8078-8d2d9617564b",
   "metadata": {},
   "source": [
    "## PixCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e17ea38-a254-4e90-a910-711734fdd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pixcorr = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "def get_pix_corr(all_images, all_recons):\n",
    "\n",
    "    \n",
    "    # Flatten images while keeping the batch dimension\n",
    "    all_images_flattened = preprocess_pixcorr(all_images).reshape(len(all_images), -1).cpu()\n",
    "    all_recons_flattened = preprocess_pixcorr(all_recons).view(len(all_recons), -1).cpu()\n",
    "    \n",
    "    print(all_images_flattened.shape)\n",
    "    print(all_recons_flattened.shape)\n",
    "    \n",
    "    corrsum = 0\n",
    "    for i in tqdm(range(len(all_images))):\n",
    "        corrsum += np.corrcoef(all_images_flattened[i], all_recons_flattened[i])[0][1]\n",
    "    corrmean = corrsum / len(all_images)\n",
    "    \n",
    "    pixcorr = corrmean\n",
    "    print(pixcorr)\n",
    "    return pixcorr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a556d5b-33a2-44aa-b48d-4b168316bbdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2326fc4c-1248-4d0f-9176-218c6460f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/zijin-gu/meshconv-decoding/issues/3\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "preprocess_ssim = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR), \n",
    "])\n",
    "\n",
    "def get_ssim(all_images, all_recons):\n",
    "\n",
    "    \n",
    "    # convert image to grayscale with rgb2grey\n",
    "    img_gray = rgb2gray(preprocess_ssim(all_images).permute((0,2,3,1)).cpu())\n",
    "    recon_gray = rgb2gray(preprocess_ssim(all_recons).permute((0,2,3,1)).cpu())\n",
    "    print(\"converted, now calculating ssim...\")\n",
    "    \n",
    "    ssim_score=[]\n",
    "    for im,rec in tqdm(zip(img_gray,recon_gray),total=len(all_images)):\n",
    "        ssim_score.append(structural_similarity(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "    \n",
    "    ssim = np.mean(ssim_score)\n",
    "    print(ssim)\n",
    "    return ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35138520-ec00-48a6-90dc-249a32a783d2",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b45cc6c-ab80-43e2-b446-c8fcb4fc54e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/fmri/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/admin/home-ckadirt/fmri/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/admin/home-ckadirt/fmri/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/admin/home-ckadirt/fmri/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "\n",
    "alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4','features.11']).to(device)\n",
    "alex_model.eval().requires_grad_(False)\n",
    "preprocess_alexnet = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def get_alexnet(all_images, all_recons):\n",
    "    # see alex_weights.transforms()\n",
    "\n",
    "    \n",
    "    layer = 'early, AlexNet(2)'\n",
    "    print(f\"\\n---{layer}---\")\n",
    "    all_per_correct = two_way_identification(all_recons.to(device).float(), all_images, \n",
    "                                                              alex_model, preprocess_alexnet, 'features.4')\n",
    "    alexnet2 = np.mean(all_per_correct)\n",
    "    print(f\"2-way Percent Correct: {alexnet2:.4f}\")\n",
    "    \n",
    "    layer = 'mid, AlexNet(5)'\n",
    "    print(f\"\\n---{layer}---\")\n",
    "    all_per_correct = two_way_identification(all_recons.to(device).float(), all_images, \n",
    "                                                              alex_model, preprocess_alexnet, 'features.11')\n",
    "    alexnet5 = np.mean(all_per_correct)\n",
    "    print(f\"2-way Percent Correct: {alexnet5:.4f}\")\n",
    "    return alexnet2, alexnet5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296bab2-d106-469e-b997-b32d21a2cf01",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9c1b2b-af2a-476d-a1ac-32ee915ac2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/fmri/lib/python3.11/site-packages/torchvision/models/feature_extraction.py:174: UserWarning: NOTE: The nodes obtained by tracing the model in eval mode are a subsequence of those obtained in train mode. When choosing nodes for feature extraction, you may need to specify output nodes for train and eval mode separately.\n",
      "  warnings.warn(msg + suggestion_msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "inception_model = create_feature_extractor(inception_v3(weights=weights), \n",
    "                                           return_nodes=['avgpool']).to(device)\n",
    "inception_model.eval().requires_grad_(False)\n",
    "preprocess_inception = transforms.Compose([\n",
    "    transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def get_inceptionv3(all_images, all_recons):\n",
    "    # see weights.transforms()\n",
    "\n",
    "    \n",
    "    all_per_correct = two_way_identification(all_recons.float(), all_images.float(),\n",
    "                                            inception_model, preprocess_inception, 'avgpool')\n",
    "            \n",
    "    inception = np.mean(all_per_correct)\n",
    "    print(f\"2-way Percent Correct: {inception:.4f}\")\n",
    "    return inception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a25f7f-8298-4413-b512-8a1173413e07",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6afbf7ce-8793-4988-a328-a632acd88aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "preprocess_clip = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "def get_clip(all_images, all_recons):\n",
    "\n",
    "    \n",
    "    all_per_correct = two_way_identification(all_recons, all_images,\n",
    "                                            clip_model.encode_image, preprocess_clip, None) # final layer\n",
    "    clip_ = np.mean(all_per_correct)\n",
    "    print(f\"2-way Percent Correct: {clip_:.4f}\")\n",
    "    return clip_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fed9f8-ef1a-4c6d-a83f-2a934b6e87fd",
   "metadata": {},
   "source": [
    "## Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14143c0f-1b32-43ef-98d8-8ed458df4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "weights = EfficientNet_B1_Weights.DEFAULT\n",
    "eff_model = create_feature_extractor(efficientnet_b1(weights=weights), \n",
    "                                    return_nodes=['avgpool'])\n",
    "eff_model.eval().requires_grad_(False)\n",
    "preprocess_efficientnet = transforms.Compose([\n",
    "    transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def get_efficientnet(all_images, all_recons):\n",
    "    # see weights.transforms()\n",
    "\n",
    "    \n",
    "    gt = eff_model(preprocess_efficientnet(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "    fake = eff_model(preprocess_efficientnet(all_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "    \n",
    "    effnet = np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean()\n",
    "    print(\"Distance:\",effnet)\n",
    "    return effnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f669d-cab7-4c75-90cd-651283f65a9e",
   "metadata": {},
   "source": [
    "## SwAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c60b0c4-79fe-4cff-95e9-99733c821e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /admin/home-ckadirt/.cache/torch/hub/facebookresearch_swav_main\n",
      "/admin/home-ckadirt/fmri/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/admin/home-ckadirt/fmri/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "swav_model = create_feature_extractor(swav_model, \n",
    "                                    return_nodes=['avgpool'])\n",
    "swav_model.eval().requires_grad_(False)\n",
    "preprocess_swav = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def get_swav(all_images, all_recons):\n",
    "    gt = swav_model(preprocess_swav(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt),-1).cpu().numpy()\n",
    "    fake = swav_model(preprocess_swav(all_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake),-1).cpu().numpy()\n",
    "    \n",
    "    swav = np.array([sp.spatial.distance.correlation(gt[i],fake[i]) for i in range(len(gt))]).mean()\n",
    "    print(\"Distance:\",swav)\n",
    "    return swav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f644d-5e4e-46ec-890c-f703b62f0c3b",
   "metadata": {},
   "source": [
    "# Brain Correlation\n",
    "### Load brain data, brain masks, image lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36c14558-fb89-426d-af2f-1f555f8025d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 1, 15724]) torch.Size([18, 3, 425, 425])\n"
     ]
    }
   ],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, stimulus = utils.load_nsd_synthetic(subject=subj, average=False, nest=True, data_root=data_path)\n",
    "else:\n",
    "    voxels, _ = utils.load_nsd_mental_imagery(subject=subj, mode=mode, stimtype=\"all\", average=True, nest=False, data_root=data_path)\n",
    "    voxels = voxels[:12]\n",
    "num_voxels = voxels.shape[-1]\n",
    "num_test = voxels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8df5ebf-014d-46bb-9fb4-c0577e544e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brain region masks\n",
    "brain_region_masks = {}\n",
    "with h5py.File(f\"{cache_dir}/brain_region_masks.hdf5\", \"r\") as file:\n",
    "    # Iterate over each subject\n",
    "    for subject in file.keys():\n",
    "        subject_group = file[subject]\n",
    "        # Load the masks data for each subject\n",
    "        subject_masks = {\"nsd_general\" : subject_group[\"nsd_general\"][:],\n",
    "                         \"V1\" : subject_group[\"V1\"][:], \n",
    "                         \"V2\" : subject_group[\"V2\"][:], \n",
    "                         \"V3\" : subject_group[\"V3\"][:], \n",
    "                         \"V4\" : subject_group[\"V4\"][:],\n",
    "                         \"higher_vis\" : subject_group[\"higher_vis\"][:]}\n",
    "        brain_region_masks[subject] = subject_masks\n",
    "subject_masks = brain_region_masks[f\"subj0{subj}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f27d3-5e45-496b-b666-b437e916c7f1",
   "metadata": {},
   "source": [
    "### Calculate Brain Correlation scores for each brain area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "528db098-6977-4dbd-9bc6-dfd6a96e0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import PearsonCorrCoef\n",
    "GNet = GNet8_Encoder(device=device,subject=subj,model_path=f\"{cache_dir}/gnet_multisubject.pt\")\n",
    "\n",
    "def get_brain_correlation(subject_masks, idx):\n",
    "\n",
    "    # Prepare image list for input to GNet\n",
    "    recon_list = []\n",
    "    for i in range(all_recons.shape[0]):\n",
    "        img = all_recons[i].detach()\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        recon_list.append(img)\n",
    "        \n",
    "    PeC = PearsonCorrCoef(num_outputs=len(recon_list))\n",
    "    beta_primes = GNet.predict(recon_list)\n",
    "    \n",
    "    region_brain_correlations = {}\n",
    "    for region, mask in subject_masks.items():\n",
    "        score = PeC(voxels[idx,0,mask].unsqueeze(0).moveaxis(0,1), beta_primes[:,mask].moveaxis(0,1))\n",
    "        region_brain_correlations[region] = float(torch.mean(score))\n",
    "    print(region_brain_correlations)\n",
    "    return region_brain_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03a824-6e32-4998-9b4b-6468d5cc99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:18<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pretrained_subj01_40sess_hypatia_pg_sessions40_all_recons_imagery\n",
      "            PixCorr      SSIM  AlexNet(2)  AlexNet(5)  InceptionV3      CLIP  EffNet-B      SwAV  FwdRetrieval  BwdRetrieval  Brain Corr. nsd_general  Brain Corr. V1  Brain Corr. V2  Brain Corr. V3  Brain Corr. V4  Brain Corr. higher_vis\n",
      "sample_1   0.051968  0.262398    0.492424    0.560606     0.560606  0.530303  0.977607  0.608589      0.766667      0.666667                 0.008858        0.008356       -0.010260        0.008070       -0.003302                0.004448\n",
      "sample_2   0.026570  0.313794    0.469697    0.401515     0.522727  0.507576  0.990517  0.603863      0.791667      0.650000                -0.017641        0.027676        0.029600       -0.001313       -0.032571               -0.027260\n",
      "sample_3  -0.046882  0.347299    0.598485    0.575758     0.575758  0.477273  0.949678  0.601224      0.741667      0.650000                 0.052418        0.062053        0.058920        0.049340        0.014088                0.029857\n",
      "sample_4   0.024937  0.300203    0.530303    0.439394     0.598485  0.492424  0.990880  0.649567      0.725000      0.608333                 0.018922        0.038289        0.032320        0.014731       -0.015811                0.015080\n",
      "sample_5   0.065889  0.339494    0.583333    0.575758     0.424242  0.477273  0.975431  0.607462      0.775000      0.708333                 0.024368        0.056732        0.062614        0.062343       -0.004229                0.006509\n",
      "sample_6   0.005240  0.342261    0.621212    0.590909     0.424242  0.484848  0.979750  0.616982      0.700000      0.625000                 0.017953        0.075784        0.089818        0.035388        0.019160               -0.007407\n",
      "sample_7   0.030224  0.309223    0.545455    0.583333     0.446970  0.500000  0.978143  0.623658      0.700000      0.641667                 0.056189        0.009283        0.056243        0.043274        0.001063                0.051972\n",
      "sample_8   0.082079  0.276329    0.553030    0.583333     0.446970  0.484848  0.992304  0.616754      0.733333      0.616667                 0.051586        0.041808        0.029536        0.047170        0.076828                0.038789\n",
      "sample_9   0.060224  0.334772    0.522727    0.507576     0.530303  0.484848  0.947626  0.586782      0.766667      0.675000                 0.034989        0.051572        0.024727        0.060564        0.020651                0.036644\n",
      "sample_10  0.013750  0.256533    0.507576    0.545455     0.492424  0.477273  0.972711  0.619977      0.741667      0.641667                 0.009936        0.031387        0.026162        0.033731        0.046573               -0.000792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_data = {\n",
    "    \"index_sample\": [],\n",
    "    \"PixCorr\": [],\n",
    "    \"SSIM\": [],\n",
    "    \"AlexNet(2)\": [],\n",
    "    \"AlexNet(5)\": [],\n",
    "    \"InceptionV3\": [],\n",
    "    \"CLIP\": [],\n",
    "    \"EffNet-B\": [],\n",
    "    \"SwAV\": [],\n",
    "    \"FwdRetrieval\": [],\n",
    "    \"BwdRetrieval\": [],\n",
    "    \"Brain Corr. nsd_general\": [],\n",
    "    \"Brain Corr. V1\": [],\n",
    "    \"Brain Corr. V2\": [],\n",
    "    \"Brain Corr. V3\": [],\n",
    "    \"Brain Corr. V4\": [],\n",
    "    \"Brain Corr. higher_vis\": [],\n",
    "    \"index_image\": []  # Add a new column for the index of the image\n",
    "}\n",
    "\n",
    "# Iterate over each sample and compute metrics with tqdm and suppressed output\n",
    "for index_sample in tqdm(range(all_recons_mult.shape[1]), desc=\"Processing samples\"):\n",
    "    for image_index in range(12):  # Loop over the 12 images\n",
    "        with open(os.devnull, 'w') as fnull, contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
    "            all_images_ = all_images[image_index:image_index+1].float()  # Process one image at a time\n",
    "            all_recons = all_recons_mult[image_index:image_index+1, index_sample, :, :].float()\n",
    "            if clip_enabled:\n",
    "                all_clipvoxels = all_clipvoxels_mult[image_index:image_index+1, index_sample, :, :].float()\n",
    "            # if blurry_recon:\n",
    "            #     all_blurryrecons = all_blurryrecons_mult[image_index:image_index+1, index_sample, :, :].float()\n",
    "\n",
    "            # fwd_sim, bwd_sim, percent_correct_fwd, percent_correct_bwd = get_retrieval_eval(all_images_, all_clipvoxels)\n",
    "            fwd_sim, bwd_sim, percent_correct_fwd, percent_correct_bwd = None, None, None, None\n",
    "            pixcorr = get_pix_corr(all_images_, all_recons)\n",
    "            ssim = get_ssim(all_images_, all_recons)\n",
    "            alexnet2, alexnet5 = get_alexnet(all_images_, all_recons)\n",
    "            inception = get_inceptionv3(all_images_, all_recons)\n",
    "            clip_ = get_clip(all_images_, all_recons)\n",
    "            effnet = get_efficientnet(all_images_, all_recons)\n",
    "            swav = get_swav(all_images_, all_recons)\n",
    "            region_brain_correlations = get_brain_correlation(subject_masks, image_index)\n",
    "\n",
    "        # Append each result to its corresponding list, and store the image index\n",
    "        metrics_data[\"index_sample\"].append(index_sample)\n",
    "        metrics_data[\"PixCorr\"].append(pixcorr)\n",
    "        metrics_data[\"SSIM\"].append(ssim)\n",
    "        metrics_data[\"AlexNet(2)\"].append(alexnet2)\n",
    "        metrics_data[\"AlexNet(5)\"].append(alexnet5)\n",
    "        metrics_data[\"InceptionV3\"].append(inception)\n",
    "        metrics_data[\"CLIP\"].append(clip_)\n",
    "        metrics_data[\"EffNet-B\"].append(effnet)\n",
    "        metrics_data[\"SwAV\"].append(swav)\n",
    "        metrics_data[\"FwdRetrieval\"].append(percent_correct_fwd)\n",
    "        metrics_data[\"BwdRetrieval\"].append(percent_correct_bwd)\n",
    "        metrics_data[\"Brain Corr. nsd_general\"].append(region_brain_correlations[\"nsd_general\"])\n",
    "        metrics_data[\"Brain Corr. V1\"].append(region_brain_correlations[\"V1\"])\n",
    "        metrics_data[\"Brain Corr. V2\"].append(region_brain_correlations[\"V2\"])\n",
    "        metrics_data[\"Brain Corr. V3\"].append(region_brain_correlations[\"V3\"])\n",
    "        metrics_data[\"Brain Corr. V4\"].append(region_brain_correlations[\"V4\"])\n",
    "        metrics_data[\"Brain Corr. higher_vis\"].append(region_brain_correlations[\"higher_vis\"])\n",
    "        metrics_data[\"index_image\"].append(image_index)  # Add image index to the data\n",
    "\n",
    "# Check that all lists have the same length before creating DataFrame\n",
    "lengths = [len(values) for values in metrics_data.values()]\n",
    "if len(set(lengths)) != 1:\n",
    "    print(\"Error: Not all metric lists have the same length\")\n",
    "    for metric, values in metrics_data.items():\n",
    "        print(f\"{metric}: {len(values)} items\")\n",
    "else:\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    # Rename the index to sample_1, sample_2, etc.\n",
    "    # df.index = [f'sample_{i+1}' for i in range(df.shape[0])]\n",
    "\n",
    "    # print(model_name_plus_suffix)\n",
    "    # print(df.to_string(index=True))\n",
    "\n",
    "    # Save the table to a CSV file\n",
    "    os.makedirs('tables/', exist_ok=True)\n",
    "    df.to_csv(f'tables/{model_name_plus_suffix}.csv', sep='\\t')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4ced68b-bf8a-4218-ad0f-f166e03c7f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sample: sample_5\n",
      "Median sample: sample_6\n"
     ]
    }
   ],
   "source": [
    "# def get_best_and_medium(df, criteria):\n",
    "#     if criteria == \"all\":\n",
    "#         # Average all metrics\n",
    "#         scores = df.mean(axis=1)\n",
    "#     else:\n",
    "#         # Average the specified criteria\n",
    "#         scores = df[criteria].mean(axis=1)\n",
    "    \n",
    "#     # Get the index of the best score (highest)\n",
    "#     best_index = scores.idxmax()\n",
    "    \n",
    "#     # Get the index of the median score\n",
    "#     median_index = scores.sort_values().index[len(scores) // 2]\n",
    "    \n",
    "#     return best_index, median_index\n",
    "\n",
    "# # Example usage:\n",
    "# # criteria = [\"AlexNet(2)\", \"SSIM\"]  # or \"all\"\n",
    "# best_index, median_index = get_best_and_medium(df, criteria)\n",
    "\n",
    "# print(f\"Best sample: {best_index}\")\n",
    "# print(f\"Median sample: {median_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff10f9c3-85a1-4cfa-87c0-6211619febeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create full grid of recon comparisons\n",
    "# from PIL import Image\n",
    "\n",
    "# imsize = 150\n",
    "\n",
    "# def save_plot(all_images, all_recons, name):\n",
    "#     if all_images.shape[-1] != imsize:\n",
    "#         all_images = transforms.Resize((imsize,imsize))(transforms.CenterCrop(all_images.shape[2])(all_images)).float()\n",
    "#     if all_recons.shape[-1] != imsize:\n",
    "#         all_recons = transforms.Resize((imsize,imsize))(transforms.CenterCrop(all_images.shape[2])(all_recons)).float()\n",
    "    \n",
    "#     num_images = all_recons.shape[0]\n",
    "#     num_rows = (2 * num_images + 11) // 12\n",
    "    \n",
    "#     # Interleave tensors\n",
    "#     merged = torch.stack([val for pair in zip(all_images, all_recons) for val in pair], dim=0)\n",
    "    \n",
    "#     # Calculate grid size\n",
    "#     grid = torch.zeros((num_rows * 12, 3, all_recons.shape[-1], all_recons.shape[-1]))\n",
    "    \n",
    "#     # Populate the grid\n",
    "#     grid[:2*num_images] = merged\n",
    "#     grid_images = [transforms.functional.to_pil_image(grid[i]) for i in range(num_rows * 12)]\n",
    "    \n",
    "#     # Create the grid image\n",
    "#     grid_image = Image.new('RGB', (all_recons.shape[-1]*12, all_recons.shape[-1] * num_rows))  # 10 images wide\n",
    "    \n",
    "#     # Paste images into the grid\n",
    "#     for i, img in enumerate(grid_images):\n",
    "#         grid_image.paste(img, (all_recons.shape[-1] * (i % 12), all_recons.shape[-1] * (i // 12)))\n",
    "    \n",
    "#     grid_image.save(f\"../figs/{model_name_plus_suffix}_{len(all_recons)}recons_{name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27c839fb-583d-4333-9f9b-b5be3369285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/fmri/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# best_idx = int(best_index[-1]) - 1\n",
    "# median_idx = int(median_index[-1]) - 1\n",
    "\n",
    "# save_plot(all_images, all_recons_mult[:,best_idx,:,:], \"best\")\n",
    "# save_plot(all_images, all_recons_mult[:,median_idx,:,:], \"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6ae46-3de9-4abb-ae2d-c8998d83d6f2",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794c2d7-ebba-4993-a09d-ffb314cb30e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create a dictionary to store variable names and their corresponding values\n",
    "# import pandas as pd\n",
    "# data = {\n",
    "#     \"Metric\": [\"PixCorr\", \"SSIM\", \"AlexNet(2)\", \"AlexNet(5)\", \"InceptionV3\", \"CLIP\", \"EffNet-B\", \"SwAV\", \"FwdRetrieval\", \"BwdRetrieval\",\n",
    "#                \"Brain Corr. nsd_general\", \"Brain Corr. V1\", \"Brain Corr. V2\", \"Brain Corr. V3\", \"Brain Corr. V4\",  \"Brain Corr. higher_vis\"],\n",
    "#     \"Value\": [pixcorr, ssim, alexnet2, alexnet5, inception, clip_, effnet, swav, percent_correct_fwd, percent_correct_bwd, \n",
    "#               region_brain_correlations[\"nsd_general\"], region_brain_correlations[\"V1\"], region_brain_correlations[\"V2\"], region_brain_correlations[\"V3\"], region_brain_correlations[\"V4\"], region_brain_correlations[\"higher_vis\"]]}\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# print(model_name_plus_suffix)\n",
    "# print(df.to_string(index=False))\n",
    "# print(df[\"Value\"].to_string(index=False))\n",
    "\n",
    "# # save table to txt file\n",
    "# os.makedirs('tables/',exist_ok=True)\n",
    "# df[\"Value\"].to_csv(f'tables/{model_name_plus_suffix}.csv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
