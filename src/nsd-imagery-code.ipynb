{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"D1-eITfYy7BG"},"outputs":[],"source":["import os\n","import os.path as op\n","import nibabel as nib\n","import numpy as np\n","import pandas as pd\n","from PIL import Image, ImageDraw, ImageFont, ImageEnhance\n","import torch\n","from torchvision import transforms\n","from tqdm import tqdm\n","import h5py\n","import pickle\n","os.chdir('../')"]},{"cell_type":"markdown","metadata":{"id":"kgRsjTjVy7BJ"},"source":["## Data preparation code, normalizes and formats into an easily loadable format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xU5bHtiTy7BK"},"outputs":[],"source":["def zscore(x, mean=None, stddev=None, return_stats=False):\n","    if mean is not None:\n","        m = mean\n","    else:\n","        m = torch.mean(x, axis=0, keepdims=True)\n","    if stddev is not None:\n","        s = stddev\n","    else:\n","        s = torch.std(x, axis=0, keepdims=True)\n","    if return_stats:\n","        return (x - m)/(s+1e-6), m, s\n","    else:\n","        return (x - m)/(s+1e-6)\n","\n","def create_whole_region_imagery_unnormalized(subject = 1):\n","\n","    nsd_general = nib.load(f\"data/nsddata/ppdata/subj0{subject}/func1pt8mm/roi/nsdgeneral.nii.gz\").get_fdata()\n","    nsd_general = np.nan_to_num(nsd_general)\n","    nsd_general = np.where(nsd_general==1.0, True, False)\n","\n","    layer_size = np.sum(nsd_general == True)\n","    os.makedirs(\"data/preprocessed_data/subject{}/\".format(subject), exist_ok=True)\n","\n","    whole_region = np.zeros((720, layer_size))\n","\n","    nsd_general_mask = np.nan_to_num(nsd_general)\n","    nsd_mask = np.array(nsd_general_mask.flatten(), dtype=bool)\n","    beta_file = f\"data/nsddata_betas/ppdata/subj0{subject}/func1pt8mm/nsdimagerybetas_fithrf_GLMdenoise_RR/betas_nsdimagery.nii.gz\"\n","\n","    imagery_betas = nib.load(beta_file).get_fdata()\n","    imagery_betas = imagery_betas.transpose((3,0,1,2))\n","    whole_region = torch.from_numpy(imagery_betas.reshape((len(imagery_betas), -1))[:,nsd_general.flatten()].astype(np.float32))\n","\n","    torch.save(whole_region, \"data/preprocessed_data/subject{}/nsd_imagery_unnormalized.pt\".format(subject))\n","    return whole_region\n","\n","def create_whole_region_imagery_normalized(subject = 1):\n","    img_stim_file = \"data/nsddata_stimuli/stimuli/nsdimagery_stimuli.pkl3\"\n","    ex_file = open(img_stim_file, 'rb')\n","    imagery_dict = pickle.load(ex_file)\n","    ex_file.close()\n","    exps = imagery_dict['exps']\n","    cues = imagery_dict['cues']\n","    meta_cond_idx = {\n","        'visA': np.arange(len(exps))[exps=='visA'],\n","        'visB': np.arange(len(exps))[exps=='visB'],\n","        'visC': np.arange(len(exps))[exps=='visC'],\n","        'imgA_1': np.arange(len(exps))[exps=='imgA_1'],\n","        'imgA_2': np.arange(len(exps))[exps=='imgA_2'],\n","        'imgB_1': np.arange(len(exps))[exps=='imgB_1'],\n","        'imgB_2': np.arange(len(exps))[exps=='imgB_2'],\n","        'imgC_1': np.arange(len(exps))[exps=='imgC_1'],\n","        'imgC_2': np.arange(len(exps))[exps=='imgC_2']\n","    }\n","    unnormalized_file = f\"data/preprocessed_data/subject{subject}/nsd_imagery_unnormalized\"\n","    whole_region = torch.load(unnormalized_file + \".pt\")\n","    whole_region = whole_region / 300.\n","    whole_region_norm = torch.zeros_like(whole_region)\n","\n","    # Normalize the data using Z scoring method for each voxel\n","    for c,idx in meta_cond_idx.items():\n","        whole_region_norm[idx] = zscore(whole_region[idx])\n","\n","    # Save the tensor of normalized data\n","    torch.save(whole_region_norm, f\"data/preprocessed_data/subject{subject}/nsd_imagery.pt\")"]},{"cell_type":"markdown","metadata":{"id":"d7hWaE-9y7BK"},"source":["## Data loading code, prepares and averages/nests the data for a specific subject and stimulus type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXVHgP60y7BL"},"outputs":[],"source":["def condition_average(x, y, cond, nest=False):\n","    idx, idx_count = np.unique(cond, return_counts=True)\n","    idx_list = [np.array(cond)==i for i in np.sort(idx)]\n","    if nest:\n","        avg_x = torch.zeros((len(idx), idx_count.max(), x.shape[1]), dtype=torch.float32)\n","    else:\n","        avg_x = torch.zeros((len(idx), 1, x.shape[1]), dtype=torch.float32)\n","    for i, m in enumerate(idx_list):\n","        if nest:\n","            avg_x[i] = x[m]\n","        else:\n","            avg_x[i] = torch.mean(x[m], axis=0)\n","\n","    return avg_x, y, len(idx_count)\n","\n","#subject: nsd subject index between 1-8\n","#mode: vision, imagery\n","#stimtype: all, simple, complex, concepts\n","#average: whether to average across trials, will produce x that is (stimuli, 1, voxels)\n","#nest: whether to nest the data according to stimuli, will produce x that is (stimuli, trials, voxels)\n","def load_nsd_mental_imagery(subject, mode, stimtype=\"all\", average=False, nest=False):\n","    # This file has a bunch of information about the stimuli and cue associations that will make loading it easier\n","    img_stim_file = \"data/nsddata_stimuli/stimuli/nsdimagery_stimuli.pkl3\"\n","    ex_file = open(img_stim_file, 'rb')\n","    imagery_dict = pickle.load(ex_file)\n","    ex_file.close()\n","    # Indicates what experiments trials belong to\n","    exps = imagery_dict['exps']\n","    # Indicates the cues for different stimuli\n","    cues = imagery_dict['cues']\n","    # Maps the cues to the stimulus image information\n","    image_map  = imagery_dict['image_map']\n","    # Organize the indices of the trials according to the modality and the type of stimuli\n","    cond_idx = {\n","    'visionsimple': np.arange(len(exps))[exps=='visA'],\n","    'visioncomplex': np.arange(len(exps))[exps=='visB'],\n","    'visionconcepts': np.arange(len(exps))[exps=='visC'],\n","    'visionall': np.arange(len(exps))[np.logical_or(np.logical_or(exps=='visA', exps=='visB'), exps=='visC')],\n","    'imagerysimple': np.arange(len(exps))[np.logical_or(exps=='imgA_1', exps=='imgA_2')],\n","    'imagerycomplex': np.arange(len(exps))[np.logical_or(exps=='imgB_1', exps=='imgB_2')],\n","    'imageryconcepts': np.arange(len(exps))[np.logical_or(exps=='imgC_1', exps=='imgC_2')],\n","    'imageryall': np.arange(len(exps))[np.logical_or(\n","                                        np.logical_or(\n","                                            np.logical_or(exps=='imgA_1', exps=='imgA_2'),\n","                                            np.logical_or(exps=='imgB_1', exps=='imgB_2')),\n","                                        np.logical_or(exps=='imgC_1', exps=='imgC_2'))]}\n","    # Load normalized betas\n","    x = torch.load(\"data/preprocessed_data/subject{}/nsd_imagery.pt\".format(subject)).requires_grad_(False).to(\"cpu\")\n","    # Find the trial indices conditioned on the type of trials we want to load\n","    cond_im_idx = {n: [image_map[c] for c in cues[idx]] for n,idx in cond_idx.items()}\n","    conditionals = cond_im_idx[mode+stimtype]\n","    # Stimuli file is of shape (18,3,425,425), these can be converted back into PIL images using transforms.ToPILImage()\n","    y = torch.load(\"data/nsddata_stimuli/stimuli/imagery_stimuli_18.pt\").requires_grad_(False).to(\"cpu\")\n","    # Prune the beta file down to specific experimental mode/stimuli type\n","    x = x[cond_idx[mode+stimtype]]\n","    # If stimtype is not all, then prune the image data down to the specific stimuli type\n","    if stimtype == \"simple\":\n","        y = y[:6]\n","    elif stimtype == \"complex\":\n","        y = y[6:12]\n","    elif stimtype == \"concepts\":\n","        y = y[12:]\n","\n","    # Average or nest the betas across trials\n","    if average or nest:\n","        x, y, sample_count = condition_average(x, y, conditionals, nest=nest)\n","    else:\n","        x = x.reshape((x.shape[0], 1, x.shape[1]))\n","\n","    print(x.shape)\n","    return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAOTAXW2y7BL","outputId":"610562bd-d1f9-47b4-954e-bef6cf85fae4"},"outputs":[{"name":"stderr","output_type":"stream","text":[" 12%|█▎        | 1/8 [00:09<01:05,  9.42s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([18, 1, 15724])\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 2/8 [00:18<00:55,  9.24s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([18, 1, 14278])\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 3/8 [00:27<00:46,  9.23s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([18, 1, 15226])\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 4/8 [00:36<00:36,  9.09s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([18, 1, 13153])\n"]},{"name":"stderr","output_type":"stream","text":[" 62%|██████▎   | 5/8 [00:44<00:26,  8.68s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([18, 1, 13039])\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 6/8 [00:55<00:18,  9.48s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([18, 1, 17907])\n"]},{"name":"stderr","output_type":"stream","text":[" 88%|████████▊ | 7/8 [01:03<00:08,  8.99s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([18, 1, 12682])\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 8/8 [01:11<00:00,  8.99s/it]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([18, 1, 14386])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["for subject in tqdm(range(1,9)):\n","    create_whole_region_imagery_unnormalized(subject)\n","    create_whole_region_imagery_normalized(subject)\n","    x, y = load_nsd_mental_imagery(subject=subject, mode=\"imagery\", stimtype=\"all\", average=True, nest=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"SS","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
