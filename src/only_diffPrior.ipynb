{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a6b885-4e8c-4a41-9560-114e7ef8a654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean\n",
    "import io\n",
    "\n",
    "import webdataset as wds\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "# # from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "# from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "# from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71aa807-e7c5-4be2-8100-a186b0b1b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 973470\n",
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "# print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "global_batch_size = batch_size = 8\n",
    "\n",
    "\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "# world_size = accelerator.state.num_processes\n",
    "# distributed = not accelerator.state.distributed_type == 'NO'\n",
    "\n",
    "# set data_type to match your mixed precision\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print = accelerator.print # only print if local_rank=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b297c3-dd4e-46cb-9eb9-6b3c8c02fd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: unCLIPDiff\n",
      "['--data_path=/weka/proj-fmri/shared/mindeyev2_dataset', '--model_name=unCLIPDiff', '--use_prior', '--prior_scale=30', '--clip_scale=1', '--subj=1', '--batch_size=8', '--no-blurry_recon', '--hidden_dim=1024', '--num_sessions=3', '--seq_len=1', '--max_lr=3e-4', '--mixup_pct=.66', '--num_epochs=12', '--no-use_image_aug', '--ckpt_interval=5', '--no-ckpt_saving']\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unCLIPDiff\"\n",
    "print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "jupyter_args = f\"--data_path=/weka/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} --use_prior --prior_scale=30 --clip_scale=1 \\\n",
    "                    --subj=1 --batch_size={batch_size} --no-blurry_recon --hidden_dim=1024 --num_sessions=3 --seq_len=1 \\\n",
    "                    --max_lr=3e-4 --mixup_pct=.66 --num_epochs=12 --no-use_image_aug --ckpt_interval=5 --no-ckpt_saving\"\n",
    "    \n",
    "jupyter_args = jupyter_args.split()\n",
    "print(jupyter_args)\n",
    "    \n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,5,7],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (zero = all sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=100.,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=1,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "\n",
    "args = parser.parse_args(jupyter_args)\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomHorizontalFlip(p=0.3),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2dd0691-cc93-4868-8390-5e9f61a53372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sessions = 3 batch_size = 8 num_iterations_per_epoch = 1600 num_samples_per_epoch = 12800\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..2}.tar\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "\n",
    "nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "\n",
    "if num_sessions == 0: num_sessions = nsessions_allsubj[subj-1]\n",
    "\n",
    "num_samples_per_epoch = 12800 // num_devices \n",
    "# or if you want less samples per epoch if you're using less data, can do (750*num_sessions) // num_devices \n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // batch_size\n",
    "\n",
    "print(\"num_sessions =\", num_sessions, \"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch,)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "train_url = f\"{data_path}/wds/subj0{subj}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "print(train_url)\n",
    "\n",
    "train_data = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\\\n",
    "                    .with_epoch(num_samples_per_epoch)\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "# if using same test dataset as MindEyeV1:\n",
    "test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "num_test = 2770 # total number of test samples\n",
    "test_batch_size = 30 # number of test samples to use during validation\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c39f1e7-0f94-47d5-821f-1b523ad52cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj01 betas loaded into memory\n",
      "voxels torch.Size([30000, 15724])\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(f'{data_path}/betas_all_subj0{subj}_fp32.hdf5', 'r')\n",
    "\n",
    "voxels = f['betas'][:]\n",
    "print(f\"subj0{subj} betas loaded into memory\")\n",
    "voxels = torch.Tensor(voxels).to(\"cpu\").to(data_type)\n",
    "print(\"voxels\", voxels.shape)\n",
    "num_voxels = voxels.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f6ef3b-f115-46ae-af91-6c1a1dad3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class imgViTBG(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.directory))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return the tensor at the given index.\n",
    "        Args:\n",
    "        - idx (int): Index of the tensor to be loaded.\n",
    "        \"\"\"\n",
    "        filename = f\"imgt{idx+1}_embed.pt\"\n",
    "        file_path = os.path.join(self.directory, filename)\n",
    "        # tensor = torch.load(file_path)\n",
    "        return file_path\n",
    "\n",
    "# Usage\n",
    "directory = '/weka/proj-fmri/shared/vitBG_embeds/'  # Replace with your directory path\n",
    "imgemb_dataset = imgViTBG(directory)\n",
    "\n",
    "def get_img_tensor(data, index_arr, batch_size):\n",
    "    emb_arr = []\n",
    "    for i in range(batch_size):\n",
    "        ind = index_arr[i]\n",
    "        path_emb = data[ind]\n",
    "        emb = torch.load(path_emb, map_location='cpu')\n",
    "        emb = emb.squeeze(0)\n",
    "        emb_arr.append(emb)\n",
    "    emb_tensor = torch.stack(emb_arr)\n",
    "    return emb_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8bf4584-f3d6-4199-84d3-40355461663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2f80fe-4b5e-420e-8111-b81562f2cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "456,065,944 total\n",
      "456,065,944 trainable\n",
      "param counts:\n",
      "472,168,344 total\n",
      "472,168,344 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "472168344"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_size, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linear = torch.nn.Linear(input_size, out_features)\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3: # do ridge mapping the same way for each timepoint\n",
    "            ridge_out = torch.cat([self.linear(x[:,xi])[:,None] for xi in range(x.shape[1])], dim=1)\n",
    "            return ridge_out\n",
    "        else:\n",
    "            x = self.linear(x).unsqueeze(1)\n",
    "            return x\n",
    "        \n",
    "model.ridge = RidgeRegression(voxels.shape[1], out_features=hidden_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, out_dim=768, in_dim=15724, seq_len=2, h=4096, n_blocks=4, drop=.15, clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        # Mixer Blocks\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.clin1 = nn.Linear(h * seq_len, out_dim, bias=True)\n",
    "\n",
    "        self.clip_proj = nn.Sequential(\n",
    "            nn.LayerNorm(clip_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(clip_size, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, clip_size)\n",
    "        )\n",
    "\n",
    "        if blurry_recon:\n",
    "            self.blin1 = nn.Sequential(\n",
    "                nn.Linear(h*seq_len, 4096, bias=True),\n",
    "                nn.LayerNorm(4096),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4096, 4096))\n",
    "            self.bgroupnorm = nn.GroupNorm(1, 256)\n",
    "            self.bupsampler = Decoder(\n",
    "                in_channels=256,\n",
    "                out_channels=4,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[32, 64, 128],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "        \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, b=torch.Tensor([0.])):        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.clin1(x).reshape(len(x), -1, self.clip_size)\n",
    "        c = self.clip_proj(backbone)\n",
    "\n",
    "        if blurry_recon:\n",
    "            b = self.blin1(x)\n",
    "            b = b.reshape(len(b), 256, 4, 4)\n",
    "            b = self.bgroupnorm(b)\n",
    "            b = self.bupsampler(b)\n",
    "        \n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4aea2e9-748b-4338-bf10-3be587ea9a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "144,594,800 total\n",
      "144,594,784 trainable\n",
      "param counts:\n",
      "616,763,144 total\n",
      "616,763,128 trainable\n"
     ]
    }
   ],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 3 #6\n",
    "    dim_head = 208 # 64\n",
    "    heads = clip_emb_dim//208 # heads * dim_head = clip_emb_dim\n",
    "    guidance_scale = 3.5\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "        voxel2clip=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a294fff-3bf0-44e8-bc99-9651bcd15f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 19200\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "else:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69bb0f9b-21f4-4805-984a-493506a93b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_config:\n",
      " {'model_name': 'unCLIPDiff', 'global_batch_size': 8, 'batch_size': 8, 'num_epochs': 12, 'max_lr': 0.0003, 'mixup_pct': 0.66}\n",
      "wandb_id: unCLIPDiff\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/weka/proj-fmri/mihirneal/MindEyeV2/src/wandb/run-20240102_135852-diffusion prior umap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://stability.wandb.io/mihirneal/MEV2/runs/diffusion%20prior%20umap\" target=\"_blank\">vitBG Embeds Run 2</a></strong> to <a href=\"https://stability.wandb.io/mihirneal/MEV2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True\n",
    "\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'MEV2'\n",
    "    # print(f\"vitBG alignment\")\n",
    "    wandb_run = \"vitBG Embeds Run 2\"\n",
    "    wandb_notes = ''\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb.login(host='https://stability.wandb.io')#, relogin=True)\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    if True: # wandb_auto_resume\n",
    "        print(\"wandb_id:\",model_name)\n",
    "        wandb.init(\n",
    "            id = \"diffusion prior umap\",\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "            resume=\"allow\",\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "        )\n",
    "else:\n",
    "    wandb_log = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1527db2c-03dd-434f-9f8d-b2574c3ec323",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(\n",
    "model, optimizer, train_dl, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7cdd74f-7886-4428-a93b-85657c572247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unCLIPDiff starting with epoch 0 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      "  8%|██████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 1/12 [05:55<1:05:15, 355.95s/it, test/blurry_pixcorr=0, test/loss=17.4, test/loss_blurry_total=0, test/loss_clip_total=4.84, test/loss_prior=12.5, test/num_steps=1, test/recon_cossim=0.097, test/recon_mse=1.9, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.773, train/fwd_pct_correct=0.729, train/loss=12.5, train/loss_blurry_total=0, train/loss_clip_total=0.286, train/loss_prior=12.3, train/lr=0.000238, train/num_steps=3200, train/recon_cossim=0.685, train/recon_mse=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 17%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 2/12 [09:23<44:49, 268.93s/it, test/blurry_pixcorr=0, test/loss=15, test/loss_blurry_total=0, test/loss_clip_total=4.93, test/loss_prior=10.1, test/num_steps=2, test/recon_cossim=0.0952, test/recon_mse=1.9, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0.0333, train/blurry_pixcorr=0, train/bwd_pct_correct=0.769, train/fwd_pct_correct=0.721, train/loss=12.1, train/loss_blurry_total=0, train/loss_clip_total=0.28, train/loss_prior=11.8, train/lr=0.000197, train/num_steps=4800, train/recon_cossim=0.698, train/recon_mse=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 25%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 3/12 [12:51<36:08, 240.99s/it, test/blurry_pixcorr=0, test/loss=19.8, test/loss_blurry_total=0, test/loss_clip_total=4.6, test/loss_prior=15.2, test/num_steps=3, test/recon_cossim=0.108, test/recon_mse=1.87, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.77, train/fwd_pct_correct=0.721, train/loss=11.7, train/loss_blurry_total=0, train/loss_clip_total=0.269, train/loss_prior=11.4, train/lr=0.00015, train/num_steps=6400, train/recon_cossim=0.709, train/recon_mse=0.381]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 33%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 4/12 [16:25<30:42, 230.36s/it, test/blurry_pixcorr=0, test/loss=16, test/loss_blurry_total=0, test/loss_clip_total=4.49, test/loss_prior=11.5, test/num_steps=4, test/recon_cossim=0.104, test/recon_mse=1.9, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.771, train/fwd_pct_correct=0.718, train/loss=11.4, train/loss_blurry_total=0, train/loss_clip_total=0.251, train/loss_prior=11.1, train/lr=0.000104, train/num_steps=8000, train/recon_cossim=0.718, train/recon_mse=0.371]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                                                                                                                                               | 5/12 [19:54<25:56, 222.39s/it, test/blurry_pixcorr=0, test/loss=14.5, test/loss_blurry_total=0, test/loss_clip_total=4.27, test/loss_prior=10.2, test/num_steps=5, test/recon_cossim=0.109, test/recon_mse=1.87, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.773, train/fwd_pct_correct=0.723, train/loss=11, train/loss_blurry_total=0, train/loss_clip_total=0.247, train/loss_prior=10.8, train/lr=6.21e-5, train/num_steps=9600, train/recon_cossim=0.729, train/recon_mse=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 50%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                                                        | 6/12 [23:22<21:46, 217.81s/it, test/blurry_pixcorr=0, test/loss=15, test/loss_blurry_total=0, test/loss_clip_total=4.18, test/loss_prior=10.8, test/num_steps=6, test/recon_cossim=0.115, test/recon_mse=1.87, test/test_bwd_pct_correct=0.0667, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.78, train/fwd_pct_correct=0.727, train/loss=10.8, train/loss_blurry_total=0, train/loss_clip_total=0.241, train/loss_prior=10.5, train/lr=2.88e-5, train/num_steps=11200, train/recon_cossim=0.736, train/recon_mse=0.351]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 58%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                 | 7/12 [26:52<17:56, 215.25s/it, test/blurry_pixcorr=0, test/loss=16.1, test/loss_blurry_total=0, test/loss_clip_total=4.21, test/loss_prior=11.9, test/num_steps=7, test/recon_cossim=0.115, test/recon_mse=1.85, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.776, train/fwd_pct_correct=0.72, train/loss=10.6, train/loss_blurry_total=0, train/loss_clip_total=0.243, train/loss_prior=10.4, train/lr=7.46e-6, train/num_steps=12800, train/recon_cossim=0.742, train/recon_mse=0.345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                         | 8/12 [30:19<14:10, 212.56s/it, test/blurry_pixcorr=0, test/loss=18.1, test/loss_blurry_total=0, test/loss_clip_total=4.26, test/loss_prior=13.8, test/num_steps=8, test/recon_cossim=0.114, test/recon_mse=1.86, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.995, train/fwd_pct_correct=0.996, train/loss=9.91, train/loss_blurry_total=0, train/loss_clip_total=0.00549, train/loss_prior=9.91, train/lr=1.25e-8, train/num_steps=14400, train/recon_cossim=0.754, train/recon_mse=0.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                   | 9/12 [33:50<10:35, 211.93s/it, test/blurry_pixcorr=0, test/loss=21.1, test/loss_blurry_total=0, test/loss_clip_total=4.26, test/loss_prior=16.8, test/num_steps=9, test/recon_cossim=0.116, test/recon_mse=1.85, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.994, train/fwd_pct_correct=0.995, train/loss=9.99, train/loss_blurry_total=0, train/loss_clip_total=0.00819, train/loss_prior=9.98, train/lr=1.2e-8, train/num_steps=16000, train/recon_cossim=0.752, train/recon_mse=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                             | 10/12 [37:18<07:01, 210.90s/it, test/blurry_pixcorr=0, test/loss=18, test/loss_blurry_total=0, test/loss_clip_total=4.27, test/loss_prior=13.7, test/num_steps=10, test/recon_cossim=0.118, test/recon_mse=1.85, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.994, train/fwd_pct_correct=0.995, train/loss=9.83, train/loss_blurry_total=0, train/loss_clip_total=0.017, train/loss_prior=9.81, train/lr=1.2e-8, train/num_steps=17600, train/recon_cossim=0.756, train/recon_mse=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      " 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 11/12 [40:45<03:29, 209.60s/it, test/blurry_pixcorr=0, test/loss=18.8, test/loss_blurry_total=0, test/loss_clip_total=4.27, test/loss_prior=14.5, test/num_steps=11, test/recon_cossim=0.116, test/recon_mse=1.86, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.995, train/fwd_pct_correct=0.996, train/loss=9.89, train/loss_blurry_total=0, train/loss_clip_total=0.0342, train/loss_prior=9.86, train/lr=1.2e-8, train/num_steps=19200, train/recon_cossim=0.755, train/recon_mse=0.329]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [44:16<00:00, 221.37s/it, test/blurry_pixcorr=0, test/loss=15.4, test/loss_blurry_total=0, test/loss_clip_total=4.28, test/loss_prior=11.1, test/num_steps=12, test/recon_cossim=0.116, test/recon_mse=1.86, test/test_bwd_pct_correct=0.0333, test/test_fwd_pct_correct=0, train/blurry_pixcorr=0, train/bwd_pct_correct=0.994, train/fwd_pct_correct=0.995, train/loss=9.99, train/loss_blurry_total=0, train/loss_clip_total=0.0453, train/loss_prior=9.94, train/lr=1.2e-8, train/num_steps=20800, train/recon_cossim=0.754, train/recon_mse=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Finished!===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/blurry_pixcorr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/loss</td><td>▄▂▇▃▁▂▃▅█▅▆▂</td></tr><tr><td>test/loss_blurry_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test/loss_clip_total</td><td>▇█▅▄▂▁▁▂▂▂▂▂</td></tr><tr><td>test/loss_prior</td><td>▄▁▆▃▁▂▃▅█▅▆▂</td></tr><tr><td>test/num_steps</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>test/recon_cossim</td><td>▂▁▅▄▅▇▇▇▇█▇▇</td></tr><tr><td>test/recon_mse</td><td>██▃█▃▄▂▃▂▁▂▃</td></tr><tr><td>test/test_bwd_pct_correct</td><td>▁▁▁▁▁█▁▁▁▁▁▁</td></tr><tr><td>test/test_fwd_pct_correct</td><td>▁█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/blurry_pixcorr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/bwd_pct_correct</td><td>▁▁▁▁▁▁▁█████</td></tr><tr><td>train/fwd_pct_correct</td><td>▁▁▁▁▁▁▁█████</td></tr><tr><td>train/loss</td><td>█▇▆▅▄▃▃▁▁▁▁▁</td></tr><tr><td>train/loss_blurry_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss_clip_total</td><td>███▇▇▇▇▁▁▁▂▂</td></tr><tr><td>train/loss_prior</td><td>█▇▆▅▄▃▃▁▁▁▁▁</td></tr><tr><td>train/lr</td><td>█▇▅▄▃▂▁▁▁▁▁▁</td></tr><tr><td>train/num_steps</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>train/recon_cossim</td><td>▁▂▃▄▅▆▇█████</td></tr><tr><td>train/recon_mse</td><td>█▇▆▅▄▃▃▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/blurry_pixcorr</td><td>0.0</td></tr><tr><td>test/loss</td><td>15.41347</td></tr><tr><td>test/loss_blurry_total</td><td>0.0</td></tr><tr><td>test/loss_clip_total</td><td>4.27731</td></tr><tr><td>test/loss_prior</td><td>11.13616</td></tr><tr><td>test/num_steps</td><td>12</td></tr><tr><td>test/recon_cossim</td><td>0.11564</td></tr><tr><td>test/recon_mse</td><td>1.86016</td></tr><tr><td>test/test_bwd_pct_correct</td><td>0.03333</td></tr><tr><td>test/test_fwd_pct_correct</td><td>0.0</td></tr><tr><td>train/blurry_pixcorr</td><td>0.0</td></tr><tr><td>train/bwd_pct_correct</td><td>0.99438</td></tr><tr><td>train/fwd_pct_correct</td><td>0.99469</td></tr><tr><td>train/loss</td><td>9.98602</td></tr><tr><td>train/loss_blurry_total</td><td>0.0</td></tr><tr><td>train/loss_clip_total</td><td>0.04532</td></tr><tr><td>train/loss_prior</td><td>9.9407</td></tr><tr><td>train/lr</td><td>0.0</td></tr><tr><td>train/num_steps</td><td>20800</td></tr><tr><td>train/recon_cossim</td><td>0.75362</td></tr><tr><td>train/recon_mse</td><td>0.33136</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vitBG Embeds Run 2</strong>: <a href=\"https://stability.wandb.io/mihirneal/MEV2/runs/diffusion%20prior%20umap\" target=\"_blank\">https://stability.wandb.io/mihirneal/MEV2/runs/diffusion%20prior%20umap</a><br/>Synced 7 W&B file(s), 12 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240102_135852-diffusion prior umap/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    \n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    test_loss_blurry_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "    \n",
    "    for train_i, (behav, past_behav, future_behav, old_behav) in enumerate(train_dl):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "    \n",
    "            voxel = voxels[behav[:,0,5].cpu().long()].to(device)\n",
    "            # image = images[behav[:,0,0].cpu().long()].to(device).float()\n",
    "\n",
    "            # cuda = torch.cuda.current_device()\n",
    "            # dev = torch.device(f\"cuda:{cuda}\")\n",
    "            arr73k = behav[:,0,0].cpu().long()\n",
    "            clip_target = get_img_tensor(imgemb_dataset, arr73k, batch_size)\n",
    "            clip_target = clip_target.to(device)\n",
    "\n",
    "            # assert clip_target.shape\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "    \n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                voxel, perm, betas, select = utils.mixco(voxel)\n",
    "    \n",
    "            voxel_ridge = model.ridge(voxel)\n",
    "    \n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "            backbone = backbone + .1598 # using mean of clip_target\n",
    "            \n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "            \n",
    "            if use_prior:\n",
    "                    loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    loss_prior_total += loss_prior.item()\n",
    "                    \n",
    "                    recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                    recon_mse += mse(prior_out,clip_target).item()\n",
    "    \n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006, \n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "    \n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_target_norm)).to(clip_voxels_norm.device) \n",
    "            fwd_percent_correct += utils.topk(torch.abs(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm)), labels, k=1).item()\n",
    "            bwd_percent_correct += utils.topk(torch.abs(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm)), labels, k=1).item()\n",
    "    \n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(voxel)), size=batch_size//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(blurry_image_enc_[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "    \n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                assert len(behav) == num_test\n",
    "                \n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    voxel = voxels[behav[:,0,5].cpu().long()]\n",
    "\n",
    "                    if seq_len > 1:\n",
    "                        past_behavior = past_behav[:,:(seq_len-1),5].cpu().long()\n",
    "                        past_voxel = voxels[past_behavior]\n",
    "                        if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                            past_voxel[torch.where(past_behavior==-1)[0]] = 0\n",
    "                        past_voxel = torch.Tensor(past_voxel).to(data_type)\n",
    "                \n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "                \n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if len(locs)==1:\n",
    "                            locs = locs.repeat(3)\n",
    "                        elif len(locs)==2:\n",
    "                            locs = locs.repeat(2)[:3]\n",
    "                        assert len(locs)==3\n",
    "                        if test_image is None:\n",
    "                            im = im.item()\n",
    "                            emb_tensor = get_img_tensor(imgemb_dataset, [im], 1)\n",
    "                            test_image = emb_tensor[None]\n",
    "                            # img = torch.load(imgemb_dataset[im])\n",
    "                            # test_image = img[None]\n",
    "                            test_voxel = voxel[locs][None]\n",
    "                            if seq_len > 1:\n",
    "                                test_past_voxel = past_voxel[locs][None]\n",
    "                        else:\n",
    "                            im = im.item()\n",
    "                            emb_tensor = get_img_tensor(imgemb_dataset, [im], 1)\n",
    "                            test_image = torch.vstack((test_image, emb_tensor[None]))\n",
    "                            test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "                            if seq_len > 1:\n",
    "                                test_past_voxel = torch.vstack((test_past_voxel, past_voxel[locs][None]))\n",
    "    \n",
    "                loss=0.\n",
    "                \n",
    "                # validation sample of 300\n",
    "                val_indices = torch.arange(len(test_voxel))[:test_batch_size]\n",
    "                # print(val_indices)\n",
    "                voxel = test_voxel[val_indices].to(device)\n",
    "                if seq_len > 1: past_voxel = test_past_voxel[val_indices].to(device)\n",
    "                image = test_image[val_indices].to(device)\n",
    "                assert len(image) == 30\n",
    "                \n",
    "                for rep in range(3):\n",
    "                    voxel_ridge = model.ridge(voxel[:,rep])\n",
    "                    if seq_len > 1:\n",
    "                        past_voxel_ridge = model.ridge(past_voxel[:,rep])\n",
    "                        voxel_ridge = torch.cat((voxel_ridge, past_voxel_ridge), axis=1)\n",
    "                    backbone0, clip_voxels0, blurry_image_enc_0 = model.backbone(voxel_ridge)\n",
    "                    if rep==0:\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                        if blurry_recon: blurry_image_enc_ = blurry_image_enc_0\n",
    "                    else:\n",
    "                        clip_voxels += clip_voxels0\n",
    "                        backbone += backbone0\n",
    "                        if blurry_recon: blurry_image_enc_ += blurry_image_enc_0\n",
    "                clip_voxels /= 3\n",
    "                backbone /= 3\n",
    "                backbone = backbone + .1598 # using mean of clip_target\n",
    "                if blurry_recon: \n",
    "                    blurry_image_enc_ /= 3\n",
    "                    blurry_image_enc = autoenc.encode(2*utils.resize(image,128)-1).latent_dist.mode() * 0.18215\n",
    "            \n",
    "                # clip_target_file = imgemb_dataset[val_indices]\n",
    "                # clip_target_file = clip_target_file.cpu().numpy()\n",
    "                # print(clip_target_file)\n",
    "                clip_target = get_img_tensor(imgemb_dataset, val_indices, test_batch_size)\n",
    "                clip_target = clip_target.to(device)\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, _ = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    \n",
    "                    # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                    prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                                    text_cond = dict(text_embed = backbone), \n",
    "                                    cond_scale = 1., timesteps = timesteps)\n",
    "                    \n",
    "                    test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                    test_recon_mse += mse(prior_out,clip_target).item()\n",
    "        \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    loss_blurry = l1(blurry_image_enc_, blurry_image_enc)\n",
    "                    test_loss_blurry_total += loss_blurry.item()\n",
    "                    loss_blurry *= blur_scale\n",
    "                    loss += loss_blurry\n",
    "    \n",
    "                    # halving the batch size because the decoder is computationally heavy\n",
    "                    blurry_recon_images = (autoenc.decode(blurry_image_enc_[:len(voxel)//2]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    blurry_recon_images = torch.vstack((blurry_recon_images, (autoenc.decode(blurry_image_enc_[len(voxel)//2:]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    pixcorr = utils.pixcorr(image, blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "        \n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_target_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            print(\"---\")\n",
    "            \n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"test/loss_blurry_total\": test_loss_blurry_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "                    \n",
    "            if use_prior: # output recons every ckpt\n",
    "                if True:\n",
    "                    # print(\"Embedding UMAP\")\n",
    "                    prior_flat = prior_out[0].cpu()\n",
    "                    clip_target_flat = clip_target[0].cpu()\n",
    "                    assert prior_flat.shape == torch.Size([256, 1664]) and clip_target_flat.shape == torch.Size([256, 1664])\n",
    "                    combined_embeddings = np.concatenate([prior_flat, clip_target_flat])\n",
    "                    umap_model = umap.UMAP(random_state=42)\n",
    "                    umap_projections = umap_model.fit_transform(combined_embeddings)\n",
    "                    # mean_euclidean_distance = np.mean(np.linalg.norm(backbone_proj - clip_target_proj, axis=2))\n",
    "                    euclidean_dist = euclidean(prior_flat.mean(axis=0), clip_target_flat.mean(axis=0))\n",
    "\n",
    "                    # Plotting\n",
    "                    prior_flat = umap_projections[:prior_flat.shape[0], :]\n",
    "                    clip_target_flat = umap_projections[prior_flat.shape[0]:, :]\n",
    "                    plt.scatter(prior_flat[:, 0], prior_flat[:, 1], label='Diffusion Prior', alpha=0.5)\n",
    "                    plt.scatter(clip_target_flat[:, 0], clip_target_flat[:, 1], label='VITBG Target', alpha=0.5)\n",
    "                    plt.title(f'UMAP Projection (Euclidean Distance: {euclidean_dist:.2f})')\n",
    "                    plt.legend()\n",
    "                    wandb.log({f\"UMAP Projection, iteration {epoch}\": wandb.Image(plt)})\n",
    "                    plt.close()\n",
    "                    \n",
    "                    \n",
    "                    # print(\"reconstructing...\")\n",
    "                    # samples = utils.unclip_recon(prior_out,\n",
    "                    #                              diffusion_engine)\n",
    "                    # if wandb_log:\n",
    "                    #     if epoch==0:\n",
    "                    #         logs[f\"test/orig\"] = wandb.Image(transforms.ToPILImage()(image[0]),\n",
    "                    #                                        caption=f\"epoch{epoch:03d}\")\n",
    "                    #     logs[f\"test/recons\"] = wandb.Image(transforms.ToPILImage()(samples[0]),\n",
    "                    #                                        caption=f\"epoch{epoch:03d}\")\n",
    "                    # if utils.is_interactive():\n",
    "                    #     if epoch==0:\n",
    "                    #         plt.figure(figsize=(2,2))\n",
    "                    #         plt.imshow(transforms.ToPILImage()(image[0]))\n",
    "                    #         plt.axis('off')\n",
    "                    #         plt.show()\n",
    "                            \n",
    "                    #     plt.figure(figsize=(2,2))\n",
    "                    #     plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                    #     plt.axis('off')\n",
    "                    #     plt.show()\n",
    "            \n",
    "            progress_bar.set_postfix(**logs)\n",
    "    \n",
    "            # Save model checkpoint and reconstruct\n",
    "            # if epoch % ckpt_interval == 0 and ckpt_saving:\n",
    "            #     if not utils.is_interactive():\n",
    "            #         save_ckpt(f'last')\n",
    "                    \n",
    "            if wandb_log: wandb.log(logs)\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5637f1-48b9-4f86-9749-2d8354466eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/blurry_pixcorr</td><td>0</td></tr><tr><td>test/loss</td><td>82.75887</td></tr><tr><td>test/loss_blurry_total</td><td>0</td></tr><tr><td>test/loss_clip_total</td><td>5.86209</td></tr><tr><td>test/loss_prior</td><td>76.89678</td></tr><tr><td>test/num_steps</td><td>12</td></tr><tr><td>test/recon_cossim</td><td>0.12527</td></tr><tr><td>test/recon_mse</td><td>13.33166</td></tr><tr><td>test/test_bwd_pct_correct</td><td>0.06667</td></tr><tr><td>test/test_fwd_pct_correct</td><td>0.03333</td></tr><tr><td>train/blurry_pixcorr</td><td>0</td></tr><tr><td>train/bwd_pct_correct</td><td>0.98461</td></tr><tr><td>train/fwd_pct_correct</td><td>0.98844</td></tr><tr><td>train/loss</td><td>59.63482</td></tr><tr><td>train/loss_blurry_total</td><td>0</td></tr><tr><td>train/loss_clip_total</td><td>0.02066</td></tr><tr><td>train/loss_prior</td><td>59.61416</td></tr><tr><td>train/lr</td><td>0.0</td></tr><tr><td>train/num_steps</td><td>19200</td></tr><tr><td>train/recon_cossim</td><td>0.80268</td></tr><tr><td>train/recon_mse</td><td>1.98714</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vitBG Embeds Run 1</strong>: <a href=\"https://stability.wandb.io/mihirneal/MEV2/runs/diffusion%20prior\" target=\"_blank\">https://stability.wandb.io/mihirneal/MEV2/runs/diffusion%20prior</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240102_130604-diffusion prior/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2fe8ec6-2e5a-4ec2-aa25-84eb0c581552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_target_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5068e09-6191-4d3b-b4e2-3eeb6bde5491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 256, 1664])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "babb1c05-d4ca-4c03-9d92-fc9085f2218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 256, 1664])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196d220-2abb-43d5-b7c1-c12c68570bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
