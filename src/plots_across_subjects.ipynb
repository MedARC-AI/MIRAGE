{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5f265e-407a-40bd-92fb-a652091fd7ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "PID of this process = 1113654\n",
      "device: cuda\n",
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffb659a-8154-4536-ab27-2d976da1bf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: pretrained_subj01_40sess_hypatia_vd2\n",
      "--model_name=pretrained_subj01_40sess_hypatia_vd2 --data_path=/weka/proj-medarc/shared/mindeyev2_dataset --cache_dir=/weka/proj-medarc/shared/cache --all_recons_path=evals/pretrained_subj01_40sess_hypatia_vd2/pretrained_subj01_40sess_hypatia_vd2_all_recons_imagery.pt --mode imagery                         --imagery_data_path=/weka/proj-medarc/shared/umn-imagery --criteria=all\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"pretrained_subj01_40sess_hypatia_vd2\"\n",
    "    # model_name = \"pretest_pretrained_subj01_40sess_hypatia_pg_sessions40\"\n",
    "    mode = \"imagery\"\n",
    "    # all_recons_path = f\"evals/{model_name}/{model_name}_all_enhancedrecons_{mode}.pt\"\n",
    "    all_recons_path = f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\"\n",
    "\n",
    "    cache_dir = \"/weka/proj-medarc/shared/cache\"\n",
    "    data_path = \"/weka/proj-medarc/shared/mindeyev2_dataset\"\n",
    "    criteria = \"all\"\n",
    "    # criteria = \"SSIM\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    jupyter_args = f\"--model_name={model_name} --data_path={data_path} --cache_dir={cache_dir} --all_recons_path={all_recons_path} --mode {mode} \\\n",
    "                         --criteria={criteria}\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb8120cd-f226-4e2c-a6c5-3cd8ef6e9bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--all_recons_path\", type=str,\n",
    "    help=\"Path to where all_recons.pt is stored\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subjs\",type=str, default='1,2,5,7',\n",
    "    help=\"Evaluate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--criteria\",type=str, default=\"all\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "criteria = criteria.replace(\"*\", \" \")\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d66b33-b327-4895-a861-ecc6ccc51296",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997f9672-b74d-4dcf-b4d7-a593fdce9cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mode == \"synthetic\":\n",
    "    all_images = torch.zeros((284, 3, 714, 1360))\n",
    "    all_images[:220] = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/nsdsynthetic/nsd_synthetic_stim_part1.pt\")\n",
    "    #The last 64 stimuli are slightly different for each subject, so we load these separately for each subject\n",
    "    all_images[220:] = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/nsdsynthetic/nsd_synthetic_stim_part2_sub{subj}.pt\")\n",
    "else:\n",
    "    all_images = torch.load(f\"{data_path}/nsddata_stimuli/stimuli/imagery_stimuli_18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4f6f99-b1be-4924-b4d4-9785680219a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects: [1, 2, 5, 7]\n",
      "ms_model_names: ['pretrained_subj01_40sess_hypatia_vd2', 'pretrained_subj02_40sess_hypatia_vd2', 'pretrained_subj05_40sess_hypatia_vd2', 'pretrained_subj07_40sess_hypatia_vd2']\n",
      "ms_all_recons_paths: ['evals/pretrained_subj01_40sess_hypatia_vd2/pretrained_subj01_40sess_hypatia_vd2_all_recons_imagery.pt', 'evals/pretrained_subj02_40sess_hypatia_vd2/pretrained_subj02_40sess_hypatia_vd2_all_recons_imagery.pt', 'evals/pretrained_subj05_40sess_hypatia_vd2/pretrained_subj05_40sess_hypatia_vd2_all_recons_imagery.pt', 'evals/pretrained_subj07_40sess_hypatia_vd2/pretrained_subj07_40sess_hypatia_vd2_all_recons_imagery.pt']\n"
     ]
    }
   ],
   "source": [
    "subjects = [int(s) for s in subjs.split(\",\")]\n",
    "print(\"subjects:\", subjects)\n",
    "ms_model_names = []\n",
    "ms_all_recons_paths = []\n",
    "for subj in subjects:\n",
    "    m_name = model_name.replace(\"subj01\", f\"subj{subj:02d}\")\n",
    "    ms_model_names.append(m_name)\n",
    "    ms_all_recons_paths.append(all_recons_path.replace(\"subj01\", f\"subj{subj:02d}\"))    \n",
    "print(\"ms_model_names:\", ms_model_names)\n",
    "print(\"ms_all_recons_paths:\", ms_all_recons_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fc0d00-3851-450e-8c3c-3d5005fe075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded evals/pretrained_subj01_40sess_hypatia_vd2/pretrained_subj01_40sess_hypatia_vd2_all_recons_imagery.pt with shape torch.Size([18, 10, 3, 512, 512])\n",
      "Loaded tables/pretrained_subj01_40sess_hypatia_vd2_all_recons_imagery.csv\n",
      "Loaded evals/pretrained_subj02_40sess_hypatia_vd2/pretrained_subj02_40sess_hypatia_vd2_all_recons_imagery.pt with shape torch.Size([18, 10, 3, 512, 512])\n",
      "Loaded tables/pretrained_subj02_40sess_hypatia_vd2_all_recons_imagery.csv\n",
      "Loaded evals/pretrained_subj05_40sess_hypatia_vd2/pretrained_subj05_40sess_hypatia_vd2_all_recons_imagery.pt with shape torch.Size([18, 10, 3, 512, 512])\n",
      "Loaded tables/pretrained_subj05_40sess_hypatia_vd2_all_recons_imagery.csv\n",
      "Loaded evals/pretrained_subj07_40sess_hypatia_vd2/pretrained_subj07_40sess_hypatia_vd2_all_recons_imagery.pt with shape torch.Size([18, 10, 3, 512, 512])\n",
      "Loaded tables/pretrained_subj07_40sess_hypatia_vd2_all_recons_imagery.csv\n"
     ]
    }
   ],
   "source": [
    "ms_all_metrics = []\n",
    "ms_all_recons = []\n",
    "target_dim = 512\n",
    "for m_name, m_all_recons_path, subj in zip(ms_model_names, ms_all_recons_paths, subjects):\n",
    "    all_recons = torch.load(m_all_recons_path)\n",
    "    # Resize the images if necessary\n",
    "    if all_recons.shape[-1] != target_dim:\n",
    "        resize_transform = transforms.Resize((target_dim, target_dim))\n",
    "        all_recons_resized = torch.zeros((18, 10, 3, target_dim, target_dim))\n",
    "        for sample in range(18):\n",
    "            for frame in range(10):\n",
    "                all_recons_resized[sample, frame] = resize_transform(all_recons[sample, frame])\n",
    "        all_recons = all_recons_resized\n",
    "    ms_all_recons.append(all_recons)\n",
    "    print(f\"Loaded {m_all_recons_path} with shape {all_recons.shape}\")\n",
    "    \n",
    "    all_metric_table = pd.read_csv(f\"tables/{m_name}_all_recons_{mode}.csv\", sep=\"\\t\")\n",
    "\n",
    "    # add a column with all values the subject \n",
    "    all_metric_table[\"Subject\"] = [subj]*len(all_metric_table)\n",
    "\n",
    "    ms_all_metrics.append(all_metric_table)\n",
    "    print(f\"Loaded tables/{m_name}_all_recons_{mode}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4f2c51-8a1d-4605-9ac4-d3185b77e863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    }
   ],
   "source": [
    "# concatename all the metrics vertically\n",
    "all_metrics = pd.concat(ms_all_metrics)\n",
    "# print(all_metrics)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "columns_to_normalize = [\n",
    "    'PixCorr', 'SSIM', 'AlexNet(2)', 'AlexNet(5)', 'InceptionV3', 'CLIP', \n",
    "    'EffNet-B', 'SwAV', 'Brain Corr. nsd_general',\n",
    "    'Brain Corr. V1', 'Brain Corr. V2', 'Brain Corr. V3', 'Brain Corr. V4', \n",
    "    'Brain Corr. higher_vis'\n",
    "]\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply normalization only to the columns that need it\n",
    "all_metrics[columns_to_normalize] = scaler.fit_transform(all_metrics[columns_to_normalize])\n",
    "# print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ced68b-bf8a-4218-ad0f-f166e03c7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_and_medium(df, criteria):\n",
    "    # Columns to ignore\n",
    "    ignore_columns = []\n",
    "    # Drop the columns to ignore from the DataFrame\n",
    "    df_filtered = df.drop(columns=ignore_columns, errors='ignore')\n",
    "    if criteria == \"all\":\n",
    "        # Average all metrics except the ignored columns\n",
    "        all_metrics = [\n",
    "            'PixCorr', 'SSIM', 'AlexNet(2)', 'AlexNet(5)', 'InceptionV3', 'CLIP', \n",
    "            'EffNet-B', 'SwAV',\n",
    "            'Brain Corr. nsd_general',\n",
    "            'Brain Corr. V1', 'Brain Corr. V2', 'Brain Corr. V3', 'Brain Corr. V4', \n",
    "            'Brain Corr. higher_vis'\n",
    "        ]\n",
    "        scores = df_filtered[all_metrics].mean(axis=1)\n",
    "    elif isinstance(criteria, str):\n",
    "        # Handle the case where criteria is a single string\n",
    "        scores = df_filtered[criteria]\n",
    "    else:\n",
    "        # Handle the case where criteria is a list of columns\n",
    "        scores = df_filtered[criteria].mean(axis=1)\n",
    "    \n",
    "    # Get the index of the best score (highest)\n",
    "    best_index = scores.idxmax()\n",
    "    \n",
    "    # Get the index of the median score\n",
    "    median_index = scores.sort_values().index[len(scores) // 2]\n",
    "    \n",
    "    # Return the best and median rows\n",
    "    best_row = df.iloc[best_index]\n",
    "    median_row = df.iloc[median_index]\n",
    "    \n",
    "    return best_row, median_row\n",
    "\n",
    "# Example usage:\n",
    "# criteria = [\"AlexNet(2)\", \"SSIM\"]  # list of columns\n",
    "# criteria = \"AlexNet(2)\"  # single string\n",
    "# best_row, median_row = get_best_and_medium(df, criteria)\n",
    "\n",
    "# print(\"Best sample row:\\n\", best_row)\n",
    "# print(\"Median sample row:\\n\", median_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35405ae1-58a1-44a3-b856-58361fc2405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 0.0\n",
      "7.0 8.0\n",
      "2.0 6.0\n",
      "2.0 8.0\n",
      "1.0 3.0\n",
      "7.0 3.0\n",
      "2.0 6.0\n",
      "2.0 8.0\n",
      "2.0 4.0\n",
      "2.0 8.0\n",
      "2.0 1.0\n",
      "1.0 4.0\n"
     ]
    }
   ],
   "source": [
    "selected_best_final_images = []\n",
    "selected_median_final_images = []\n",
    "for current_image_index, i in enumerate(all_images[:12]):\n",
    "    # select all the rows with the current image index\n",
    "    current_image_metrics = all_metrics[all_metrics[\"sample\"] == current_image_index].reset_index()\n",
    "    # convert current_image_metrics to a dataframe\n",
    "    # current_image_metrics = pd.DataFrame(current_image_metrics)\n",
    "\n",
    "    selected_row_best, selected_row_median = get_best_and_medium(current_image_metrics, criteria)\n",
    "\n",
    "    # selected_row_best = current_image_metrics.loc[best_index]\n",
    "    # print(selected_row_best)\n",
    "    # selected_row_median = current_image_metrics.loc[median_index]\n",
    "\n",
    "    selected_subject_best = selected_row_best[\"Subject\"]\n",
    "    selected_subject_median = selected_row_median[\"Subject\"]\n",
    "\n",
    "    # this is in the second column of the table\n",
    "    selected_sample_best = selected_row_best[\"repetition\"]\n",
    "    selected_sample_median = selected_row_median[\"repetition\"]\n",
    "\n",
    "    selected_subject_best_index = subjects.index(selected_subject_best)\n",
    "    selected_subject_median_index = subjects.index(selected_subject_median)\n",
    "    # print(selected_subject_best_index, current_image_index, selected_sample_best)\n",
    "    # print(selected_subject_median_index, current_image_index, selected_sample_median)\n",
    "    # print(len(ms_all_recons), ms_all_recons[0].shape)\n",
    "    # print(ms_all_recons[selected_subject_best_index].shape, selected_sample_best, current_image_index)\n",
    "    # print(selected_subject_best, selected_sample_best)\n",
    "    selected_best_final_images.append(ms_all_recons[selected_subject_best_index][current_image_index][int(selected_sample_best)])\n",
    "    selected_median_final_images.append(ms_all_recons[selected_subject_median_index][current_image_index][int(selected_sample_median)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36fbb7b8-3f8a-4446-9686-145dc8a5c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recons_best = torch.stack(selected_best_final_images)\n",
    "all_recons_median = torch.stack(selected_median_final_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cefc34b-d14c-485d-b029-0a22c98d39bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 3, 425, 425])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff10f9c3-85a1-4cfa-87c0-6211619febeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "imsize = 150\n",
    "\n",
    "def resize_with_padding(image, target_size):\n",
    "    # Get the size difference between target size and current image size\n",
    "    current_size = image.shape[-2:]\n",
    "    padding = [max(0, (target_size[i] - current_size[i])) // 2 for i in range(2)]\n",
    "    \n",
    "    # Pad both sides equally (top/bottom and left/right)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),  # Resize to target size while maintaining aspect ratio\n",
    "        transforms.Pad((padding[1], padding[0], padding[1], padding[0]), fill=0)  # Pad to target size, avoiding negative padding\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def save_plot(all_images, all_recons, name):\n",
    "    # Resize images without cropping and maintain aspect ratio with padding\n",
    "    if all_images.shape[-1] != imsize:\n",
    "        all_images = resize_with_padding(all_images, (imsize, imsize)).float()\n",
    "    if all_recons.shape[-1] != imsize:\n",
    "        all_recons = resize_with_padding(all_recons, (imsize, imsize)).float()\n",
    "    \n",
    "    num_images = all_recons.shape[0]\n",
    "    num_rows = (2 * num_images + 11) // 12\n",
    "    \n",
    "    # Interleave tensors\n",
    "    merged = torch.stack([val for pair in zip(all_images, all_recons) for val in pair], dim=0)\n",
    "    \n",
    "    # Calculate grid size\n",
    "    grid = torch.zeros((num_rows * 12, 3, imsize, imsize))\n",
    "    \n",
    "    # Populate the grid\n",
    "    grid[:2*num_images] = merged\n",
    "    grid_images = [transforms.functional.to_pil_image(grid[i]) for i in range(num_rows * 12)]\n",
    "    \n",
    "    # Create the grid image\n",
    "    grid_image = Image.new('RGB', (imsize * 12, imsize * num_rows))  # 12 images wide\n",
    "    \n",
    "    # Paste images into the grid\n",
    "    for i, img in enumerate(grid_images):\n",
    "        grid_image.paste(img, (imsize * (i % 12), imsize * (i // 12)))\n",
    "    \n",
    "    grid_image.save(f\"../figs/{model_name_plus_suffix}_{len(all_recons)}recons_{name}_{mode}_{criteria}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27c839fb-583d-4333-9f9b-b5be3369285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_plus_suffix = model_name.replace(\"subj01\", f\"subj{[str(s) for s in subjects]}\")\n",
    "\n",
    "save_plot(all_images[:12], all_recons_best, \"best_multisubject\")\n",
    "save_plot(all_images[:12], all_recons_median, \"median_multisubject\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6ae46-3de9-4abb-ae2d-c8998d83d6f2",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b794c2d7-ebba-4993-a09d-ffb314cb30e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create a dictionary to store variable names and their corresponding values\n",
    "# import pandas as pd\n",
    "# data = {\n",
    "#     \"Metric\": [\"PixCorr\", \"SSIM\", \"AlexNet(2)\", \"AlexNet(5)\", \"InceptionV3\", \"CLIP\", \"EffNet-B\", \"SwAV\", \"FwdRetrieval\", \"BwdRetrieval\",\n",
    "#                \"Brain Corr. nsd_general\", \"Brain Corr. V1\", \"Brain Corr. V2\", \"Brain Corr. V3\", \"Brain Corr. V4\",  \"Brain Corr. higher_vis\"],\n",
    "#     \"Value\": [pixcorr, ssim, alexnet2, alexnet5, inception, clip_, effnet, swav, percent_correct_fwd, percent_correct_bwd, \n",
    "#               region_brain_correlations[\"nsd_general\"], region_brain_correlations[\"V1\"], region_brain_correlations[\"V2\"], region_brain_correlations[\"V3\"], region_brain_correlations[\"V4\"], region_brain_correlations[\"higher_vis\"]]}\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# print(model_name_plus_suffix)\n",
    "# print(df.to_string(index=False))\n",
    "# print(df[\"Value\"].to_string(index=False))\n",
    "\n",
    "# # save table to txt file\n",
    "# os.makedirs('tables/',exist_ok=True)\n",
    "# df[\"Value\"].to_csv(f'tables/{model_name_plus_suffix}.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df1bf004-2128-45fc-9de5-c01c1c187721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# every_to_plot_0 = None\n",
    "# for i in ms_all_recons:\n",
    "#     set_samples = i[1] # shape 10, 3, 512, 512\n",
    "#     if every_to_plot_0 is None:\n",
    "#         every_to_plot_0 = set_samples\n",
    "#     else:\n",
    "#         every_to_plot_0 = torch.cat((every_to_plot_0, set_samples), 0)\n",
    "\n",
    "# print(every_to_plot_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9067132a-d5cb-4d5a-a531-c0bc4f28e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_image_metrics['Brain Corr. higher_vis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33266038-7f11-491b-8d9c-bc9274190db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_image_metrics['Brain Corr. higher_vis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcf78ae6-1f84-47e8-9a1b-6f2fbf9291fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# iiii = 4\n",
    "\n",
    "# every_to_plot_0 = None\n",
    "# for i in ms_all_recons:\n",
    "#     set_samples = i[iiii]  # shape 10, 3, 512, 512\n",
    "#     if every_to_plot_0 is None:\n",
    "#         every_to_plot_0 = set_samples\n",
    "#     else:\n",
    "#         every_to_plot_0 = torch.cat((every_to_plot_0, set_samples), 0)\n",
    "\n",
    "# print(every_to_plot_0.shape)\n",
    "\n",
    "# current_image_metrics = all_metrics[all_metrics[\"index_image\"] == iiii].reset_index()\n",
    "# scores_values = current_image_metrics['PixCorr'].values  # Get the score values\n",
    "\n",
    "# # Plot all images in a grid with dynamic columns and annotate with score values\n",
    "# def plot_images(images, scores, num_rows=10):\n",
    "#     num_images = len(images)\n",
    "#     num_cols = (num_images + num_rows - 1) // num_rows  # Calculate the number of columns\n",
    "\n",
    "#     fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "#     axs = axs.flatten()  # Flatten the axes array for easier indexing\n",
    "\n",
    "#     for idx, image in enumerate(images):\n",
    "#         axs[idx].imshow(image.permute(1, 2, 0))  # Plot image\n",
    "#         axs[idx].axis('off')  # Remove axis\n",
    "#         axs[idx].set_title(f\"Score: {scores[idx]:.2f}\", fontsize=8)  # Add score as the title\n",
    "\n",
    "#     # Hide any unused subplots\n",
    "#     for i in range(len(images), len(axs)):\n",
    "#         axs[i].axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage\n",
    "# plot_images(every_to_plot_0.cpu().float(), scores_values, num_rows=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea33429f-f020-47c1-b236-8b6bbe69b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_pixcorr = transforms.Compose([\n",
    "#     transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "# ])\n",
    "\n",
    "# def get_pix_corr(all_images, all_recons):\n",
    "\n",
    "    \n",
    "#     # Flatten images while keeping the batch dimension\n",
    "#     all_images_flattened = preprocess_pixcorr(all_images).reshape(len(all_images), -1).cpu()\n",
    "#     all_recons_flattened = preprocess_pixcorr(all_recons).view(len(all_recons), -1).cpu()\n",
    "    \n",
    "#     print(all_images_flattened.shape)\n",
    "#     print(all_recons_flattened.shape)\n",
    "    \n",
    "#     corrsum = 0\n",
    "#     for i in tqdm(range(len(all_images))):\n",
    "#         corrsum += np.corrcoef(all_images_flattened[i], all_recons_flattened[i])[0][1]\n",
    "#     corrmean = corrsum / len(all_images)\n",
    "    \n",
    "#     pixcorr = corrmean\n",
    "#     print(pixcorr)\n",
    "#     return pixcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1867a5-5ca0-4b33-b540-23a2e110aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pix_corr(all_images[4],every_to_plot_0[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
