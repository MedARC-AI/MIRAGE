{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.linear_model import Ridge\n",
    "from versatile_diffusion import Reconstructor\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: pretrained_subj01_40sess_hypatia_no_blurry2\n",
      "--data_path=../dataset                     --cache_dir=../cache                     --model_name=pretrained_subj01_40sess_hypatia_no_blurry2 --subj=1                     --hidden_dim=1024 --n_blocks=4 --new_test --mode imagery --no-blurry_recon\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # model_name = \"final_subj01_pretrained_40sess_24bs\"\n",
    "    model_name = \"pretrained_subj01_40sess_hypatia_no_blurry2\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=../dataset \\\n",
    "                    --cache_dir=../cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --hidden_dim=1024 --n_blocks=4 --mode imagery --no-blurry_recon\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to load diffusion prior (True) or just rely on ridge backbone for CLIP features (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gen_rep\",type=int,default=10,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr\",type=float,default=-1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--normalize_preds\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_raw\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--deprecated\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "\n",
    "if seed > 0 and gen_rep == 1:\n",
    "    # seed all random functions, but only if doing 1 rep\n",
    "    utils.seed_everything(seed)\n",
    "    \n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459b128",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 16, 15724]) torch.Size([18, 3, 425, 425])\n"
     ]
    }
   ],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, all_images = utils.load_nsd_synthetic(subject=subj, average=False, nest=True)\n",
    "elif subj > 8:\n",
    "    _, _, voxels, all_images = utils.load_imageryrf(subject=subj-8, mode=mode, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "else:\n",
    "    voxels, all_images = utils.load_nsd_mental_imagery(subject=subj, mode=mode, stimtype=\"all\", snr=snr, average=True, nest=False)\n",
    "num_voxels = voxels.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592da0ed",
   "metadata": {},
   "source": [
    "# Load Versatile Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_emb_dim = 768\n",
    "clip_seq_dim = 257\n",
    "clip_text_seq_dim=77\n",
    "reconstructor = Reconstructor(device=device, cache_dir=cache_dir, deprecated=deprecated)\n",
    "clip_extractor = reconstructor\n",
    "clip_variant = \"ViT-L-14\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa6402",
   "metadata": {},
   "source": [
    "# Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    \n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    class MindEyeModule(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MindEyeModule, self).__init__()\n",
    "        def forward(self, x):\n",
    "            return x\n",
    "            \n",
    "    dp_model = MindEyeModule()\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 64\n",
    "    heads = clip_emb_dim//64 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "    dp_model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    if dual_guidance:\n",
    "        prior_network_txt = PriorNetwork(\n",
    "                dim=out_dim,\n",
    "                depth=depth,\n",
    "                dim_head=dim_head,\n",
    "                heads=heads,\n",
    "                causal=False,\n",
    "                num_tokens = clip_text_seq_dim,\n",
    "                learned_query_mode=\"pos_emb\"\n",
    "            )\n",
    "    \n",
    "\n",
    "        dp_model.diffusion_prior_txt = BrainDiffusionPrior(\n",
    "            net=prior_network_txt,\n",
    "            image_embed_dim=out_dim,\n",
    "            condition_on_text_encodings=False,\n",
    "            timesteps=timesteps,\n",
    "            cond_drop_prob=0.2,\n",
    "            image_embed_scale=None,\n",
    "        )\n",
    "        dp_model.diffusion_prior_txt.requires_grad_(False)\n",
    "        utils.count_params(dp_model.diffusion_prior_txt)\n",
    "    dp_model.diffusion_prior.requires_grad_(False)\n",
    "    utils.count_params(dp_model.diffusion_prior)\n",
    "    num_params = utils.count_params(dp_model)\n",
    "    dp_model.to(device).requires_grad_(False)\n",
    "\n",
    "    # Load pretrained model ckpt\n",
    "    tag='last'\n",
    "    outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    try:\n",
    "        checkpoint = torch.load(f'{outdir}/{tag}.pth', map_location='cpu')\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        # dp_model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        dp_model.load_state_dict(state_dict, strict=True)\n",
    "        del checkpoint\n",
    "    except: # probably ckpt is saved using deepspeed format\n",
    "        import deepspeed\n",
    "        print(\"load ckpt failed, loading deepspeed ckpt...\")\n",
    "        state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "        dp_model.load_state_dict(state_dict, strict=False)\n",
    "        del state_dict\n",
    "    print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491a12d",
   "metadata": {},
   "source": [
    "### Compute ground truth embeddings for training data (for feature normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc750b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize_preds:\n",
    "    file_path = f\"{data_path}/preprocessed_data/subject{subj}/{clip_variant}_image_embeddings_train.pt\"\n",
    "    clip_image_train = torch.load(file_path)\n",
    "        \n",
    "    if dual_guidance:\n",
    "        file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{clip_variant}_text_embeddings_train.pt\"\n",
    "        clip_text_train = torch.load(file_path_txt)\n",
    "            \n",
    "    if blurry_recon:\n",
    "        file_path = f\"{data_path}/preprocessed_data/subject{subj}/autoenc_image_embeddings_train.pt\"\n",
    "        vae_image_train = torch.load(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388c0d6",
   "metadata": {},
   "source": [
    "### Compute ground truth embeddings for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{data_path}/preprocessed_data/subject{subj}/{clip_variant}_image_embeddings_{mode}.pt\"\n",
    "\n",
    "images_test = transforms.Resize((224, 224))(all_images)\n",
    "text_test = np.load(f\"{data_path}/preprocessed_data/captions_18.npy\")\n",
    "if not os.path.exists(file_path):\n",
    "    # Generate CLIP Image embeddings\n",
    "    print(\"Generating CLIP Image embeddings!\")\n",
    "    clip_image_test = torch.zeros((len(images_test), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    for i in tqdm(range(0, len(images_test)), desc=\"Encoding images...\"):\n",
    "        clip_image_test[i] = clip_extractor.embed_image(images_test[i].unsqueeze(0)).detach().to(\"cpu\")\n",
    "    torch.save(clip_image_test, file_path)\n",
    "else:\n",
    "    clip_image_test = torch.load(file_path)\n",
    "    \n",
    "if dual_guidance:\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{clip_variant}_text_embeddings_{mode}.pt\"\n",
    "    if not os.path.exists(file_path_txt):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating CLIP Text embeddings!\")\n",
    "        clip_text_test = torch.zeros((len(text_test), clip_text_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "        for i in tqdm(range(0, len(text_test)), desc=\"Encoding captions...\"):\n",
    "            clip_text_test[i] = clip_extractor.embed_text(text_test[i]).detach().to(\"cpu\")\n",
    "        torch.save(clip_text_test, file_path_txt)\n",
    "    else:\n",
    "        clip_text_test = torch.load(file_path_txt)\n",
    "if blurry_recon:\n",
    "    file_path = f\"{data_path}/preprocessed_data/subject{subj}/autoenc_image_embeddings_{mode}.pt\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating VAE Image embeddings!\")\n",
    "        vae_image_test = torch.zeros((len(images_test), 3136)).to(\"cpu\")\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            for i in tqdm(range(0, len(images_test)), desc=\"Encoding images...\"):\n",
    "                vae_image_test[i] = (autoenc.encode(2*images_test[i].unsqueeze(0).detach().to(device=device, dtype=torch.float16)-1).latent_dist.mode() * 0.18215).detach().to(\"cpu\").flatten()\n",
    "            torch.save(vae_image_test, file_path)\n",
    "    else:\n",
    "        vae_image_test = torch.load(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dbfdb",
   "metadata": {},
   "source": [
    "# Predicting latent vectors for reconstruction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clip_image = torch.zeros((len(images_test), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "with open(f'{outdir}/ridge_image_weights.pkl', 'rb') as f:\n",
    "    image_datadict = pickle.load(f)\n",
    "model = Ridge(\n",
    "    alpha=60000,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "model.coef_ = image_datadict[\"coef\"]\n",
    "model.intercept_ = image_datadict[\"intercept\"]\n",
    "pred_clip_image = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_seq_dim, clip_emb_dim))\n",
    "\n",
    "if dual_guidance:\n",
    "    with open(f'{outdir}/ridge_text_weights.pkl', 'rb') as f:\n",
    "        text_datadict = pickle.load(f)\n",
    "    pred_clip_text = torch.zeros((len(text_test), clip_text_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = text_datadict[\"coef\"]\n",
    "    model.intercept_ = text_datadict[\"intercept\"]\n",
    "    pred_clip_text = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_text_seq_dim, clip_emb_dim))\n",
    "if blurry_recon:\n",
    "    pred_blurry_vae = torch.zeros((len(images_test), 3136)).to(\"cpu\")\n",
    "    with open(f'{outdir}/ridge_blurry_weights.pkl', 'rb') as f:\n",
    "        blurry_datadict = pickle.load(f)\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = blurry_datadict[\"coef\"]\n",
    "    model.intercept_ = blurry_datadict[\"intercept\"]\n",
    "    pred_blurry_vae = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, 3136))\n",
    "    \n",
    "    \n",
    "if normalize_preds:\n",
    "    for sequence in range(clip_seq_dim):\n",
    "        std_pred_clip_image = (pred_clip_image[:, sequence] - torch.mean(pred_clip_image[:, sequence],axis=0)) / torch.std(pred_clip_image[:, sequence],axis=0)\n",
    "        pred_clip_image[:, sequence] = std_pred_clip_image * torch.std(clip_image_train[:, sequence],axis=0) + torch.mean(clip_image_train[:, sequence],axis=0)\n",
    "    if dual_guidance:\n",
    "        for sequence in range(clip_text_seq_dim):\n",
    "            std_pred_clip_text = (pred_clip_text[:, sequence] - torch.mean(pred_clip_text[:, sequence],axis=0)) / torch.std(pred_clip_text[:, sequence],axis=0)\n",
    "            pred_clip_text[:, sequence] = std_pred_clip_text * torch.std(clip_text_train[:, sequence],axis=0) + torch.mean(clip_text_train[:, sequence],axis=0)\n",
    "    if blurry_recon:\n",
    "        std_pred_blurry_vae = (pred_blurry_vae - torch.mean(pred_blurry_vae,axis=0)) / torch.std(pred_blurry_vae,axis=0)\n",
    "        pred_blurry_vae = std_pred_blurry_vae * torch.std(vae_image_train,axis=0) + torch.mean(vae_image_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111b10a7ce544328855a10161c359b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/raid1/home/kneel027/miniconda3/envs/mindeye_imagery/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/export/raid1/home/kneel027/miniconda3/envs/mindeye_imagery/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "sample loop:   6%|▌         | 1/18 [00:10<02:57, 10.44s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a347b96ae749139603f83805deeebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  11%|█         | 2/18 [00:15<01:58,  7.42s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539d01444c3a435791ddac8ab5040970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  17%|█▋        | 3/18 [00:21<01:36,  6.46s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f55b06abd254e86b8a9d814b59e2118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  22%|██▏       | 4/18 [00:26<01:24,  6.00s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102aba83906d46c88bf2757eda893203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  28%|██▊       | 5/18 [00:31<01:14,  5.77s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628d0e34c8274e69b085b3a9ef29f4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  33%|███▎      | 6/18 [00:37<01:07,  5.61s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8d305b75794364ab846e8da4a13c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  39%|███▉      | 7/18 [00:42<01:00,  5.53s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f584b531aa45acac56b345efe45a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  44%|████▍     | 8/18 [00:47<00:54,  5.47s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa2a4579789458ba5c3f0c87bad8577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  50%|█████     | 9/18 [00:53<00:48,  5.44s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c7d95bd7204b06b2e3a2e1ffb2b8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  56%|█████▌    | 10/18 [00:58<00:43,  5.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fabc68e49145ab98df548383f010bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  61%|██████    | 11/18 [01:03<00:37,  5.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314dc0d019774fa99519d85ad3184e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  67%|██████▋   | 12/18 [01:09<00:32,  5.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d142ed33c6584326a8a0bef402d13f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  72%|███████▏  | 13/18 [01:14<00:26,  5.40s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae1f7af3f2a40d5bbfdc8af8dc64d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  78%|███████▊  | 14/18 [01:19<00:21,  5.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7512d298cf96442d932a4fe398cf821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  83%|████████▎ | 15/18 [01:25<00:16,  5.38s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fd3870539842e99aebcfa28f0f9ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  89%|████████▉ | 16/18 [01:30<00:10,  5.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536998413fa948f9b07f0e56eb8b6af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  94%|█████████▍| 17/18 [01:36<00:05,  5.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7559e67068f24cc28eb7b4c6e3a3068b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop: 100%|██████████| 18/18 [01:41<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 3, 768, 768])\n",
      "torch.Size([18, 3, 256, 256])\n",
      "saved pretrained_subj01_40sess_hypatia_no_blurry2 mi outputs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/export/raid1/home/kneel027/miniconda3/envs/mindeye_imagery/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "final_recons = None\n",
    "final_predcaptions = None\n",
    "final_clipvoxels = None\n",
    "final_blurryrecons = None\n",
    "raw_root = f\"/export/raid1/home/kneel027/Second-Sight/output/mental_imagery_paper_b3/{mode}/{model_name}/subject{subj}/\"\n",
    "print(\"raw_root:\", raw_root)\n",
    "recons_per_sample = 16\n",
    "data_type = torch.float16\n",
    "for rep in tqdm(range(gen_rep)):\n",
    "    seed = random.randint(0,10000000)\n",
    "    utils.seed_everything(seed = seed)\n",
    "    print(f\"seed = {seed}\")\n",
    "    # get all reconstructions    \n",
    "    # all_images = None\n",
    "    all_blurryrecons = None\n",
    "    all_recons = None\n",
    "    all_predcaptions = []\n",
    "    all_clipvoxels = None\n",
    "    \n",
    "    minibatch_size = 1\n",
    "    num_samples_per_image = 1\n",
    "    plotting = False\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "            \n",
    "            clip_voxels = pred_clip_image[idx].unsqueeze(0)\n",
    "            if dual_guidance:\n",
    "                clip_text_voxels = pred_clip_text[idx].unsqueeze(0)\n",
    "            else:\n",
    "                clip_text_voxels = None\n",
    "            print(f\"Ridge clip properties: shape {clip_voxels.shape}, {clip_text_voxels.shape}, type {clip_voxels.dtype}, {clip_text_voxels.dtype}, mean: {torch.mean(clip_voxels)}, {torch.mean(clip_text_voxels)}\")\n",
    "            # Save retrieval submodule outputs\n",
    "            if all_clipvoxels is None:\n",
    "                all_clipvoxels = clip_voxels.to('cpu')\n",
    "            else:\n",
    "                all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.to('cpu')))\n",
    "            \n",
    "            # Set defaults if diffusion prior is not enabled\n",
    "            prior_out = clip_voxels.reshape((-1, clip_seq_dim, clip_emb_dim)).to(device=device, dtype=data_type)\n",
    "            if dual_guidance:\n",
    "                prior_out_txt = clip_text_voxels.reshape((-1, clip_text_seq_dim, clip_emb_dim)).to(device=device, dtype=data_type)\n",
    "            else:\n",
    "                prior_out_txt = None\n",
    "            # Overwrite guidance variables if diffusion prior is enabled\n",
    "            if use_prior:\n",
    "                print(f\"Converted CLIP clip properties: shape {prior_out.shape}, {prior_out_txt.shape}, type {prior_out.dtype}, {prior_out_txt.dtype}, mean: {torch.mean(prior_out)}, {torch.mean(prior_out_txt)}, num_nans {torch.isnan(prior_out).sum()}, {torch.isnan(prior_out_txt).sum()}\")\n",
    "                # Feed voxels through versatile diffusion diffusion prior\n",
    "                prior_out = dp_model.diffusion_prior.p_sample_loop(prior_out.shape, \n",
    "                                text_cond = dict(text_embed = prior_out), \n",
    "                                cond_scale = 1., timesteps = 20)\n",
    "                if dual_guidance:\n",
    "                    prior_out_txt = dp_model.diffusion_prior_txt.p_sample_loop(prior_out_txt.shape, \n",
    "                                    text_cond = dict(text_embed = prior_out_txt), \n",
    "                                    cond_scale = 1., timesteps = 20)\n",
    "                print(f\"Diffusion Prior clip properties: shape {prior_out.shape}, {prior_out_txt.shape}, type {prior_out.dtype}, {prior_out_txt.dtype}, mean: {torch.mean(prior_out)}, {torch.mean(prior_out_txt)}, num_nans {torch.isnan(prior_out).sum()}, {torch.isnan(prior_out_txt).sum()}\")\n",
    "            \n",
    "            if blurry_recon:\n",
    "                blurred_image = (autoenc.decode(pred_blurry_vae[idx].reshape((1,4,28,28)).half().to(device)/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                im = torch.Tensor(blurred_image)\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im.cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im.cpu()))\n",
    "                if plotting:\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(im))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "            \n",
    "            # Feed outputs through versatile diffusion\n",
    "            samples_multi = [reconstructor.reconstruct(\n",
    "                                image=transforms.ToPILImage()(torch.Tensor(blurred_image[0])),\n",
    "                                c_i=prior_out,\n",
    "                                c_t=prior_out_txt,\n",
    "                                n_samples=1,\n",
    "                                textstrength=0.4,\n",
    "                                strength=0.85,\n",
    "                                seed=seed) for _ in range(recons_per_sample)]\n",
    "            samples = utils.pick_best_recon(samples_multi, clip_voxels, clip_extractor)\n",
    "            if isinstance(samples, PIL.Image.Image):\n",
    "                samples = transforms.ToTensor()(samples)\n",
    "            samples = samples.unsqueeze(0)\n",
    "            \n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    \n",
    "            if plotting: \n",
    "                print(model_name)\n",
    "                err # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "            if save_raw:\n",
    "                # print(f\"Saving raw images to {raw_root}/{idx}/{rep}.png\")\n",
    "                os.makedirs(f\"{raw_root}/{idx}/\", exist_ok=True)\n",
    "                transforms.ToPILImage()(samples[0]).save(f\"{raw_root}/{idx}/{rep}.png\")\n",
    "                transforms.ToPILImage()(all_images[idx]).save(f\"{raw_root}/{idx}/ground_truth.png\")\n",
    "                if rep == 0:\n",
    "                    transforms.ToPILImage()(torch.Tensor(blurred_image[0]).cpu()).save(f\"{raw_root}/{idx}/low_level.png\")\n",
    "                    torch.save(clip_voxels, f\"{raw_root}/{idx}/clip_image_voxels.pt\")\n",
    "                    if dual_guidance:\n",
    "                        torch.save(clip_text_voxels, f\"{raw_root}/{idx}/clip_text_voxels.pt\")\n",
    "        # resize outputs before saving\n",
    "        imsize = 256\n",
    "        # saving\n",
    "        # print(all_recons.shape)\n",
    "        # torch.save(all_images,\"evals/all_images.pt\")\n",
    "        if final_recons is None:\n",
    "            final_recons = all_recons.unsqueeze(1)\n",
    "            # final_predcaptions = np.expand_dims(all_predcaptions, axis=1)\n",
    "            final_clipvoxels = all_clipvoxels.unsqueeze(1)\n",
    "            if blurry_recon:\n",
    "                final_blurryrecons = all_blurryrecons.unsqueeze(1)\n",
    "        else:\n",
    "            final_recons = torch.cat((final_recons, all_recons.unsqueeze(1)), dim=1)\n",
    "            # final_predcaptions = np.concatenate((final_predcaptions, np.expand_dims(all_predcaptions, axis=1)), axis=1)\n",
    "            final_clipvoxels = torch.cat((final_clipvoxels, all_clipvoxels.unsqueeze(1)), dim=1)\n",
    "            if blurry_recon:\n",
    "                final_blurryrecons = torch.cat((all_blurryrecons.unsqueeze(1),final_blurryrecons), dim = 1)\n",
    "        \n",
    "if blurry_recon:\n",
    "    torch.save(final_blurryrecons,f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}.pt\")\n",
    "torch.save(final_recons,f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\")\n",
    "# torch.save(final_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions_{mode}.pt\")\n",
    "torch.save(final_clipvoxels,f\"evals/{model_name}/{model_name}_all_clipvoxels_{mode}.pt\")\n",
    "print(f\"saved {model_name} mi outputs!\")\n",
    "\n",
    "# if not utils.is_interactive():\n",
    "#     sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b703704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imsize = 150\n",
    "# if all_images.shape[-1] != imsize:\n",
    "#     all_images = transforms.Resize((imsize,imsize))(transforms.CenterCrop(all_images.shape[2])(all_images)).float()\n",
    "# if all_recons.shape[-1] != imsize:\n",
    "#     all_recons = transforms.Resize((imsize,imsize))(transforms.CenterCrop(all_images.shape[2])(all_recons)).float()\n",
    "# print(all_images.shape, all_recons.shape)\n",
    "# num_images = all_recons.shape[0]\n",
    "# num_rows = (2 * num_images + 11) // 12\n",
    "\n",
    "# # Interleave tensors\n",
    "# merged = torch.stack([val for pair in zip(all_images, all_recons) for val in pair], dim=0)\n",
    "\n",
    "# # Calculate grid size\n",
    "# grid = torch.zeros((num_rows * 12, 3, all_recons.shape[-1], all_recons.shape[-1]))\n",
    "\n",
    "# # Populate the grid\n",
    "# grid[:2*num_images] = merged\n",
    "# grid_images = [transforms.functional.to_pil_image(grid[i]) for i in range(num_rows * 12)]\n",
    "\n",
    "# # Create the grid image\n",
    "# grid_image = Image.new('RGB', (all_recons.shape[-1] * 12, all_recons.shape[-1] * num_rows))  # 12 images wide\n",
    "\n",
    "# # Paste images into the grid\n",
    "# for i, img in enumerate(grid_images):\n",
    "#     grid_image.paste(img, (all_recons.shape[-1] * (i % 12), all_recons.shape[-1] * (i // 12)))\n",
    "\n",
    "# # Create title row image\n",
    "# title_height = 150\n",
    "# title_image = Image.new('RGB', (grid_image.width, title_height), color=(255, 255, 255))\n",
    "# draw = ImageDraw.Draw(title_image)\n",
    "# font = ImageFont.truetype(\"arial.ttf\", 38)  # Change font size to 3 times bigger (15*3)\n",
    "# title_text = f\"Model: {model_name}, Mode: {mode}\"\n",
    "# bbox = draw.textbbox((0, 0), title_text, font=font)\n",
    "# text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "# draw.text(((grid_image.width - text_width) / 2, (title_height - text_height) / 2), title_text, fill=\"black\", font=font)\n",
    "\n",
    "# # Combine title and grid images\n",
    "# final_image = Image.new('RGB', (grid_image.width, grid_image.height + title_height))\n",
    "# final_image.paste(title_image, (0, 0))\n",
    "# final_image.paste(grid_image, (0, title_height))\n",
    "\n",
    "# final_image.save(f\"../figs/{model_name}_{len(all_recons)}recons_{mode}.png\")\n",
    "# print(f\"saved ../figs/{model_name}_{len(all_recons)}recons_{mode}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
