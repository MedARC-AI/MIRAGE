{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.4.1+cu121)\n",
      "    Python  3.11.6 (you have 3.11.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/xformers/triton/softmax.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16 if _triton_softmax_fp16_enabled else None)\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/xformers/triton/softmax.py:86: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/xformers/ops/swiglu_op.py:106: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/xformers/ops/swiglu_op.py:127: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n",
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/accelerate/accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from versatile_diffusion import Reconstructor\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: p_trained_subj01_40sess_hypatia_new_vd_dual_proj\n",
      "--data_path=/weka/proj-medarc/shared/umn-imagery                     --cache_dir=/weka/proj-medarc/shared/cache                     --model_name=p_trained_subj01_40sess_hypatia_new_vd_dual_proj --subj=1                     --hidden_dim=1024 --n_blocks=4 --mode vision --blurry_recon                     --imagery_data_path=/weka/proj-medarc/shared/umn-imagery --dual_guidance\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # model_name = \"final_subj01_pretrained_40sess_24bs\"\n",
    "    model_name = \"p_trained_subj01_40sess_hypatia_new_vd_dual_proj\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/umn-imagery \\\n",
    "                    --cache_dir=/weka/proj-medarc/shared/cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --hidden_dim=1024 --n_blocks=4 --mode vision --blurry_recon \\\n",
    "                    --imagery_data_path=/weka/proj-medarc/shared/umn-imagery --dual_guidance\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gen_rep\",type=int,default=10,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr\",type=float,default=-1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--imagery_data_path\",type=str, default=None\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "\n",
    "if seed > 0 and gen_rep == 1:\n",
    "    # seed all random functions, but only if doing 1 rep\n",
    "    utils.seed_everything(seed)\n",
    "\n",
    "if not imagery_data_path:\n",
    "    imagery_data_path = data_path\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 8, 15724]) torch.Size([18, 3, 425, 425])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/ckadirt/MindEye_Imagery/src/utils.py:680: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(f\"{data_root}/preprocessed_data/subject{subject}/nsd_imagery.pt\").requires_grad_(False).to(\"cpu\")\n",
      "/weka/proj-fmri/ckadirt/MindEye_Imagery/src/utils.py:692: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y = torch.load(f\"{data_root}/nsddata_stimuli/stimuli/imagery_stimuli_18.pt\").requires_grad_(False).to(\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, all_images = utils.load_nsd_synthetic(subject=subj, average=False, nest=True, data_root=imagery_data_path)\n",
    "elif subj > 8:\n",
    "    _, _, voxels, all_images = utils.load_imageryrf(subject=subj-8, mode=mode, stimtype=\"object\", average=False, nest=True, split=True, data_root=imagery_data_path)\n",
    "else:\n",
    "    voxels, all_images = utils.load_nsd_mental_imagery(subject=subj, mode=mode, stimtype=\"all\", average=False, nest=True, data_root=imagery_data_path)\n",
    "num_voxels = voxels.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6f419ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructor: Loading model... fp16: True\n",
      "Taking new code 2.\n",
      "\n",
      "#######################\n",
      "# Running in eps mode #\n",
      "#######################\n",
      "\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/versatile_diffusion/lib/model_zoo/common/get_model.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(cfg.pth, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pth from /weka/proj-medarc/shared/cache/versatile_diffusion_ckpts/kl-f8.pth\n",
      "Load autoencoderkl with total 83653863 parameters,72921.759 parameter sum.\n",
      "Load optimus_bert_connector with total 109489920 parameters,19107.967 parameter sum.\n",
      "Load optimus_gpt2_connector with total 132109824 parameters,19036.291 parameter sum.\n",
      "Load pth from /weka/proj-medarc/shared/cache/versatile_diffusion_ckpts/optimus-vae.pth\n",
      "Load optimus_vae_next with total 241599744 parameters,-344611.688 parameter sum.\n",
      "Load clip_image_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load clip_text_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load openai_unet_2d_next with total 859520964 parameters,99818.335 parameter sum.\n",
      "Load openai_unet_0d_next with total 1706797888 parameters,249893.201 parameter sum.\n",
      "Load vd_v2_0 with total 3746805485 parameters,206036.626 parameter sum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mei-env/lib/python3.11/site-packages/versatile_diffusion/reconstructor.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(f'{cache_dir}/vd-four-flow-v1-0-fp16.pth', map_location='cpu')\n",
      "/tmp/ipykernel_3481195/4222809111.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "280,407,420 total\n",
      "280,407,420 trainable\n",
      "param counts:\n",
      "296,509,820 total\n",
      "296,509,820 trainable\n",
      "param counts:\n",
      "56,055,184 total\n",
      "56,055,168 trainable\n",
      "param counts:\n",
      "55,640,464 total\n",
      "55,640,448 trainable\n",
      "param counts:\n",
      "408,205,468 total\n",
      "408,205,436 trainable\n",
      "\n",
      "---loading /weka/proj-fmri/ckadirt/MindEye_Imagery/train_logs/p_trained_subj01_40sess_hypatia_new_vd_dual_proj/last.pth ckpt---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3481195/4222809111.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(outdir+f'/{tag}.pth', map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt loaded!\n"
     ]
    }
   ],
   "source": [
    "clip_emb_dim = 768\n",
    "clip_seq_dim = 257\n",
    "clip_text_seq_dim=77\n",
    "reconstructor = Reconstructor(device=device, cache_dir=f'{cache_dir}/versatile_diffusion_ckpts')\n",
    "clip_extractor = reconstructor\n",
    "clip_variant = \"ViT-L-14\"\n",
    "\n",
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    \n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim, seq_len=seq_len)\n",
    "\n",
    "from diffusers.models.vae import Decoder\n",
    "from models import BrainNetwork\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, n_blocks=n_blocks,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, \n",
    "                          blurry_recon=blurry_recon, text_clip=dual_guidance) \n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = clip_emb_dim//64 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "if dual_guidance:\n",
    "    prior_network_txt = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = 77,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "\n",
    "    model.diffusion_prior_txt = BrainDiffusionPrior(\n",
    "        net=prior_network_txt,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "model.to(device)\n",
    "\n",
    "utils.count_params(model.diffusion_prior)\n",
    "if dual_guidance:\n",
    "    utils.count_params(model.diffusion_prior_txt)\n",
    "utils.count_params(model)\n",
    "\n",
    "# Load pretrained model ckpt\n",
    "tag='last'\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "try:\n",
    "    checkpoint = torch.load(outdir+f'/{tag}.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    layer_mapping = {\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_q.weight\": \"backbone.bupsampler.mid_block.attentions.0.query.weight\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_q.bias\": \"backbone.bupsampler.mid_block.attentions.0.query.bias\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_k.weight\": \"backbone.bupsampler.mid_block.attentions.0.key.weight\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_k.bias\": \"backbone.bupsampler.mid_block.attentions.0.key.bias\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_v.weight\": \"backbone.bupsampler.mid_block.attentions.0.value.weight\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_v.bias\": \"backbone.bupsampler.mid_block.attentions.0.value.bias\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_out.0.weight\": \"backbone.bupsampler.mid_block.attentions.0.proj_attn.weight\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_out.0.bias\": \"backbone.bupsampler.mid_block.attentions.0.proj_attn.bias\"\n",
    "    }\n",
    "    new_ckpt = {}\n",
    "    for old_key, value in state_dict.items():\n",
    "        new_key = layer_mapping.get(old_key, old_key)  # Get the new key, or use the old key if not in mapping\n",
    "        new_ckpt[new_key] = value\n",
    "    model = torch.compile(model)\n",
    "    model.load_state_dict(new_ckpt, strict=True)\n",
    "    del checkpoint\n",
    "except: # probably ckpt is saved using deepspeed format\n",
    "    import deepspeed\n",
    "    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del state_dict\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Creating versatile diffusion reconstruction pipeline...')\n",
    "# from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "# from diffusers.models import DualTransformer2DModel\n",
    "# # vd_cache_dir = \"/home/naxos2-raid25/kneel027/home/kneel027/fMRI-reconstruction-NSD/versatile_diffusion\"\n",
    "# # try:\n",
    "# #     vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(cache_dir).to(device)\n",
    "# # except:\n",
    "# print(\"Downloading Versatile Diffusion to\", cache_dir)\n",
    "# vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "#         \"shi-labs/versatile-diffusion\",\n",
    "#         torch_dtype=torch.float16,\n",
    "#         cache_dir = cache_dir).to(device)\n",
    "# vd_pipe.remove_unused_weights()\n",
    "# vd_pipe.image_unet.eval()\n",
    "# vd_pipe.vae.eval()\n",
    "# vd_pipe.image_unet.requires_grad_(False)\n",
    "# vd_pipe.vae.requires_grad_(False)\n",
    "\n",
    "# vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(cache_dir + \"/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7\", subfolder=\"scheduler\")\n",
    "# num_inference_steps = 20\n",
    "\n",
    "# # Set weighting of Dual-Guidance \n",
    "# if dual_guidance:\n",
    "#     text_image_ratio = .4 # .5 means equally weight text and image, 0 means use only image\n",
    "# else:\n",
    "#     text_image_ratio = 0.\n",
    "# for name, module in vd_pipe.image_unet.named_modules():\n",
    "#     if isinstance(module, DualTransformer2DModel):\n",
    "#         module.mix_ratio = text_image_ratio\n",
    "#         for i, type in enumerate((\"text\", \"image\")):\n",
    "#             if type == \"text\":\n",
    "#                 module.condition_lengths[i] = 77\n",
    "#                 module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "#             else:\n",
    "#                 module.condition_lengths[i] = 257\n",
    "#                 module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "\n",
    "# unet = vd_pipe.image_unet\n",
    "# vae = vd_pipe.vae\n",
    "# noise_scheduler = vd_pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7322b49-bf01-4d81-bc72-e2579565a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_reconstructors = 4\n",
    "# reconstructors = []\n",
    "# reconstructors.append(reconstructor)\n",
    "# for i in range(num_reconstructors):\n",
    "#     if i == 0:\n",
    "#         reconstructors.append(reconstructor)\n",
    "#     else:\n",
    "#         reconstructors.append(Reconstructor(device=device, cache_dir=f'{cache_dir}/versatile_diffusion_ckpts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284033a2-3158-455a-9e0c-f881a44fa28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# import time\n",
    "# def reconstruct_task(i, reconstructor, blurred_image, prior_out, prior_out_txt, seed):\n",
    "#     image_pil = transforms.ToPILImage()(torch.Tensor(blurred_image[0]))\n",
    "#     return reconstructor.reconstruct(\n",
    "#         image=image_pil,\n",
    "#         c_i=prior_out[i],\n",
    "#         c_t=prior_out_txt[i],\n",
    "#         n_samples=1,\n",
    "#         textstrength=0.4,\n",
    "#         strength=0.85,\n",
    "#         seed=seed\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_3481195/1143636356.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
      "\n",
      "sample loop:   0%|                                                                                                                                   | 0/18 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 15724])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3040c9e5d9ea40148e2982eebbc7e182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfe38b6f625423e909f6fb4aff05921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.22403907775879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sample loop:   6%|██████▊                                                                                                                    | 1/18 [00:32<09:08, 32.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 15724])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150785934f21429b8edb1ca9191d4bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d77bfdc2d54a1c9d63d474427bae82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.149746656417847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sample loop:  11%|█████████████▋                                                                                                             | 2/18 [01:04<08:35, 32.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 15724])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184758cc1b7146348bb25871ee35704d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38134e4ada934ce1aa74f28af2506a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.18331527709961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sample loop:  17%|████████████████████▌                                                                                                      | 3/18 [01:36<08:03, 32.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 15724])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a5b395ca02464392518959e1f184d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028be6e39fbe4c4eb8ef4a7507b3538d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.140665769577026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sample loop:  22%|███████████████████████████▎                                                                                               | 4/18 [02:08<07:30, 32.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 15724])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11264d308d8d4101982bc5661431314b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b106937c4744e9dac52cf494d0865a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_recons = None\n",
    "final_predcaptions = None\n",
    "final_clipvoxels = None\n",
    "final_blurryrecons = None\n",
    "\n",
    "\n",
    "\n",
    "recons_per_sample = 16\n",
    "\n",
    "\n",
    "for rep in tqdm(range(gen_rep)):\n",
    "    utils.seed_everything(seed = random.randint(0,10000000))\n",
    "    # get all reconstructions    \n",
    "    # all_images = None\n",
    "    all_blurryrecons = None\n",
    "    all_recons = None\n",
    "    all_predcaptions = []\n",
    "    all_clipvoxels = None\n",
    "    \n",
    "    minibatch_size = 1\n",
    "    num_samples_per_image = 1\n",
    "    plotting = False\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "            voxel = voxels[idx]\n",
    "            voxel = torch.mean(voxel, dim=0).to(device).unsqueeze(0).unsqueeze(0)\n",
    "            print(voxel.shape)\n",
    "            voxel_ridge = model.ridge(voxel,0) # 0th index of subj_list\n",
    "            backbone, backbone_txt, clip_voxels, blurry_image_enc = model.backbone(voxel_ridge)\n",
    "            blurry_image_enc = blurry_image_enc[0]\n",
    "                \n",
    "                    \n",
    "            # Save retrieval submodule outputs\n",
    "            if all_clipvoxels is None:\n",
    "                all_clipvoxels = clip_voxels.to('cpu')\n",
    "            else:\n",
    "                all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.to('cpu')))\n",
    "            \n",
    "            # Feed voxels through versatile diffusion diffusion prior\n",
    "            backbone = backbone.repeat(recons_per_sample, 1, 1)\n",
    "            prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                            text_cond = dict(text_embed = backbone), \n",
    "                            cond_scale = 1., timesteps = 20)\n",
    "            prior_out_txt = None\n",
    "            if dual_guidance:\n",
    "                backbone_txt = backbone_txt.repeat(recons_per_sample, 1, 1)\n",
    "                prior_out_txt = model.diffusion_prior_txt.p_sample_loop(backbone_txt.shape, \n",
    "                                text_cond = dict(text_embed = backbone_txt), \n",
    "                                cond_scale = 1., timesteps = 20)\n",
    "            # pred_caption_emb = clip_convert(prior_out)\n",
    "            # generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "            # generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            # all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "            \n",
    "            if blurry_recon:\n",
    "                blurred_image = (autoenc.decode(blurry_image_enc/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                \n",
    "                im = torch.Tensor(blurred_image)\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im.cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im.cpu()))\n",
    "                if plotting:\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(im))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "            \n",
    "            # Feed outputs through versatile diffusion\n",
    "            start_time = time.time()\n",
    "            # samples_multi = [reconstructor.reconstruct(\n",
    "            #                     image=transforms.ToPILImage()(torch.Tensor(blurred_image[0])),\n",
    "            #                     c_i=prior_out[i],\n",
    "            #                     c_t=prior_out_txt[i],\n",
    "            #                     n_samples=1,\n",
    "            #                     textstrength=0.4,\n",
    "            #                     strength=0.85,\n",
    "            #                     seed=seed) for i in range(recons_per_sample)]\n",
    "            samples_multi = reconstructor.reconstruct_batch(\n",
    "                                image=transforms.ToPILImage()(torch.Tensor(blurred_image[0])),\n",
    "                                c_i=prior_out,\n",
    "                                c_t=prior_out_txt,\n",
    "                                # n_samples=1,\n",
    "                                textstrength=0.4,\n",
    "                                strength=0.85,\n",
    "                                seed=seed)\n",
    "            # samples_multi = []\n",
    "            # with concurrent.futures.ThreadPoolExecutor(max_workers=num_reconstructors) as executor:\n",
    "            #     futures = []\n",
    "            #     for i in range(recons_per_sample):\n",
    "            #         # Assign each task to a reconstructor in a round-robin fashion\n",
    "            #         reconstructor = reconstructors[i % num_reconstructors]\n",
    "            #         futures.append(executor.submit(reconstruct_task, i, reconstructor, blurred_image, prior_out, prior_out_txt, seed))\n",
    "                \n",
    "            #     # Collect the results as they complete\n",
    "            #     for future in concurrent.futures.as_completed(futures):\n",
    "            #         samples_multi.append(future.result())\n",
    "            print(time.time()-start_time)\n",
    "            samples = utils.pick_best_recon(samples_multi, clip_voxels, clip_extractor)\n",
    "            if isinstance(samples, PIL.Image.Image):\n",
    "                samples = transforms.ToTensor()(samples)\n",
    "            samples = samples.unsqueeze(0)\n",
    "            \n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    \n",
    "            if plotting: \n",
    "                print(model_name)\n",
    "                err # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "            \n",
    "    \n",
    "        # resize outputs before saving\n",
    "        imsize = 256\n",
    "        # saving\n",
    "        # print(all_recons.shape)\n",
    "        # torch.save(all_images,\"evals/all_images.pt\")\n",
    "        if final_recons is None:\n",
    "            final_recons = all_recons.unsqueeze(1)\n",
    "            # final_predcaptions = np.expand_dims(all_predcaptions, axis=1)\n",
    "            final_clipvoxels = all_clipvoxels.unsqueeze(1)\n",
    "            if blurry_recon:\n",
    "                final_blurryrecons = all_blurryrecons.unsqueeze(1)\n",
    "        else:\n",
    "            final_recons = torch.cat((final_recons, all_recons.unsqueeze(1)), dim=1)\n",
    "            # final_predcaptions = np.concatenate((final_predcaptions, np.expand_dims(all_predcaptions, axis=1)), axis=1)\n",
    "            final_clipvoxels = torch.cat((final_clipvoxels, all_clipvoxels.unsqueeze(1)), dim=1)\n",
    "            if blurry_recon:\n",
    "                final_blurryrecons = torch.cat((all_blurryrecons.unsqueeze(1),final_blurryrecons), dim = 1)\n",
    "        \n",
    "if blurry_recon:\n",
    "    torch.save(final_blurryrecons,f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}.pt\")\n",
    "torch.save(final_recons,f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\")\n",
    "# torch.save(final_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions_{mode}.pt\")\n",
    "torch.save(final_clipvoxels,f\"evals/{model_name}/{model_name}_all_clipvoxels_{mode}.pt\")\n",
    "print(f\"saved {model_name} mi outputs!\")\n",
    "\n",
    "# if not utils.is_interactive():\n",
    "#     sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e5d74-10c3-44e8-93bf-ca21d414e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e282fd-7e26-4bfc-b237-30c5d4d1798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming final_recons is your tensor of shape [18, 1, 3, 512, 512]\n",
    "# # final_recons = ... (your tensor)\n",
    "\n",
    "# # Select the first image in the batch and the first reconstruction\n",
    "# # Shape after selection: [3, 512, 512]\n",
    "# image_tensor = final_recons[0, 0]\n",
    "\n",
    "# # If the tensor is on a GPU, move it to CPU\n",
    "# if image_tensor.is_cuda:\n",
    "#     image_tensor = image_tensor.cpu()\n",
    "\n",
    "# # Detach the tensor from the computation graph and convert to NumPy\n",
    "# image_np = image_tensor.detach().numpy()\n",
    "\n",
    "# # Transpose the tensor to have shape [512, 512, 3] for plotting\n",
    "# image_np = np.transpose(image_np, (1, 2, 0))\n",
    "\n",
    "# # Optional: Normalize the image to [0, 1] if it's not already\n",
    "# # This step depends on how your data is scaled\n",
    "# # Uncomment the following lines if normalization is needed\n",
    "# # min_val = image_np.min()\n",
    "# # max_val = image_np.max()\n",
    "# # image_np = (image_np - min_val) / (max_val - min_val)\n",
    "\n",
    "# # Ensure the image has valid pixel values\n",
    "# image_np = np.clip(image_np, 0, 1)\n",
    "\n",
    "# # Plot the image\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.imshow(image_np)\n",
    "# plt.title(\"First Image from final_recons\")\n",
    "# plt.axis('off')  # Hide axis\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd5a7bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mis_interactive():\n\u001b[1;32m      2\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac3804-be01-45d0-a283-38900766b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prior_out.shape, prior_out_txt.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mei-env",
   "language": "python",
   "name": "mei-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
