{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageEnhance\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "# sys.path.append('generative_models/')\n",
    "# import sgm\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.linear_model import Ridge\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "# from models import *\n",
    "device = \"cuda\"\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: jonathan_unclip\n",
      "--data_path=/weka/proj-medarc/shared/mindeyev2_dataset                     --cache_dir=/weka/proj-medarc/shared/cache                     --model_name=jonathan_unclip --subj=1                     --mode vision                     --no-dual_guidance                    --no-prompt_recon                     --no-blurry_recon --gen_rep=1\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # model_name = \"final_subj01_pretrained_40sess_24bs\"\n",
    "    model_name = \"jonathan_unclip\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --cache_dir=/weka/proj-medarc/shared/cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --mode vision \\\n",
    "                    --no-dual_guidance\\\n",
    "                    --no-prompt_recon \\\n",
    "                    --no-blurry_recon --gen_rep=1\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gen_rep\",type=int,default=10,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--snr\",type=float,default=-1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--normalize_preds\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_raw\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--raw_path\",type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--strength\",type=float,default=0.70,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--textstrength\",type=float,default=0.5,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--top_n_rank_order_rois\",type=int, default=-1,\n",
    "    help=\"Used for selecting the top n rois on a whole brain to narrow down voxels.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--samplewise_rank_order_rois\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Use the samplewise rank order rois versus voxelwise\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--vdvae\",action=argparse.BooleanOptionalAction, default=False,\n",
    "    help=\"Use the braindiffuser VDVAE as the low level image\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--filter_contrast\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Filter the low level output to be more intense and smoothed\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--filter_sharpness\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Filter the low level output to be more intense and smoothed\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--filter_color\",action=argparse.BooleanOptionalAction, default=False,\n",
    "    help=\"Filter the low level output to be more intense and smoothed\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prompt_recon\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Use for prompt generation\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "\n",
    "if seed > 0 and gen_rep == 1:\n",
    "    # seed all random functions, but only if doing 1 rep\n",
    "    utils.seed_everything(seed)\n",
    "    \n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459b128",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 1, 15724]) torch.Size([18, 3, 425, 425])\n"
     ]
    }
   ],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, all_images = utils.load_nsd_synthetic(subject=subj, average=False, nest=True)\n",
    "elif subj > 8:\n",
    "    _, _, voxels, all_images = utils.load_imageryrf(subject=subj-8, mode=mode, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "else:\n",
    "    voxels, all_images = utils.load_nsd_mental_imagery(subject=subj, mode=mode, stimtype=\"all\", average=True, nest=False, data_root=\"/weka/proj-medarc/shared/umn-imagery\")\n",
    "    #, top_n_rois=top_n_rank_order_rois, samplewise=samplewise_rank_order_rois\n",
    "num_voxels = voxels.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa6402",
   "metadata": {},
   "source": [
    "# Load pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592da0ed",
   "metadata": {},
   "source": [
    "### Load unCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a83acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "vector_suffix torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "\n",
    "# prep unCLIP\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                       denoiser_config=denoiser_config,\n",
    "                       first_stage_config=first_stage_config,\n",
    "                       conditioner_config=conditioner_config,\n",
    "                       sampler_config=sampler_config,\n",
    "                       scale_factor=scale_factor,\n",
    "                       disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "\n",
    "ckpt_path = f'{cache_dir}/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a78dc068-beb1-4434-bb69-5268c9325455",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding_variant = \"ViT-bigG-14\"\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491a12d",
   "metadata": {},
   "source": [
    "### Compute ground truth embeddings for training data (for feature normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc750b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is erroring, feature extraction failed in Train.ipynb\n",
    "if normalize_preds:\n",
    "    file_path = f\"{data_path}/preprocessed_data/subject{subj}/{image_embedding_variant}_image_embeddings_train.pt\"\n",
    "    clip_image_train = torch.load(file_path)\n",
    "        \n",
    "    if dual_guidance:\n",
    "        file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{text_embedding_variant}_text_embeddings_train.pt\"\n",
    "        clip_text_train = torch.load(file_path_txt)\n",
    "    if prompt_recon:\n",
    "        file_path_prompt = f\"{data_path}/preprocessed_data/subject{subj}/{prompt_embedding_variant}_prompt_embeddings_train.pt\"\n",
    "        git_text_train = torch.load(file_path_prompt)        \n",
    "    if blurry_recon:\n",
    "        file_path = f\"{data_path}/preprocessed_data/subject{subj}/{latent_embedding_variant}_latent_embeddings_train.pt\"\n",
    "        vae_image_train = torch.load(file_path)\n",
    "    else:\n",
    "        strength = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dbfdb",
   "metadata": {},
   "source": [
    "# Predicting latent vectors for reconstruction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79d24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clip_image = torch.zeros((len(all_images), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "with open(f'{outdir}/ridge_image_weights.pkl', 'rb') as f:\n",
    "    image_datadict = pickle.load(f)\n",
    "model = Ridge(\n",
    "    alpha=60000,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "model.coef_ = image_datadict[\"coef\"]\n",
    "model.intercept_ = image_datadict[\"intercept\"]\n",
    "pred_clip_image = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_seq_dim, clip_emb_dim))\n",
    "\n",
    "if dual_guidance:\n",
    "    with open(f'{outdir}/ridge_text_weights.pkl', 'rb') as f:\n",
    "        text_datadict = pickle.load(f)\n",
    "    pred_clip_text = torch.zeros((len(all_images), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = text_datadict[\"coef\"]\n",
    "    model.intercept_ = text_datadict[\"intercept\"]\n",
    "    pred_clip_text = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_text_seq_dim, clip_text_emb_dim))\n",
    "\n",
    "if prompt_recon:\n",
    "    with open(f'{outdir}/ridge_prompt_weights.pkl', 'rb') as f:\n",
    "        prompt_datadict = pickle.load(f)\n",
    "    pred_clip_text = torch.zeros((len(all_images), git_seq_dim, git_emb_dim)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = prompt_datadict[\"coef\"]\n",
    "    model.intercept_ = prompt_datadict[\"intercept\"]\n",
    "    pred_git_text = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, git_seq_dim, git_emb_dim))\n",
    "\n",
    "if blurry_recon:\n",
    "    pred_blurry_vae = torch.zeros((len(all_images), latent_emb_dim)).to(\"cpu\")\n",
    "    with open(f'{outdir}/ridge_blurry_weights.pkl', 'rb') as f:\n",
    "        blurry_datadict = pickle.load(f)\n",
    "    model = Ridge(\n",
    "        alpha=60000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = blurry_datadict[\"coef\"]\n",
    "    model.intercept_ = blurry_datadict[\"intercept\"]\n",
    "    pred_blurry_vae = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, latent_emb_dim))    \n",
    "    \n",
    "if normalize_preds:\n",
    "    std_pred_clip_image = (pred_clip_image - torch.mean(pred_clip_image,axis=0)) / (torch.std(pred_clip_image,axis=0) + 1e-6)\n",
    "    pred_clip_image = std_pred_clip_image * torch.std(clip_image_train,axis=0) + torch.mean(clip_image_train,axis=0)\n",
    "    if dual_guidance:\n",
    "        std_pred_clip_text = (pred_clip_text - torch.mean(pred_clip_text,axis=0)) / (torch.std(pred_clip_text,axis=0) + 1e-6)\n",
    "        pred_clip_text = std_pred_clip_text * torch.std(clip_text_train,axis=0) + torch.mean(clip_text_train,axis=0)\n",
    "    if blurry_recon:\n",
    "        std_pred_blurry_vae = (pred_blurry_vae - torch.mean(pred_blurry_vae,axis=0)) / (torch.std(pred_blurry_vae,axis=0) + 1e-6)\n",
    "        pred_blurry_vae = std_pred_blurry_vae * torch.std(vae_image_train,axis=0) + torch.mean(vae_image_train,axis=0)\n",
    "    if prompt_recon:\n",
    "        for sequence in range(git_seq_dim):\n",
    "            std_pred_git_text = (pred_git_text[:, sequence] - torch.mean(pred_git_text[:, sequence],axis=0)) / (torch.std(pred_git_text[:, sequence],axis=0) + 1e-6)\n",
    "            pred_git_text[:, sequence] = std_pred_git_text * torch.std(git_text_train[:, sequence],axis=0) + torch.mean(git_text_train[:, sequence],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce308583-bf71-4c1b-9996-f9ce6c7e9416",
   "metadata": {},
   "source": [
    "### Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05296a63-5a1a-4264-b500-e0599a786968",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_recon:\n",
    "    from transformers import AutoProcessor\n",
    "    from modeling_git import GitForCausalLMClipEmb\n",
    "    all_predcaptions = []\n",
    "    processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "    git_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "    git_text_model.to(device) \n",
    "    git_text_model.eval().requires_grad_(False)\n",
    "    print(pred_git_text.shape)\n",
    "\n",
    "    for pred_text in pred_git_text:\n",
    "        pred_embedding = pred_text.to(device).to(torch.float32).unsqueeze(0)\n",
    "        generated_ids = git_text_model.generate(pixel_values=pred_embedding, max_length=20)\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "    torch.save(all_predcaptions,f\"evals/{model_name}/{prompt_embedding_variant}_all_predcaptions_imagery.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [01:42<00:00,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved jonathan_unclip mi outputs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_recons = None\n",
    "final_predcaptions = None\n",
    "final_clipvoxels = None\n",
    "final_blurryrecons = None\n",
    "\n",
    "if save_raw:\n",
    "    raw_root = f\"{raw_path}/{mode}/{model_name}/subject{subj}/\"\n",
    "    print(\"raw_root:\", raw_root)\n",
    "    os.makedirs(raw_root,exist_ok=True)\n",
    "    torch.save(pred_clip_image, f\"{raw_root}/{image_embedding_variant}_image_voxels.pt\")\n",
    "    if dual_guidance:\n",
    "        torch.save(pred_clip_text, f\"{raw_root}/{text_embedding_variant}_text_voxels.pt\")\n",
    "    if blurry_recon:\n",
    "        torch.save(pred_blurry_vae, f\"{raw_root}/{latent_embedding_variant}_latent_voxels.pt\")\n",
    "\n",
    "\n",
    "for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "    clip_voxels = pred_clip_image[idx]\n",
    "    if dual_guidance:\n",
    "        clip_text_voxels = pred_clip_text[idx]\n",
    "    else:\n",
    "        clip_text_voxels = None\n",
    "    # Save retrieval submodule outputs\n",
    "    if final_clipvoxels is None:\n",
    "        final_clipvoxels = clip_voxels.unsqueeze(0).to('cpu')\n",
    "    else:\n",
    "        final_clipvoxels = torch.vstack((final_clipvoxels, clip_voxels.unsqueeze(0).to('cpu')))\n",
    "    latent_voxels=None\n",
    "    if blurry_recon:\n",
    "        latent_voxels = pred_blurry_vae[idx].unsqueeze(0)\n",
    "        blurred_image = vdvae.reconstruct(latents=latent_voxels)\n",
    "        if filter_sharpness:\n",
    "            # This helps make the output not blurry when using the VDVAE\n",
    "            blurred_image = ImageEnhance.Sharpness(blurred_image).enhance(20)\n",
    "        if filter_contrast:\n",
    "            # This boosts the structural impact of the blurred_image\n",
    "            blurred_image = ImageEnhance.Contrast(blurred_image).enhance(1.5)\n",
    "        if filter_color: \n",
    "            blurred_image = ImageEnhance.Color(blurred_image).enhance(0.5)\n",
    "        im = transforms.ToTensor()(blurred_image)\n",
    "        if final_blurryrecons is None:\n",
    "            final_blurryrecons = im.cpu()\n",
    "        else:\n",
    "            final_blurryrecons = torch.vstack((final_blurryrecons, im.cpu()))\n",
    "\n",
    "    samples = utils.unclip_recon(clip_voxels.half().unsqueeze(0),\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix,\n",
    "                             num_samples=gen_rep)\n",
    "    \n",
    "    if save_raw:\n",
    "        os.makedirs(f\"{raw_root}/{idx}/\", exist_ok=True)\n",
    "        for rep in range(gen_rep):\n",
    "            transforms.ToPILImage()(samples[rep]).save(f\"{raw_root}/{idx}/{rep}.png\")\n",
    "        transforms.ToPILImage()(all_images[idx]).save(f\"{raw_root}/{idx}/ground_truth.png\")\n",
    "        transforms.ToPILImage()(transforms.ToTensor()(blurred_image).cpu()).save(f\"{raw_root}/{idx}/low_level.png\")\n",
    "        torch.save(clip_voxels, f\"{raw_root}/{idx}/clip_image_voxels.pt\")\n",
    "        if dual_guidance:\n",
    "            torch.save(clip_text_voxels, f\"{raw_root}/{idx}/clip_text_voxels.pt\")\n",
    "\n",
    "    if final_recons is None:\n",
    "        final_recons = samples.unsqueeze(0).cpu()\n",
    "    else:\n",
    "        final_recons = torch.cat((final_recons, samples.unsqueeze(0).cpu()), dim=0)\n",
    "        \n",
    "if blurry_recon:\n",
    "    torch.save(final_blurryrecons,f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}.pt\")\n",
    "torch.save(final_recons,f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\")\n",
    "torch.save(final_clipvoxels,f\"evals/{model_name}/{model_name}_all_clipvoxels_{mode}.pt\")\n",
    "print(f\"saved {model_name} mi outputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8966a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00a4de85fdab48d8ba6148a3499ab1ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cb613ecd721443079a57b6b948e23267",
       "style": "IPY_MODEL_1e4454e5ad904ad09ed4d7f96115cffe",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1c1c523231b444649e2c3d9649eaf8ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1e4454e5ad904ad09ed4d7f96115cffe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3075246e19b84d3da901075c894a06a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_aeeadfcedd244dfcbf1fa5e6c4e046d4",
       "max": 2,
       "style": "IPY_MODEL_a290323c5a5d49bd9c4c9b53f62e61a7",
       "value": 2
      }
     },
     "316427ff65834671bdd9328d2e2767e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d1625e5a5a2b478ab955f05b3bbdfa00",
       "style": "IPY_MODEL_75acc40bd3a4417e857cdb1b36c406d2",
       "value": " 2/2 [00:15&lt;00:00,  6.52s/it]"
      }
     },
     "3b46581f48e146e099b748cf00c86f86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d0b83acb96c402193d93d8be87c2ac5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_92cac460dddf45639b08475d8053ae6d",
        "IPY_MODEL_3075246e19b84d3da901075c894a06a7",
        "IPY_MODEL_f5cd379674034833974ee404f07c4f5a"
       ],
       "layout": "IPY_MODEL_864eb80eaa18462791ada66c3d37d965"
      }
     },
     "5aa95642ec784d8da81d37ba8a4a7109": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75acc40bd3a4417e857cdb1b36c406d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "864eb80eaa18462791ada66c3d37d965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "884d4ead2102415db902fdf43d0aa2a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "92cac460dddf45639b08475d8053ae6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d42d3ab73be74acf8af6e4c7bbdd5d0f",
       "style": "IPY_MODEL_1c1c523231b444649e2c3d9649eaf8ce",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "a1649dbb57144fb4a0843efd9a6ae9ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c6b323d4996343d28e9fedee895a29f7",
       "max": 2,
       "style": "IPY_MODEL_d4b4d14789104df69ba0fb2f5941a472",
       "value": 2
      }
     },
     "a290323c5a5d49bd9c4c9b53f62e61a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a80a34d9554b4084ba018d1e8a5817e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_00a4de85fdab48d8ba6148a3499ab1ec",
        "IPY_MODEL_a1649dbb57144fb4a0843efd9a6ae9ed",
        "IPY_MODEL_316427ff65834671bdd9328d2e2767e1"
       ],
       "layout": "IPY_MODEL_5aa95642ec784d8da81d37ba8a4a7109"
      }
     },
     "aeeadfcedd244dfcbf1fa5e6c4e046d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c6b323d4996343d28e9fedee895a29f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cb613ecd721443079a57b6b948e23267": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d1625e5a5a2b478ab955f05b3bbdfa00": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d42d3ab73be74acf8af6e4c7bbdd5d0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d4b4d14789104df69ba0fb2f5941a472": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f5cd379674034833974ee404f07c4f5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3b46581f48e146e099b748cf00c86f86",
       "style": "IPY_MODEL_884d4ead2102415db902fdf43d0aa2a9",
       "value": " 2/2 [00:12&lt;00:00,  5.17s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
