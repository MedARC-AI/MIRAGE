{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageEnhance\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "# sys.path.append('generative_models/')\n",
    "# import sgm\n",
    "from sc_reconstructor import SC_Reconstructor\n",
    "from vdvae import VDVAE\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.linear_model import Ridge\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "# from models import *\n",
    "device = \"cuda\"\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # model_name = \"final_subj01_pretrained_40sess_24bs\"\n",
    "    model_name = \"subj01_40sess_hypatia_ridge_sc2\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=../dataset \\\n",
    "                    --cache_dir=../cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --mode vision \\\n",
    "                    --dual_guidance\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",choices=[\"vision\",\"imagery\",\"shared1000\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gen_rep\",type=int,default=10,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--normalize_preds\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_raw\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--raw_path\",type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--strength\",type=float,default=0.70,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--textstrength\",type=float,default=0.5,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--filter_contrast\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Filter the low level output to be more intense and smoothed\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--filter_sharpness\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Filter the low level output to be more intense and smoothed\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_images_per_sample\",type=int, default=16,\n",
    "    help=\"Number of images to generate and select between for final recon\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--retrieval\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prompt_recon\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Use for prompt generation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--caption_type\",type=str,default='medium',choices=['coco','short', 'medium', 'schmedium'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--compile_models\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Use for speeding up stable cascade\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_trial_reps\",type=int, default=16,\n",
    "    help=\"Number of trial repetitions to average test betas across\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gt\",action=argparse.BooleanOptionalAction, default=False,\n",
    "    help=\"enable ground truth clip\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gtc\",action=argparse.BooleanOptionalAction, default=False,\n",
    "    help=\"enable ground truth clip\",\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "print(f\"args: {args}\")\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "\n",
    "if seed > 0 and gen_rep == 1:\n",
    "    # seed all random functions, but only if doing 1 rep\n",
    "    utils.seed_everything(seed)\n",
    "    \n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459b128",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, all_images = utils.load_nsd_synthetic(subject=subj, average=False, nest=True)\n",
    "elif subj > 8:\n",
    "    _, _, voxels, all_images = utils.load_imageryrf(subject=subj-8, mode=mode, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "    \n",
    "elif mode == \"shared1000\":\n",
    "    x_train, valid_nsd_ids_train, x_test, test_nsd_ids = utils.load_nsd(subject=subj, data_path=data_path)\n",
    "    voxels = torch.mean(x_test, dim=1, keepdim=True)\n",
    "    print(f\"Loaded subj {subj} test betas! {voxels.shape}\")\n",
    "    f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "    images = f['images']\n",
    "\n",
    "    all_images = torch.zeros((len(test_nsd_ids), 3, 224, 224))\n",
    "    for i, idx in enumerate(test_nsd_ids):\n",
    "        all_images[i] =  torch.from_numpy(images[idx])\n",
    "    del images, f\n",
    "    print(f\"Filtered down to only the {len(test_nsd_ids)} test images for subject {subj}!\")\n",
    "else:\n",
    "    voxels, all_images = utils.load_nsd_mental_imagery(subject=subj, \n",
    "                                                       mode=mode, \n",
    "                                                       stimtype=\"all\", \n",
    "                                                       average=True, \n",
    "                                                       nest=False,\n",
    "                                                       num_reps=num_trial_reps)\n",
    "print(voxels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa6402",
   "metadata": {},
   "source": [
    "# Load pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592da0ed",
   "metadata": {},
   "source": [
    "# Load Stable Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructor = SC_Reconstructor(compile_models=compile_models, device=device)\n",
    "if blurry_recon:\n",
    "    vdvae = VDVAE(device=device, cache_dir=cache_dir)\n",
    "\n",
    "image_embedding_variant = \"stable_cascade\"\n",
    "clip_emb_dim = 768\n",
    "clip_seq_dim = 1\n",
    "\n",
    "retrieval_embedding_variant = \"stable_cascade_hidden\"\n",
    "retrieval_emb_dim = 1024\n",
    "retrieval_seq_dim = 257\n",
    "\n",
    "text_embedding_variant = \"stable_cascade\"\n",
    "clip_text_seq_dim=77\n",
    "clip_text_emb_dim=1280\n",
    "\n",
    "latent_embedding_variant = \"vdvae\"\n",
    "latent_emb_dim = 91168\n",
    "\n",
    "prompt_embedding_variant = \"git\"\n",
    "git_seq_dim = 257\n",
    "git_emb_dim = 1024\n",
    "\n",
    "if caption_type != \"coco\":\n",
    "    text_embedding_variant += f\"_{caption_type}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491a12d",
   "metadata": {},
   "source": [
    "### Compute ground truth embeddings for training data (for feature normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc750b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is erroring, feature extraction failed in Train.ipynb\n",
    "if normalize_preds:\n",
    "    file_path = f\"{data_path}/preprocessed_data/subject{subj}/{image_embedding_variant}_image_embeddings_train.pt\"\n",
    "    clip_image_train = torch.load(file_path)\n",
    "        \n",
    "    if dual_guidance:\n",
    "        file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{text_embedding_variant}_text_embeddings_train.pt\"\n",
    "        clip_text_train = torch.load(file_path_txt)\n",
    "        \n",
    "    if blurry_recon:\n",
    "        file_path = f\"{data_path}/preprocessed_data/subject{subj}/{latent_embedding_variant}_latent_embeddings_train.pt\"\n",
    "        vae_image_train = torch.load(file_path)\n",
    "    else:\n",
    "        strength = 1.0\n",
    "        \n",
    "    if prompt_recon:\n",
    "        file_path_prompt = f\"{data_path}/preprocessed_data/subject{subj}/{prompt_embedding_variant}_prompt_embeddings_train.pt\"\n",
    "        git_text_train = torch.load(file_path_prompt) \n",
    "           \n",
    "    if retrieval:\n",
    "        file_path = f\"{data_path}/preprocessed_data/subject{subj}/{retrieval_embedding_variant}_retrieval_embeddings_train.pt\"\n",
    "        retrieval_image_train = torch.load(file_path)\n",
    "    else:\n",
    "        num_images_per_sample = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dbfdb",
   "metadata": {},
   "source": [
    "# Predicting latent vectors for reconstruction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gt:\n",
    "    file_path = f\"{data_path}/preprocessed_data/gt/{image_embedding_variant}_image_embeddings.pt\"\n",
    "    os.makedirs(f\"{data_path}/preprocessed_data/gt\", exist_ok=True)\n",
    "    emb_batch_size = 1\n",
    "    if not os.path.exists(file_path):\n",
    "        # Generate CLIP Image embeddings\n",
    "        print(\"Generating Image embeddings!\")\n",
    "        pred_clip_image = torch.zeros((len(all_images), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "        for i in tqdm(range(len(all_images) // emb_batch_size), desc=\"Encoding clip images...\"):\n",
    "            batch_list = []\n",
    "            for img in all_images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]:\n",
    "                batch_list.append(transforms.ToPILImage()(img))\n",
    "            pred_clip_image[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = reconstructor.embed_image(batch_list).to(\"cpu\")\n",
    "\n",
    "        torch.save(pred_clip_image, file_path)\n",
    "    else:\n",
    "        pred_clip_image = torch.load(file_path)\n",
    "\n",
    "            \n",
    "    if dual_guidance:\n",
    "        emb_batch_size = 1\n",
    "        gt_captions = np.load(f\"{data_path}/preprocessed_data/captions_18.npy\")\n",
    "        file_path_txt = f\"{data_path}/preprocessed_data/gt/{text_embedding_variant}_text_embeddings.pt\"\n",
    "        os.makedirs(f\"{data_path}/preprocessed_data/gt\", exist_ok=True)\n",
    "        if not os.path.exists(file_path_txt):\n",
    "            # Generate CLIP Text embeddings\n",
    "            print(\"Generating Text embeddings!\")\n",
    "            pred_clip_text = torch.zeros((len(gt_captions), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "            for i in tqdm(range(len(gt_captions) // emb_batch_size), desc=\"Encoding captions...\"):\n",
    "                batch_captions = gt_captions[i * emb_batch_size:i * emb_batch_size + emb_batch_size].tolist()\n",
    "                pred_clip_text[i * emb_batch_size:i * emb_batch_size + emb_batch_size] =  reconstructor.embed_text(batch_captions).to(\"cpu\")\n",
    "            torch.save(pred_clip_text, file_path_txt)\n",
    "        else:\n",
    "            pred_clip_text = torch.load(file_path_txt)\n",
    "\n",
    "\n",
    "    if blurry_recon:\n",
    "        emb_batch_size = 1\n",
    "        file_path = f\"{data_path}/preprocessed_data/gt/{latent_embedding_variant}_latent_embeddings.pt\"\n",
    "        os.makedirs(f\"{data_path}/preprocessed_data/gt\", exist_ok=True)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"Generating Latent Image embeddings!\")\n",
    "            pred_blurry_vae = torch.zeros((len(all_images), latent_emb_dim)).to(\"cpu\")\n",
    "            for i in tqdm(range(len(all_images)), desc=\"Encoding blurry images...\"):\n",
    "                img = transforms.ToPILImage()(all_images[i])\n",
    "                pred_blurry_vae[i * emb_batch_size:i * emb_batch_size + emb_batch_size] = vdvae.embed_latent(img).reshape(-1, latent_emb_dim).to(\"cpu\")\n",
    "            torch.save(pred_blurry_vae, file_path)\n",
    "        else:\n",
    "            pred_blurry_vae = torch.load(file_path)\n",
    "        \n",
    "    print(f\"Loaded vectors for subj{subj}!\")\n",
    "else:\n",
    "    pred_clip_image = torch.zeros((len(all_images), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "    with open(f'{outdir}/ridge_image_weights.pkl', 'rb') as f:\n",
    "        image_datadict = pickle.load(f)\n",
    "    model = Ridge(\n",
    "        alpha=100000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = image_datadict[\"coef\"]\n",
    "    model.intercept_ = image_datadict[\"intercept\"]\n",
    "    pred_clip_image = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_seq_dim, clip_emb_dim))\n",
    "\n",
    "    if dual_guidance:\n",
    "        with open(f'{outdir}/ridge_text_weights.pkl', 'rb') as f:\n",
    "            text_datadict = pickle.load(f)\n",
    "        pred_clip_text = torch.zeros((len(all_images), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "        model = Ridge(\n",
    "            alpha=100000,\n",
    "            max_iter=50000,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.coef_ = text_datadict[\"coef\"]\n",
    "        model.intercept_ = text_datadict[\"intercept\"]\n",
    "        pred_clip_text = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_text_seq_dim, clip_text_emb_dim))\n",
    "\n",
    "    if prompt_recon:\n",
    "        with open(f'{outdir}/ridge_prompt_weights.pkl', 'rb') as f:\n",
    "            prompt_datadict = pickle.load(f)\n",
    "        pred_git_text = torch.zeros((len(all_images), git_seq_dim, git_emb_dim)).to(\"cpu\")\n",
    "        model = Ridge(\n",
    "            alpha=100000,\n",
    "            max_iter=50000,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.coef_ = prompt_datadict[\"coef\"]\n",
    "        model.intercept_ = prompt_datadict[\"intercept\"]\n",
    "        pred_git_text = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, git_seq_dim, git_emb_dim))\n",
    "\n",
    "    if blurry_recon:\n",
    "        pred_blurry_vae = torch.zeros((len(all_images), latent_emb_dim)).to(\"cpu\")\n",
    "        with open(f'{outdir}/ridge_blurry_weights.pkl', 'rb') as f:\n",
    "            blurry_datadict = pickle.load(f)\n",
    "        model = Ridge(\n",
    "            alpha=100000,\n",
    "            max_iter=50000,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model.coef_ = blurry_datadict[\"coef\"]\n",
    "        model.intercept_ = blurry_datadict[\"intercept\"]\n",
    "        pred_blurry_vae = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, latent_emb_dim))    \n",
    "\n",
    "    if retrieval:\n",
    "        pred_retrieval = torch.zeros((len(all_images), retrieval_seq_dim, retrieval_emb_dim)).to(\"cpu\")\n",
    "        with open(f'{outdir}/ridge_retrieval_weights.pkl', 'rb') as f:\n",
    "            retrieval_datadict = pickle.load(f)\n",
    "        model = Ridge(\n",
    "            alpha=100000,\n",
    "            max_iter=50000,\n",
    "            random_state=42,\n",
    "        )\n",
    "        voxels_norm = torch.nn.functional.normalize(voxels[:,0], p=2, dim=1)\n",
    "        model.coef_ = retrieval_datadict[\"coef\"]\n",
    "        model.intercept_ = retrieval_datadict[\"intercept\"]\n",
    "        pred_retrieval = torch.from_numpy(model.predict(voxels_norm).reshape(-1, retrieval_seq_dim, retrieval_emb_dim))\n",
    "        \n",
    "        \n",
    "    if normalize_preds:\n",
    "        std_pred_clip_image = (pred_clip_image - torch.mean(pred_clip_image,axis=0)) / (torch.std(pred_clip_image,axis=0) + 1e-6)\n",
    "        pred_clip_image = std_pred_clip_image * torch.std(clip_image_train,axis=0) + torch.mean(clip_image_train,axis=0)\n",
    "        del clip_image_train\n",
    "        if dual_guidance:\n",
    "            std_pred_clip_text = (pred_clip_text - torch.mean(pred_clip_text,axis=0)) / (torch.std(pred_clip_text,axis=0) + 1e-6)\n",
    "            pred_clip_text = std_pred_clip_text * torch.std(clip_text_train,axis=0) + torch.mean(clip_text_train,axis=0)\n",
    "            del clip_text_train\n",
    "        if blurry_recon:\n",
    "            std_pred_blurry_vae = (pred_blurry_vae - torch.mean(pred_blurry_vae,axis=0)) / (torch.std(pred_blurry_vae,axis=0) + 1e-6)\n",
    "            pred_blurry_vae = std_pred_blurry_vae * torch.std(vae_image_train,axis=0) + torch.mean(vae_image_train,axis=0)\n",
    "            del vae_image_train\n",
    "        if retrieval:\n",
    "            std_pred_retrieval = (pred_retrieval - torch.mean(pred_retrieval,axis=0)) / (torch.std(pred_retrieval,axis=0) + 1e-6)\n",
    "            pred_retrieval = std_pred_retrieval * torch.std(retrieval_image_train,axis=0) + torch.mean(retrieval_image_train,axis=0)\n",
    "            # L2 Normalize for optimal cosine similarity\n",
    "            pred_retrieval = torch.nn.functional.normalize(pred_retrieval, p=2, dim=2)\n",
    "            del retrieval_image_train\n",
    "        if prompt_recon:\n",
    "            for sequence in range(git_seq_dim):\n",
    "                std_pred_git_text = (pred_git_text[:, sequence] - torch.mean(pred_git_text[:, sequence],axis=0)) / (torch.std(pred_git_text[:, sequence],axis=0) + 1e-6)\n",
    "                pred_git_text[:, sequence] = std_pred_git_text * torch.std(git_text_train[:, sequence],axis=0) + torch.mean(git_text_train[:, sequence],axis=0)\n",
    "            del git_text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb36f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gtc:\n",
    "    emb_batch_size = 1\n",
    "    gt_captions = np.load(f\"{data_path}/preprocessed_data/captions_18.npy\")\n",
    "    file_path_txt = f\"{data_path}/preprocessed_data/gt/{text_embedding_variant}_text_embeddings.pt\"\n",
    "    os.makedirs(f\"{data_path}/preprocessed_data/gt\", exist_ok=True)\n",
    "    if not os.path.exists(file_path_txt):\n",
    "        # Generate CLIP Text embeddings\n",
    "        print(\"Generating Text embeddings!\")\n",
    "        pred_clip_text = torch.zeros((len(gt_captions), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "        for i in tqdm(range(len(gt_captions) // emb_batch_size), desc=\"Encoding captions...\"):\n",
    "            batch_captions = gt_captions[i * emb_batch_size:i * emb_batch_size + emb_batch_size].tolist()\n",
    "            pred_clip_text[i * emb_batch_size:i * emb_batch_size + emb_batch_size] =  reconstructor.embed_text(batch_captions).to(\"cpu\")\n",
    "        torch.save(pred_clip_text, file_path_txt)\n",
    "    else:\n",
    "        pred_clip_text = torch.load(file_path_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_recons = None\n",
    "final_blurryrecons = None\n",
    "if save_raw:\n",
    "    model_tag = f\"{strength}-str_{textstrength}-mix\"\n",
    "    if gt:\n",
    "        model_tag += \"_gt\"\n",
    "    elif gtc:\n",
    "        model_tag += \"_gtc\"\n",
    "    raw_root = f\"{raw_path}/{mode}/mirage_augmentations/{model_tag}/subject{subj}/\"\n",
    "    print(\"raw_root:\", raw_root)\n",
    "    os.makedirs(raw_root,exist_ok=True)\n",
    "    torch.save(pred_clip_image, f\"{raw_root}/{image_embedding_variant}_image_voxels.pt\")\n",
    "    if dual_guidance:\n",
    "        torch.save(pred_clip_text, f\"{raw_root}/{text_embedding_variant}_text_voxels.pt\")\n",
    "    if blurry_recon:\n",
    "        torch.save(pred_blurry_vae, f\"{raw_root}/{latent_embedding_variant}_latent_voxels.pt\")\n",
    "    if retrieval:\n",
    "        torch.save(pred_retrieval, f\"{raw_root}/{retrieval_embedding_variant}_retrieval_voxels.pt\")\n",
    "\n",
    "\n",
    "for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "    clip_voxels = pred_clip_image[idx]\n",
    "    if dual_guidance:\n",
    "        clip_text_voxels = pred_clip_text[idx]\n",
    "    else:\n",
    "        clip_text_voxels = None\n",
    "    \n",
    "    latent_voxels=None\n",
    "    if blurry_recon:\n",
    "        latent_voxels = pred_blurry_vae[idx].unsqueeze(0)\n",
    "        blurred_image = vdvae.reconstruct(latents=latent_voxels)\n",
    "        if filter_sharpness:\n",
    "            # This helps make the output not blurry when using the VDVAE\n",
    "            blurred_image = ImageEnhance.Sharpness(blurred_image).enhance(20)\n",
    "        if filter_contrast:\n",
    "            # This boosts the structural impact of the blurred_image\n",
    "            blurred_image = ImageEnhance.Contrast(blurred_image).enhance(1.5)\n",
    "        im = transforms.ToTensor()(blurred_image)\n",
    "        if final_blurryrecons is None:\n",
    "            final_blurryrecons = im.cpu()\n",
    "        else:\n",
    "            final_blurryrecons = torch.vstack((final_blurryrecons, im.cpu()))\n",
    "                \n",
    "    samples = reconstructor.reconstruct(image=blurred_image,\n",
    "                                        c_i=clip_voxels,\n",
    "                                        c_t=clip_text_voxels,\n",
    "                                        n_samples=gen_rep,\n",
    "                                        textstrength=textstrength,\n",
    "                                        strength=strength)\n",
    "    \n",
    "    if save_raw:\n",
    "        os.makedirs(f\"{raw_root}/{idx}/\", exist_ok=True)\n",
    "        for rep in range(gen_rep):\n",
    "            transforms.ToPILImage()(samples[rep]).save(f\"{raw_root}/{idx}/{rep}.png\")\n",
    "        \n",
    "        if rep == 0:\n",
    "            transforms.ToPILImage()(all_images[idx]).save(f\"{raw_root}/{idx}/ground_truth.png\")\n",
    "            if blurry_recon:\n",
    "                transforms.ToPILImage()(transforms.ToTensor()(blurred_image).cpu()).save(f\"{raw_root}/{idx}/low_level.png\")\n",
    "            torch.save(clip_voxels, f\"{raw_root}/{idx}/clip_image_voxels.pt\")\n",
    "            if dual_guidance:\n",
    "                torch.save(clip_text_voxels, f\"{raw_root}/{idx}/clip_text_voxels.pt\")\n",
    "            if prompt_recon:\n",
    "                with open(f\"{raw_root}/{idx}/predicted_caption.txt\", \"w\") as f:\n",
    "                    f.write(all_predcaptions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8966a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye_imagery_sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
