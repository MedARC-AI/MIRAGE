{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: pretrained_subj01_40sess_hypatia_vd2_sessions40\n",
      "--data_path=/weka/proj-medarc/shared/umn-imagery                     --cache_dir=/weka/proj-medarc/shared/cache                     --model_name=pretrained_subj01_40sess_hypatia_vd2_sessions40 --subj=1                     --hidden_dim=1024 --n_blocks=4 --mode vision --no-blurry_recon\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # model_name = \"final_subj01_pretrained_40sess_24bs\"\n",
    "    model_name = \"pretrained_subj01_40sess_hypatia_vd2_sessions40\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/umn-imagery \\\n",
    "                    --cache_dir=/weka/proj-medarc/shared/cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --hidden_dim=1024 --n_blocks=4 --mode vision --no-blurry_recon\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10,11],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--rep\",type=int,default=1,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "\n",
    "if seed > 0 and rep == 1:\n",
    "    # seed all random functions, but only if doing 1 rep\n",
    "    utils.seed_everything(seed)\n",
    "\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 8, 15724]) torch.Size([18, 3, 425, 425])\n"
     ]
    }
   ],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, all_images = utils.load_nsd_synthetic(subject=subj, average=False, nest=True, data_root=data_path)\n",
    "elif subj > 8:\n",
    "    _, _, voxels, all_images = utils.load_imageryrf(subject=subj-8, mode=mode, stimtype=\"object\", average=False, nest=True, split=True, data_root=data_path)\n",
    "else:\n",
    "    voxels, all_images = utils.load_nsd_mental_imagery(subject=subj, mode=mode, stimtype=\"all\", average=False, nest=True, data_root=data_path)\n",
    "num_voxels = voxels.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afc4858-b6a6-4a52-9303-b4a50ea5cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n",
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "218,016,636 total\n",
      "218,016,636 trainable\n",
      "param counts:\n",
      "234,119,036 total\n",
      "234,119,036 trainable\n",
      "param counts:\n",
      "56,055,184 total\n",
      "56,055,168 trainable\n",
      "param counts:\n",
      "290,174,220 total\n",
      "290,174,204 trainable\n",
      "\n",
      "---loading /weka/proj-fmri/ckadirt/MindEye_Imagery/train_logs/pretrained_subj01_40sess_hypatia_vd2_sessions40/last.pth ckpt---\n",
      "\n",
      "ckpt loaded!\n"
     ]
    }
   ],
   "source": [
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=True, norm_embs=True, device=device)\n",
    "clip_seq_dim = 257\n",
    "clip_emb_dim = 768\n",
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    # Create a mapping from the old layer names to the new layer names\n",
    "    layer_mapping = {\n",
    "        \"encoder.mid_block.attentions.0.to_q.weight\": \"encoder.mid_block.attentions.0.query.weight\",\n",
    "        \"encoder.mid_block.attentions.0.to_q.bias\": \"encoder.mid_block.attentions.0.query.bias\",\n",
    "        \"encoder.mid_block.attentions.0.to_k.weight\": \"encoder.mid_block.attentions.0.key.weight\",\n",
    "        \"encoder.mid_block.attentions.0.to_k.bias\": \"encoder.mid_block.attentions.0.key.bias\",\n",
    "        \"encoder.mid_block.attentions.0.to_v.weight\": \"encoder.mid_block.attentions.0.value.weight\",\n",
    "        \"encoder.mid_block.attentions.0.to_v.bias\": \"encoder.mid_block.attentions.0.value.bias\",\n",
    "        \"encoder.mid_block.attentions.0.to_out.0.weight\": \"encoder.mid_block.attentions.0.proj_attn.weight\",\n",
    "        \"encoder.mid_block.attentions.0.to_out.0.bias\": \"encoder.mid_block.attentions.0.proj_attn.bias\",\n",
    "        \"decoder.mid_block.attentions.0.to_q.weight\": \"decoder.mid_block.attentions.0.query.weight\",\n",
    "        \"decoder.mid_block.attentions.0.to_q.bias\": \"decoder.mid_block.attentions.0.query.bias\",\n",
    "        \"decoder.mid_block.attentions.0.to_k.weight\": \"decoder.mid_block.attentions.0.key.weight\",\n",
    "        \"decoder.mid_block.attentions.0.to_k.bias\": \"decoder.mid_block.attentions.0.key.bias\",\n",
    "        \"decoder.mid_block.attentions.0.to_v.weight\": \"decoder.mid_block.attentions.0.value.weight\",\n",
    "        \"decoder.mid_block.attentions.0.to_v.bias\": \"decoder.mid_block.attentions.0.value.bias\",\n",
    "        \"decoder.mid_block.attentions.0.to_out.0.weight\": \"decoder.mid_block.attentions.0.proj_attn.weight\",\n",
    "        \"decoder.mid_block.attentions.0.to_out.0.bias\": \"decoder.mid_block.attentions.0.proj_attn.bias\"\n",
    "    }\n",
    "\n",
    "    # Create a new state dictionary with the renamed layers\n",
    "    new_ckpt = {}\n",
    "    for old_key, value in ckpt.items():\n",
    "        new_key = layer_mapping.get(old_key, old_key)  # Get the new key, or use the old key if not in mapping\n",
    "        new_ckpt[new_key] = value\n",
    "    autoenc.load_state_dict(new_ckpt)\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim, seq_len=seq_len)\n",
    "\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=2, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "        \n",
    "        if blurry_recon:\n",
    "            self.blin1 = nn.Linear(h*seq_len,4*28*28,bias=True)\n",
    "            self.bdropout = nn.Dropout(.3)\n",
    "            self.bnorm = nn.GroupNorm(1, 64)\n",
    "            self.bupsampler = Decoder(\n",
    "                in_channels=64,\n",
    "                out_channels=4,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[32, 64, 128],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "            self.b_maps_projector = nn.Sequential(\n",
    "                nn.Conv2d(64, 512, 1, bias=False),\n",
    "                nn.GroupNorm(1,512),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(512, 512, 1, bias=False),\n",
    "                nn.GroupNorm(1,512),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(512, 512, 1, bias=True),\n",
    "            )\n",
    "            \n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b,t = torch.Tensor([0.]), torch.Tensor([[0.],[0.]]), torch.Tensor([0.])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        c = self.clip_proj(backbone)\n",
    "\n",
    "        if blurry_recon:\n",
    "            b = self.blin1(x)\n",
    "            b = self.bdropout(b)\n",
    "            b = b.reshape(b.shape[0], -1, 7, 7).contiguous()\n",
    "            b = self.bnorm(b)\n",
    "            b_aux = self.b_maps_projector(b).flatten(2).permute(0,2,1)\n",
    "            b_aux = b_aux.view(len(b_aux), 49, 512)\n",
    "            b = (self.bupsampler(b), b_aux)\n",
    "        \n",
    "        return backbone, c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = clip_emb_dim//64 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "utils.count_params(model.diffusion_prior)\n",
    "utils.count_params(model)\n",
    "\n",
    "# Load pretrained model ckpt\n",
    "tag='last'\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "try:\n",
    "    checkpoint = torch.load(outdir+f'/{tag}.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    layer_mapping = {\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_q.weight\": \"backbone.bupsampler.mid_block.attentions.0.query.weight\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_q.bias\": \"backbone.bupsampler.mid_block.attentions.0.query.bias\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_k.weight\": \"backbone.bupsampler.mid_block.attentions.0.key.weight\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_k.bias\": \"backbone.bupsampler.mid_block.attentions.0.key.bias\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_v.weight\": \"backbone.bupsampler.mid_block.attentions.0.value.weight\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_v.bias\": \"backbone.bupsampler.mid_block.attentions.0.value.bias\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_out.0.weight\": \"backbone.bupsampler.mid_block.attentions.0.proj_attn.weight\",\n",
    "        \"backbone.bupsampler.mid_block.attentions.0.to_out.0.bias\": \"backbone.bupsampler.mid_block.attentions.0.proj_attn.bias\"\n",
    "    }\n",
    "    new_ckpt = {}\n",
    "    for old_key, value in state_dict.items():\n",
    "        new_key = layer_mapping.get(old_key, old_key)  # Get the new key, or use the old key if not in mapping\n",
    "        new_ckpt[new_key] = value\n",
    "    \n",
    "    model.load_state_dict(new_ckpt, strict=True)\n",
    "    del checkpoint\n",
    "except: # probably ckpt is saved using deepspeed format\n",
    "    import deepspeed\n",
    "    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del state_dict\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295824db-ab3d-450c-90fb-f656e48994ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup text caption networks\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from modeling_git import GitForCausalLMClipEmb\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "clip_text_model.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "clip_text_seq_dim = 257\n",
    "clip_text_emb_dim = 1024\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(clip_seq_dim, clip_text_seq_dim)\n",
    "        self.linear2 = nn.Linear(clip_emb_dim, clip_text_emb_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "        \n",
    "# clip_convert = CLIPConverter()\n",
    "# state_dict = torch.load(f\"{cache_dir}/bigG_to_L_epoch8.pth\", map_location='cpu')['model_state_dict']\n",
    "# clip_convert.load_state_dict(state_dict, strict=True)\n",
    "# clip_convert.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "# del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating versatile diffusion reconstruction pipeline...\n",
      "Downloading Versatile Diffusion to /weka/proj-medarc/shared/cache\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df5bf0fd3a04c98824921897f771077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "print('Creating versatile diffusion reconstruction pipeline...')\n",
    "from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "from diffusers.models import DualTransformer2DModel\n",
    "# vd_cache_dir = \"/home/naxos2-raid25/kneel027/home/kneel027/fMRI-reconstruction-NSD/versatile_diffusion\"\n",
    "# try:\n",
    "#     vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(cache_dir).to(device)\n",
    "# except:\n",
    "print(\"Downloading Versatile Diffusion to\", cache_dir)\n",
    "vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "        \"shi-labs/versatile-diffusion\",\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir = cache_dir).to(device)\n",
    "vd_pipe.remove_unused_weights()\n",
    "vd_pipe.image_unet.eval()\n",
    "vd_pipe.vae.eval()\n",
    "vd_pipe.image_unet.requires_grad_(False)\n",
    "vd_pipe.vae.requires_grad_(False)\n",
    "\n",
    "vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(cache_dir + \"/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7\", subfolder=\"scheduler\")\n",
    "num_inference_steps = 20\n",
    "\n",
    "# Set weighting of Dual-Guidance \n",
    "text_image_ratio = .0 # .5 means equally weight text and image, 0 means use only image\n",
    "for name, module in vd_pipe.image_unet.named_modules():\n",
    "    if isinstance(module, DualTransformer2DModel):\n",
    "        module.mix_ratio = text_image_ratio\n",
    "        for i, type in enumerate((\"text\", \"image\")):\n",
    "            if type == \"text\":\n",
    "                module.condition_lengths[i] = 77\n",
    "                module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "            else:\n",
    "                module.condition_lengths[i] = 257\n",
    "                module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "\n",
    "unet = vd_pipe.image_unet\n",
    "vae = vd_pipe.vae\n",
    "noise_scheduler = vd_pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:   0%|                                                                                                                                                                                       | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82311249f6345aa9ec37278abd52de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-ckadirt/mindeye/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "sample loop:   6%|█████████▋                                                                                                                                                                     | 1/18 [00:08<02:29,  8.78s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb1e3796be242d4b588368de134b748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  11%|███████████████████▍                                                                                                                                                           | 2/18 [00:14<01:51,  6.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecb2d7d369c4119b2071b0211774a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  17%|█████████████████████████████▏                                                                                                                                                 | 3/18 [00:20<01:35,  6.38s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d94c56ad77e4f0c9b6738f527c5d852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  22%|██████████████████████████████████████▉                                                                                                                                        | 4/18 [00:25<01:25,  6.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3b86f4a5aa4ce99af526d104d48306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  28%|████████████████████████████████████████████████▌                                                                                                                              | 5/18 [00:31<01:17,  5.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f23404633049a1b4919879b5300c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  33%|██████████████████████████████████████████████████████████▎                                                                                                                    | 6/18 [00:37<01:10,  5.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035cd6abfc7f454b84f0ed366e24629f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  39%|████████████████████████████████████████████████████████████████████                                                                                                           | 7/18 [00:42<01:03,  5.79s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c22639a644448168303d9c2ada17694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  44%|█████████████████████████████████████████████████████████████████████████████▊                                                                                                 | 8/18 [00:48<00:57,  5.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130f4617c1e04e9eae3e96e9c156e949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  50%|███████████████████████████████████████████████████████████████████████████████████████▌                                                                                       | 9/18 [00:54<00:51,  5.73s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec721c68e6946de918e5eaaac336e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  56%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                             | 10/18 [00:59<00:45,  5.71s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39058ea916b34f17b40f4c90e7c8a605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                   | 11/18 [01:05<00:39,  5.70s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b96296939a47dc9be89960f24091d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                          | 12/18 [01:11<00:34,  5.69s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a1f059ef9c48288ca0549511e2f65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 13/18 [01:16<00:28,  5.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3de2e7b1c6a4436aa5ad95c2c131936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 14/18 [01:22<00:22,  5.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3088159f16a42358d2a560cf06c90ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 15/18 [01:28<00:17,  5.67s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07bb055529f4c9aad6249465e608e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 16/18 [01:33<00:11,  5.67s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d18e19838fc49328cac059f98ad40bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop:  94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎         | 17/18 [01:39<00:05,  5.67s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4b77fa28734f4aa0ab5b050bbf2f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample loop: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [01:45<00:00,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 3, 512, 512])\n",
      "torch.Size([18, 3, 256, 256])\n",
      "saved pretrained_subj01_40sess_hypatia_vd2_sessions40 mi outputs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get all reconstructions\n",
    "model.to(device)\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "# all_images = None\n",
    "all_blurryrecons = None\n",
    "all_recons = None\n",
    "all_predcaptions = []\n",
    "all_clipvoxels = None\n",
    "\n",
    "minibatch_size = 1\n",
    "num_samples_per_image = 1\n",
    "plotting = False\n",
    "recons_per_sample = 16\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "    for r in tqdm(range(rep), desc=\"rep loop\"):\n",
    "        for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "            voxel = voxels[idx]\n",
    "            voxel = voxel.to(device)\n",
    "            for rep in range(voxel.shape[0]):\n",
    "                voxel_ridge = model.ridge(voxel[None,None,rep],0) # 0th index of subj_list\n",
    "                backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "                if rep==0:\n",
    "                    clip_voxels = clip_voxels0\n",
    "                    backbone = backbone0\n",
    "                    blurry_image_enc = blurry_image_enc0[0]\n",
    "                else:\n",
    "                    clip_voxels += clip_voxels0\n",
    "                    backbone += backbone0\n",
    "                    blurry_image_enc += blurry_image_enc0[0]\n",
    "            clip_voxels /= voxel.shape[0]\n",
    "            backbone /= voxel.shape[0]\n",
    "            blurry_image_enc /= voxel.shape[0]\n",
    "                    \n",
    "            # Save retrieval submodule outputs\n",
    "            if all_clipvoxels is None:\n",
    "                all_clipvoxels = clip_voxels.to('cpu')\n",
    "            else:\n",
    "                all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.to('cpu')))\n",
    "            \n",
    "            # Feed voxels through versatile diffusion diffusion prior\n",
    "            backbone = backbone.repeat(recons_per_sample, 1, 1)\n",
    "            prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                            text_cond = dict(text_embed = backbone), \n",
    "                            cond_scale = 1., timesteps = 20)\n",
    "            \n",
    "            # pred_caption_emb = clip_convert(prior_out)\n",
    "            # generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "            # generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            # all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "            \n",
    "            if blurry_recon:\n",
    "                blurred_image = (autoenc.decode(blurry_image_enc/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                \n",
    "                im = torch.Tensor(blurred_image)\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im.cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im.cpu()))\n",
    "                if plotting:\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(im))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "            \n",
    "            # Feed diffusion prior outputs through versatile diffusion\n",
    "            text_token = None\n",
    "            generator = torch.Generator(device=device)\n",
    "            samples, brain_recons, best_picks = utils.versatile_diffusion_recon(brain_clip_embeddings=prior_out, \n",
    "                                proj_embeddings = clip_voxels, \n",
    "                                img_lowlevel = blurred_image, \n",
    "                                img2img_strength = .85, \n",
    "                                text_token=text_token,\n",
    "                                clip_extractor = clip_extractor, \n",
    "                                vae=vae, \n",
    "                                unet=unet, \n",
    "                                noise_scheduler=noise_scheduler, \n",
    "                                generator=generator,\n",
    "                                num_inference_steps = num_inference_steps,\n",
    "                                recons_per_sample=recons_per_sample)\n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    \n",
    "            if plotting: \n",
    "                print(model_name)\n",
    "                err # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "    # resize outputs before saving\n",
    "    imsize = 256\n",
    "    print(all_recons.shape)\n",
    "    all_recons = transforms.Resize((imsize,imsize))(all_recons).float()\n",
    "    if blurry_recon: \n",
    "        all_blurryrecons = transforms.Resize((imsize,imsize))(all_blurryrecons).float()\n",
    "\n",
    "            \n",
    "    if all_recons_reps is None:\n",
    "        all_recons_reps = all_recons.cpu()\n",
    "        all_blurryrecons_reps = all_blurryrecons.cpu()\n",
    "        all_clipvoxels_reps = all_clipvoxels.cpu()\n",
    "    else:\n",
    "        all_recons_reps = torch.vstack((all_recons_reps, all_recons.cpu()))\n",
    "        all_blurryrecons_reps = torch.vstack((all_blurryrecons_reps, all_blurryrecons.cpu()))\n",
    "        all_clipvoxels_reps = torch.vstack((all_clipvoxels_reps, all_clipvoxels.cpu()))\n",
    "        \n",
    "        \n",
    "# saving\n",
    "print(all_recons.shape)\n",
    "# torch.save(all_images,\"evals/all_images.pt\")\n",
    "if blurry_recon:\n",
    "    torch.save(all_blurryrecons,f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}.pt\")\n",
    "torch.save(all_recons,f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\")\n",
    "# torch.save(all_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions_{mode}.pt\")\n",
    "torch.save(all_clipvoxels,f\"evals/{model_name}/{model_name}_all_clipvoxels_{mode}.pt\")\n",
    "print(f\"saved {model_name} mi outputs!\")\n",
    "\n",
    "print(all_recons_reps.shape)\n",
    "# torch.save(all_images,\"evals/all_images.pt\")\n",
    "if blurry_recon:\n",
    "    torch.save(all_blurryrecons_reps,f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}_{reps}reps.pt\")\n",
    "torch.save(all_recons_reps,f\"evals/{model_name}/{model_name}_all_recons_{mode}_{reps}reps.pt\")\n",
    "# torch.save(all_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions_{mode}.pt\")\n",
    "torch.save(all_clipvoxels_reps,f\"evals/{model_name}/{model_name}_all_clipvoxels_{mode}_{reps}reps.pt\")\n",
    "print(f\"saved {model_name} mi outputs {reps} reps!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b703704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 3, 150, 150]) torch.Size([18, 3, 150, 150])\n",
      "saved ../figs/pretrained_subj01_40sess_hypatia_vd2_sessions40_18recons_vision.png\n"
     ]
    }
   ],
   "source": [
    "imsize = 150\n",
    "if all_images.shape[-1] != imsize:\n",
    "    all_images = transforms.Resize((imsize,imsize))(transforms.CenterCrop(all_images.shape[2])(all_images)).float()\n",
    "if all_recons.shape[-1] != imsize:\n",
    "    all_recons = transforms.Resize((imsize,imsize))(transforms.CenterCrop(all_images.shape[2])(all_recons)).float()\n",
    "print(all_images.shape, all_recons.shape)\n",
    "num_images = all_recons.shape[0]\n",
    "num_rows = (2 * num_images + 11) // 12\n",
    "\n",
    "# Interleave tensors\n",
    "merged = torch.stack([val for pair in zip(all_images, all_recons) for val in pair], dim=0)\n",
    "\n",
    "# Calculate grid size\n",
    "grid = torch.zeros((num_rows * 12, 3, all_recons.shape[-1], all_recons.shape[-1]))\n",
    "\n",
    "# Populate the grid\n",
    "grid[:2*num_images] = merged\n",
    "grid_images = [transforms.functional.to_pil_image(grid[i]) for i in range(num_rows * 12)]\n",
    "\n",
    "# Create the grid image\n",
    "grid_image = Image.new('RGB', (all_recons.shape[-1] * 12, all_recons.shape[-1] * num_rows))  # 12 images wide\n",
    "\n",
    "# Paste images into the grid\n",
    "for i, img in enumerate(grid_images):\n",
    "    grid_image.paste(img, (all_recons.shape[-1] * (i % 12), all_recons.shape[-1] * (i // 12)))\n",
    "\n",
    "# Create title row image\n",
    "title_height = 150\n",
    "title_image = Image.new('RGB', (grid_image.width, title_height), color=(255, 255, 255))\n",
    "draw = ImageDraw.Draw(title_image)\n",
    "font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", 38)  # Change font size to 3 times bigger (15*3)\n",
    "title_text = f\"Model: {model_name}, Mode: {mode}\"\n",
    "bbox = draw.textbbox((0, 0), title_text, font=font)\n",
    "text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "draw.text(((grid_image.width - text_width) / 2, (title_height - text_height) / 2), title_text, fill=\"black\", font=font)\n",
    "\n",
    "# Combine title and grid images\n",
    "final_image = Image.new('RGB', (grid_image.width, grid_image.height + title_height))\n",
    "final_image.paste(title_image, (0, 0))\n",
    "final_image.paste(grid_image, (0, title_height))\n",
    "\n",
    "final_image.save(f\"../figs/{model_name}_{len(all_recons)}recons_{mode}.png\")\n",
    "print(f\"saved ../figs/{model_name}_{len(all_recons)}recons_{mode}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd5a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
