{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageEnhance\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "# sys.path.append('generative_models/')\n",
    "# import sgm\n",
    "from sc_reconstructor import SC_Reconstructor\n",
    "from vdvae import VDVAE\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.linear_model import Ridge\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "# from models import *\n",
    "device = \"cuda\"\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: jonathan_unclip\n",
      "--data_path=/weka/proj-medarc/shared/mindeyev2_dataset                     --cache_dir=/weka/proj-medarc/shared/cache                     --model_name=jonathan_unclip --subj=1                     --mode imagery                     --no-dual_guidance --no-blurry_recon --no-prompt_recon\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # model_name = \"final_subj01_pretrained_40sess_24bs\"\n",
    "    model_name = \"jonathan_unclip\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \\\n",
    "                    --cache_dir=/weka/proj-medarc/shared/cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --mode vision \\\n",
    "                    --no-dual_guidance --no-blurry_recon\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(model_name='jonathan_unclip', data_path='/weka/proj-medarc/shared/mindeyev2_dataset', cache_dir='/weka/proj-medarc/shared/cache', subj=1, blurry_recon=False, seed=42, mode='imagery', gen_rep=10, dual_guidance=False, normalize_preds=True, save_raw=False, raw_path=None, strength=0.7, textstrength=0.5, filter_contrast=True, filter_sharpness=True, num_images_per_sample=16, retrieval=True, prompt_recon=False, caption_type='medium', compile_models=True, num_trial_reps=16)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",type=str,default=\"vision\",choices=[\"vision\",\"imagery\",\"shared1000\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gen_rep\",type=int,default=10,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dual_guidance\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--normalize_preds\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_raw\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--raw_path\",type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--strength\",type=float,default=0.70,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--textstrength\",type=float,default=0.5,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--filter_contrast\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Filter the low level output to be more intense and smoothed\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--filter_sharpness\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Filter the low level output to be more intense and smoothed\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_images_per_sample\",type=int, default=16,\n",
    "    help=\"Number of images to generate and select between for final recon\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--retrieval\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"Use the decoded captions for dual guidance\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prompt_recon\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Use for prompt generation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--caption_type\",type=str,default='medium',choices=['coco','short', 'medium', 'schmedium'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--compile_models\",action=argparse.BooleanOptionalAction, default=True,\n",
    "    help=\"Use for speeding up stable cascade\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_trial_reps\",type=int, default=16,\n",
    "    help=\"Number of trial repetitions to average test betas across\",\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "print(f\"args: {args}\")\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "\n",
    "if seed > 0 and gen_rep == 1:\n",
    "    # seed all random functions, but only if doing 1 rep\n",
    "    utils.seed_everything(seed)\n",
    "    \n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459b128",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load nsd mi: nsdgeneral False, whole brain False, top n rois -1, samplewise False\n",
      "torch.Size([18, 1, 15724]) torch.Size([18, 3, 425, 425])\n",
      "torch.Size([18, 1, 15724])\n"
     ]
    }
   ],
   "source": [
    "if mode == \"synthetic\":\n",
    "    voxels, all_images = utils.load_nsd_synthetic(subject=subj, average=False, nest=True)\n",
    "elif subj > 8:\n",
    "    _, _, voxels, all_images = utils.load_imageryrf(subject=subj-8, mode=mode, stimtype=\"object\", average=False, nest=True, split=True)\n",
    "elif mode == \"shared1000\":\n",
    "    x_train, valid_nsd_ids_train, x_test, test_nsd_ids = utils.load_nsd(subject=subj, data_path=data_path)\n",
    "    voxels = torch.mean(x_test, dim=1, keepdim=True)\n",
    "    print(f\"Loaded subj {subj} test betas! {voxels.shape}\")\n",
    "    f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "    images = f['images']\n",
    "\n",
    "    all_images = torch.zeros((len(test_nsd_ids), 3, 224, 224))\n",
    "    for i, idx in enumerate(test_nsd_ids):\n",
    "        all_images[i] =  torch.from_numpy(images[idx])\n",
    "    del images, f\n",
    "    print(f\"Filtered down to only the {len(test_nsd_ids)} test images for subject {subj}!\")\n",
    "else:\n",
    "    voxels, all_images = utils.load_nsd_mental_imagery(subject=subj, \n",
    "                                                       mode=mode, \n",
    "                                                       stimtype=\"all\", \n",
    "                                                       average=True, \n",
    "                                                       nest=False,\n",
    "                                                       num_reps=num_trial_reps,\n",
    "                                                       data_root=\"/weka/proj-medarc/shared/umn-imagery\")\n",
    "print(voxels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa6402",
   "metadata": {},
   "source": [
    "# Load pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345fe4d-b79c-4965-87a4-d7348906bcb2",
   "metadata": {},
   "source": [
    "### Load Stable Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8460aec-6c35-457e-9536-13f411bbcd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable Cascade Reconstructor: Loading model...\n",
      "['model_version', 'effnet_checkpoint_path', 'previewer_checkpoint_path']\n",
      "['transforms', 'clip_preprocess', 'gdf', 'sampling_configs', 'effnet_preprocess']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e06e4fe5204d0d9a08ef13f4e77ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenizer', 'text_model', 'generator', 'effnet', 'previewer']\n",
      "STAGE C READY\n"
     ]
    }
   ],
   "source": [
    "from sc_reconstructor import SC_Reconstructor\n",
    "reconstructor = SC_Reconstructor(compile_models=False, embedder_only=True, device=device, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9108d-f1f7-44ac-b305-7af09ba5d64b",
   "metadata": {},
   "source": [
    "### Load unCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1424061-d006-44d2-8d67-d18ca18ac3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "vector_suffix torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "\n",
    "# prep unCLIP\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "config = OmegaConf.load(\"generative_models/configs/inference/sd_xl_base.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "refiner_params = config[\"model\"][\"params\"]\n",
    "\n",
    "network_config = refiner_params[\"network_config\"]\n",
    "denoiser_config = refiner_params[\"denoiser_config\"]\n",
    "first_stage_config = refiner_params[\"first_stage_config\"]\n",
    "conditioner_config = refiner_params[\"conditioner_config\"]\n",
    "scale_factor = refiner_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = refiner_params[\"disable_first_stage_autocast\"]\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "\n",
    "diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                       denoiser_config=denoiser_config,\n",
    "                       first_stage_config=first_stage_config,\n",
    "                       conditioner_config=conditioner_config,\n",
    "                       sampler_config=sampler_config,\n",
    "                       scale_factor=scale_factor,\n",
    "                       disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "\n",
    "base_text_embedder1 = FrozenCLIPEmbedder(\n",
    "    layer=conditioner_config['params']['emb_models'][0]['params']['layer'],\n",
    "    layer_idx=conditioner_config['params']['emb_models'][0]['params']['layer_idx'],\n",
    ")\n",
    "base_text_embedder1.to(device)\n",
    "\n",
    "base_text_embedder2 = FrozenOpenCLIPEmbedder2(\n",
    "    arch=conditioner_config['params']['emb_models'][1]['params']['arch'],\n",
    "    version=conditioner_config['params']['emb_models'][1]['params']['version'],\n",
    "    freeze=conditioner_config['params']['emb_models'][1]['params']['freeze'],\n",
    "    layer=conditioner_config['params']['emb_models'][1]['params']['layer'],\n",
    "    always_return_pooled=conditioner_config['params']['emb_models'][1]['params']['always_return_pooled'],\n",
    "    legacy=conditioner_config['params']['emb_models'][1]['params']['legacy'],\n",
    ")\n",
    "base_text_embedder2.to(device)\n",
    "\n",
    "batch={\"txt\": \"\",\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(device),\n",
    "      \"target_size_as_tuple\": torch.ones(1, 2).to(device) * 1024}\n",
    "out = base_engine.conditioner(batch)\n",
    "crossattn = out[\"crossattn\"].to(device)\n",
    "vector_suffix = out[\"vector\"][:,-1536:].to(device)\n",
    "print(\"crossattn\", crossattn.shape)\n",
    "print(\"vector_suffix\", vector_suffix.shape)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd474cf9-c4f3-4fde-a612-1d666f2c48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1 # PS: I tried increasing this to 16 and picking highest cosine similarity like we did in MindEye1, it didnt seem to increase eval performance!\n",
    "img2img_timepoint = 13 # 9 # higher number means more reliance on prompt, less reliance on matching the conditioning image\n",
    "base_engine.sampler.guider.scale = 5 # 5 # cfg\n",
    "def denoiser(x, sigma, c): return base_engine.denoiser(base_engine.model, x, sigma, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81f715ac-62c6-498c-a885-2ab3a27995a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding_variant = \"ViT-bigG-14\"\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664\n",
    "\n",
    "retrieval_embedding_variant = \"stable_cascade_hidden\"\n",
    "retrieval_emb_dim = 1024\n",
    "retrieval_seq_dim = 257\n",
    "\n",
    "prompt_embedding_variant = \"git\"\n",
    "git_seq_dim = 257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592da0ed",
   "metadata": {},
   "source": [
    "# Load Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491a12d",
   "metadata": {},
   "source": [
    "### Compute ground truth embeddings for training data (for feature normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc750b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is erroring, feature extraction failed in Train.ipynb\n",
    "if normalize_preds:\n",
    "    file_path = f\"{data_path}/preprocessed_data/subject{subj}/{image_embedding_variant}_image_embeddings_train.pt\"\n",
    "    clip_image_train = torch.load(file_path)\n",
    "        \n",
    "    if dual_guidance:\n",
    "        file_path_txt = f\"{data_path}/preprocessed_data/subject{subj}/{text_embedding_variant}_text_embeddings_train.pt\"\n",
    "        clip_text_train = torch.load(file_path_txt)\n",
    "        \n",
    "    if blurry_recon:\n",
    "        file_path = f\"{data_path}/preprocessed_data/subject{subj}/{latent_embedding_variant}_latent_embeddings_train.pt\"\n",
    "        vae_image_train = torch.load(file_path)\n",
    "    else:\n",
    "        strength = 1.0\n",
    "        \n",
    "    if prompt_recon:\n",
    "        file_path_prompt = f\"{data_path}/preprocessed_data/subject{subj}/{prompt_embedding_variant}_prompt_embeddings_train.pt\"\n",
    "        git_text_train = torch.load(file_path_prompt) \n",
    "           \n",
    "    if retrieval:\n",
    "        file_path = f\"{data_path}/preprocessed_data/subject{subj}/{retrieval_embedding_variant}_retrieval_embeddings_train.pt\"\n",
    "        retrieval_image_train = torch.load(file_path)\n",
    "    else:\n",
    "        num_images_per_sample = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dbfdb",
   "metadata": {},
   "source": [
    "# Predicting latent vectors for reconstruction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79d24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clip_image = torch.zeros((len(all_images), clip_seq_dim, clip_emb_dim)).to(\"cpu\")\n",
    "with open(f'{outdir}/ridge_image_weights.pkl', 'rb') as f:\n",
    "    image_datadict = pickle.load(f)\n",
    "model = Ridge(\n",
    "    alpha=100000,\n",
    "    max_iter=50000,\n",
    "    random_state=42,\n",
    ")\n",
    "model.coef_ = image_datadict[\"coef\"]\n",
    "model.intercept_ = image_datadict[\"intercept\"]\n",
    "pred_clip_image = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_seq_dim, clip_emb_dim))\n",
    "\n",
    "if dual_guidance:\n",
    "    with open(f'{outdir}/ridge_text_weights.pkl', 'rb') as f:\n",
    "        text_datadict = pickle.load(f)\n",
    "    pred_clip_text = torch.zeros((len(all_images), clip_text_seq_dim, clip_text_emb_dim)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=100000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = text_datadict[\"coef\"]\n",
    "    model.intercept_ = text_datadict[\"intercept\"]\n",
    "    pred_clip_text = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, clip_text_seq_dim, clip_text_emb_dim))\n",
    "\n",
    "if prompt_recon:\n",
    "    with open(f'{outdir}/ridge_prompt_weights.pkl', 'rb') as f:\n",
    "        prompt_datadict = pickle.load(f)\n",
    "    pred_git_text = torch.zeros((len(all_images), git_seq_dim, git_emb_dim)).to(\"cpu\")\n",
    "    model = Ridge(\n",
    "        alpha=100000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = prompt_datadict[\"coef\"]\n",
    "    model.intercept_ = prompt_datadict[\"intercept\"]\n",
    "    pred_git_text = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, git_seq_dim, git_emb_dim))\n",
    "\n",
    "if blurry_recon:\n",
    "    pred_blurry_vae = torch.zeros((len(all_images), latent_emb_dim)).to(\"cpu\")\n",
    "    with open(f'{outdir}/ridge_blurry_weights.pkl', 'rb') as f:\n",
    "        blurry_datadict = pickle.load(f)\n",
    "    model = Ridge(\n",
    "        alpha=100000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.coef_ = blurry_datadict[\"coef\"]\n",
    "    model.intercept_ = blurry_datadict[\"intercept\"]\n",
    "    pred_blurry_vae = torch.from_numpy(model.predict(voxels[:,0]).reshape(-1, latent_emb_dim))    \n",
    "\n",
    "if retrieval:\n",
    "    pred_retrieval = torch.zeros((len(all_images), retrieval_seq_dim, retrieval_emb_dim)).to(\"cpu\")\n",
    "    with open(f'{outdir}/ridge_retrieval_weights.pkl', 'rb') as f:\n",
    "        retrieval_datadict = pickle.load(f)\n",
    "    model = Ridge(\n",
    "        alpha=100000,\n",
    "        max_iter=50000,\n",
    "        random_state=42,\n",
    "    )\n",
    "    voxels_norm = torch.nn.functional.normalize(voxels[:,0], p=2, dim=1)\n",
    "    model.coef_ = retrieval_datadict[\"coef\"]\n",
    "    model.intercept_ = retrieval_datadict[\"intercept\"]\n",
    "    pred_retrieval = torch.from_numpy(model.predict(voxels_norm).reshape(-1, retrieval_seq_dim, retrieval_emb_dim))\n",
    "    \n",
    "    \n",
    "if normalize_preds:\n",
    "    std_pred_clip_image = (pred_clip_image - torch.mean(pred_clip_image,axis=0)) / (torch.std(pred_clip_image,axis=0) + 1e-6)\n",
    "    pred_clip_image = std_pred_clip_image * torch.std(clip_image_train,axis=0) + torch.mean(clip_image_train,axis=0)\n",
    "    del clip_image_train\n",
    "    if dual_guidance:\n",
    "        std_pred_clip_text = (pred_clip_text - torch.mean(pred_clip_text,axis=0)) / (torch.std(pred_clip_text,axis=0) + 1e-6)\n",
    "        pred_clip_text = std_pred_clip_text * torch.std(clip_text_train,axis=0) + torch.mean(clip_text_train,axis=0)\n",
    "        del clip_text_train\n",
    "    if blurry_recon:\n",
    "        std_pred_blurry_vae = (pred_blurry_vae - torch.mean(pred_blurry_vae,axis=0)) / (torch.std(pred_blurry_vae,axis=0) + 1e-6)\n",
    "        pred_blurry_vae = std_pred_blurry_vae * torch.std(vae_image_train,axis=0) + torch.mean(vae_image_train,axis=0)\n",
    "        del vae_image_train\n",
    "    if retrieval:\n",
    "        std_pred_retrieval = (pred_retrieval - torch.mean(pred_retrieval,axis=0)) / (torch.std(pred_retrieval,axis=0) + 1e-6)\n",
    "        pred_retrieval = std_pred_retrieval * torch.std(retrieval_image_train,axis=0) + torch.mean(retrieval_image_train,axis=0)\n",
    "        # L2 Normalize for optimal cosine similarity\n",
    "        pred_retrieval = torch.nn.functional.normalize(pred_retrieval, p=2, dim=2)\n",
    "        del retrieval_image_train\n",
    "    if prompt_recon:\n",
    "        for sequence in range(git_seq_dim):\n",
    "            std_pred_git_text = (pred_git_text[:, sequence] - torch.mean(pred_git_text[:, sequence],axis=0)) / (torch.std(pred_git_text[:, sequence],axis=0) + 1e-6)\n",
    "            pred_git_text[:, sequence] = std_pred_git_text * torch.std(git_text_train[:, sequence],axis=0) + torch.mean(git_text_train[:, sequence],axis=0)\n",
    "        del git_text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72db1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_recon:\n",
    "    from transformers import AutoProcessor\n",
    "    from modeling_git import GitForCausalLMClipEmb\n",
    "    all_predcaptions = []\n",
    "    processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "    git_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "    git_text_model.to(device) \n",
    "    git_text_model.eval().requires_grad_(False)\n",
    "\n",
    "    for pred_text in pred_git_text:\n",
    "        pred_embedding = pred_text.to(device).to(torch.float32).unsqueeze(0)\n",
    "        generated_ids = git_text_model.generate(pixel_values=pred_embedding, max_length=20)\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "    torch.save(all_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions_{mode}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e3ac5-5a32-4ecb-8a6a-810eb66900fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_engine = DiffusionEngine(network_config=network_config,\n",
    "                       denoiser_config=denoiser_config,\n",
    "                       first_stage_config=first_stage_config,\n",
    "                       conditioner_config=conditioner_config,\n",
    "                       sampler_config=sampler_config, # using the one defined by the unclip\n",
    "                       scale_factor=scale_factor,\n",
    "                       disable_first_stage_autocast=disable_first_stage_autocast,\n",
    "                       ckpt_path=base_ckpt_path)\n",
    "base_engine.eval().requires_grad_(False)\n",
    "base_engine.to(device)\n",
    "\n",
    "base_engine.sampler.guider.scale = 5 # 5 # cfg\n",
    "\n",
    "def enhanceRecons(all_recons, captions):\n",
    "    all_enhancedrecons = None\n",
    "    \n",
    "    for img_idx in tqdm(range(len(all_recons))):\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16), base_engine.ema_scope():\n",
    "            base_engine.sampler.num_steps = 25\n",
    "            \n",
    "            image = all_recons[[img_idx]]\n",
    "            \n",
    "            if plotting:\n",
    "                print(\"blur pixcorr:\",utils.pixcorr(all_blurryrecons[[img_idx]].float(), all_images[[img_idx]].float()))\n",
    "                print(\"blur cossim:\",nn.functional.cosine_similarity(clip_img_embedder(utils.resize(all_blurryrecons[[img_idx]].float(),256).to(device)).flatten(1), \n",
    "                                                             clip_img_embedder(utils.resize(all_images[[img_idx]].float(),224).to(device)).flatten(1)))\n",
    "    \n",
    "                print(\"recon pixcorr:\",utils.pixcorr(image,all_images[[img_idx]].float()))\n",
    "                print(\"recon cossim:\",nn.functional.cosine_similarity(clip_img_embedder(utils.resize(image,224).to(device)).flatten(1), \n",
    "                                                             clip_img_embedder(utils.resize(all_images[[img_idx]].float(),224).to(device)).flatten(1)))\n",
    "            \n",
    "            image = image.to(device)\n",
    "            prompt = all_predcaptions[[img_idx]][0]\n",
    "            # prompt = \"\"\n",
    "            if plotting: \n",
    "                print(\"prompt:\",prompt)\n",
    "                plt.imshow(transforms.ToPILImage()(all_blurryrecons[img_idx].float()))\n",
    "                plt.show()\n",
    "                plt.imshow(transforms.ToPILImage()(all_recons[img_idx].float()))\n",
    "                plt.show()\n",
    "                plt.imshow(transforms.ToPILImage()(image[0]))\n",
    "                plt.show()\n",
    "    \n",
    "            # z = torch.randn(num_samples,4,96,96).to(device)\n",
    "            assert image.shape[-1]==768\n",
    "            z = base_engine.encode_first_stage(image*2-1).repeat(num_samples,1,1,1)\n",
    "    \n",
    "            openai_clip_text = base_text_embedder1(prompt)\n",
    "            clip_text_tokenized, clip_text_emb  = base_text_embedder2(prompt)\n",
    "            clip_text_emb = torch.hstack((clip_text_emb, vector_suffix))\n",
    "            clip_text_tokenized = torch.cat((openai_clip_text, clip_text_tokenized),dim=-1)\n",
    "            c = {\"crossattn\": clip_text_tokenized.repeat(num_samples,1,1), \"vector\": clip_text_emb.repeat(num_samples,1)}\n",
    "            uc = {\"crossattn\": crossattn_uc.repeat(num_samples,1,1), \"vector\": vector_uc.repeat(num_samples,1)}\n",
    "    \n",
    "            noise = torch.randn_like(z)\n",
    "            sigmas = base_engine.sampler.discretization(base_engine.sampler.num_steps).to(device)\n",
    "            init_z = (z + noise * append_dims(sigmas[-img2img_timepoint], z.ndim)) / torch.sqrt(1.0 + sigmas[0] ** 2.0)\n",
    "            sigmas = sigmas[-img2img_timepoint:].repeat(num_samples,1)\n",
    "    \n",
    "            base_engine.sampler.num_steps = sigmas.shape[-1] - 1\n",
    "            noised_z, _, _, _, c, uc = base_engine.sampler.prepare_sampling_loop(init_z, cond=c, uc=uc, \n",
    "                                                                num_steps=base_engine.sampler.num_steps)\n",
    "            for timestep in range(base_engine.sampler.num_steps):\n",
    "                noised_z = base_engine.sampler.sampler_step(sigmas[:,timestep],\n",
    "                                                            sigmas[:,timestep+1],\n",
    "                                                            denoiser, noised_z, cond=c, uc=uc, gamma=0)\n",
    "            samples_z_base = noised_z\n",
    "            samples_x = base_engine.decode_first_stage(samples_z_base)\n",
    "            samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    \n",
    "            # find best sample\n",
    "            if plotting==False and num_samples==1:\n",
    "                samples = samples[0]\n",
    "            else:\n",
    "                sample_cossim = nn.functional.cosine_similarity(clip_img_embedder(utils.resize(samples,224).to(device)).flatten(1), \n",
    "                                    clip_img_embedder(utils.resize(all_images[[img_idx]].float(),224).to(device)).flatten(1))\n",
    "                which_sample = torch.argmax(sample_cossim)\n",
    "                best_cossim = torch.max(sample_cossim)\n",
    "    \n",
    "                if plotting:\n",
    "                    print(\"samples\", samples.shape)\n",
    "                    for n in range(num_samples):\n",
    "                        recon = transforms.ToPILImage()(samples[n])\n",
    "                        plt.imshow(recon)\n",
    "                        plt.show()\n",
    "                        if (n==which_sample).item(): print(\"CHOSEN ABOVE\")\n",
    "                        print(\"upsampled pixcorr:\",utils.pixcorr(samples[[n]].cpu(),all_images[[img_idx]].float()))\n",
    "                        print(\"upsampled cossim:\",nn.functional.cosine_similarity(clip_img_embedder(utils.resize(samples[[n]],224).to(device)).flatten(1), \n",
    "                                                             clip_img_embedder(utils.resize(all_images[[img_idx]].float(),224).to(device)).flatten(1)))\n",
    "                    err # dont want to do entire for loop with plotting=True\n",
    "    \n",
    "                samples = samples[which_sample]\n",
    "    \n",
    "            samples = samples.cpu()[None]\n",
    "            if all_enhancedrecons is None:\n",
    "                all_enhancedrecons = samples\n",
    "            else:\n",
    "                all_enhancedrecons = torch.vstack((all_enhancedrecons, samples))\n",
    "                \n",
    "    all_enhancedrecons = transforms.Resize((256,256))(all_enhancedrecons).float()\n",
    "    print(\"all_enhancedrecons\", all_enhancedrecons.shape)\n",
    "    torch.save(all_enhancedrecons,f\"evals/{model_name}/{model_name}_all_enhancedrecons.pt\")\n",
    "    print(f\"saved evals/{model_name}/{model_name}_all_enhancedrecons.pt\")\n",
    "    \n",
    "    if not utils.is_interactive():\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                              | 0/10 [00:00<?, ?it/s]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/weka/proj-fmri/jonxu/MindEye_Imagery/mei-env/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:38, 12.86s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:18, 12.39s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:37<03:03, 12.25s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:49<02:50, 12.19s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:38, 12.15s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:25, 12.14s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:13, 12.14s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:37<02:01, 12.15s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:49<01:49, 12.16s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:01<01:37, 12.17s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:14<01:25, 12.18s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:26<01:13, 12.19s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:38<01:01, 12.21s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:50<00:48, 12.22s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:03<00:36, 12.23s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:15<00:24, 12.24s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:27<00:12, 12.25s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:39<00:00, 12.22s/it]\u001b[A\n",
      " 10%|████████▌                                                                            | 1/10 [03:39<32:59, 219.93s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:28, 12.25s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:15, 12.24s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:03, 12.25s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:48<02:51, 12.24s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:39, 12.24s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:26, 12.24s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:14, 12.25s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.26s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.27s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.27s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:15<01:26, 12.36s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:14, 12.34s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:39<01:01, 12.33s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:52<00:49, 12.31s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:04<00:36, 12.30s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.29s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:28<00:12, 12.29s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:41<00:00, 12.29s/it]\u001b[A\n",
      " 20%|█████████████████                                                                    | 2/10 [07:21<29:25, 220.66s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:31, 12.45s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:17, 12.36s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:04, 12.30s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:49<02:51, 12.28s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:39, 12.28s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:27, 12.27s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:14, 12.26s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.26s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.26s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.32s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:15<01:26, 12.31s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:13, 12.30s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:40<01:01, 12.37s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:52<00:49, 12.35s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:04<00:36, 12.33s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.32s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:29<00:12, 12.31s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:41<00:00, 12.32s/it]\u001b[A\n",
      " 30%|█████████████████████████▌                                                           | 3/10 [11:02<25:48, 221.20s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:29, 12.31s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:16, 12.29s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:04, 12.29s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:49<02:52, 12.29s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:39, 12.28s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:27, 12.28s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:14, 12.27s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.26s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.25s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.26s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:14<01:25, 12.26s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:13, 12.26s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:39<01:01, 12.26s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:51<00:49, 12.26s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:04<00:36, 12.27s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.27s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:28<00:12, 12.27s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:40<00:00, 12.27s/it]\u001b[A\n",
      " 40%|██████████████████████████████████                                                   | 4/10 [14:43<22:06, 221.09s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:28, 12.25s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:16, 12.26s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:03, 12.26s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:49<02:51, 12.25s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:39, 12.25s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:27, 12.26s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:14, 12.27s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.26s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.26s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.25s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:14<01:25, 12.26s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:13, 12.27s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:39<01:01, 12.35s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:52<00:49, 12.35s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:04<00:36, 12.33s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.31s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:28<00:12, 12.30s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:41<00:00, 12.28s/it]\u001b[A\n",
      " 50%|██████████████████████████████████████████▌                                          | 5/10 [18:25<18:25, 221.14s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:28, 12.26s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:16, 12.28s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:03, 12.27s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:49<02:51, 12.26s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:39, 12.26s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:27, 12.27s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:14, 12.26s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.26s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.26s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.26s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:14<01:25, 12.26s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:13, 12.26s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:39<01:01, 12.26s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:51<00:49, 12.26s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:03<00:36, 12.27s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.26s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:28<00:12, 12.26s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:40<00:00, 12.26s/it]\u001b[A\n",
      " 60%|███████████████████████████████████████████████████                                  | 6/10 [22:05<14:44, 221.05s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:28, 12.26s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:16, 12.26s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:03, 12.26s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:49<02:51, 12.25s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:39, 12.25s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:27, 12.25s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:14, 12.26s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.26s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.26s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.26s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:14<01:25, 12.27s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:13, 12.27s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:39<01:01, 12.27s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:51<00:49, 12.26s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:03<00:36, 12.27s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.27s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:28<00:12, 12.27s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:40<00:00, 12.26s/it]\u001b[A\n",
      " 70%|███████████████████████████████████████████████████████████▍                         | 7/10 [25:46<11:02, 221.00s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:28, 12.24s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:15, 12.25s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:03, 12.26s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:49<02:51, 12.25s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:39, 12.25s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:27, 12.25s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:14, 12.26s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.26s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.26s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.26s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:14<01:25, 12.26s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:13, 12.26s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:39<01:01, 12.26s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:51<00:49, 12.26s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:03<00:36, 12.27s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.30s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:28<00:12, 12.30s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:40<00:00, 12.27s/it]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████████████                 | 8/10 [29:27<07:22, 221.01s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:28, 12.24s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:15, 12.24s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:04, 12.33s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:49<02:53, 12.36s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:40, 12.33s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:27, 12.31s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:26<02:15, 12.31s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.29s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.27s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.26s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:15<01:25, 12.26s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:13, 12.26s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:39<01:01, 12.27s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:51<00:49, 12.27s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:04<00:36, 12.27s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.27s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:28<00:12, 12.27s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:41<00:00, 12.28s/it]\u001b[A\n",
      " 90%|████████████████████████████████████████████████████████████████████████████▌        | 9/10 [33:09<03:41, 221.09s/it]\n",
      "sample loop:   0%|                                                                                 | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "sample loop:   6%|████                                                                     | 1/18 [00:12<03:28, 12.25s/it]\u001b[A\n",
      "sample loop:  11%|████████                                                                 | 2/18 [00:24<03:15, 12.24s/it]\u001b[A\n",
      "sample loop:  17%|████████████▏                                                            | 3/18 [00:36<03:03, 12.24s/it]\u001b[A\n",
      "sample loop:  22%|████████████████▏                                                        | 4/18 [00:48<02:51, 12.25s/it]\u001b[A\n",
      "sample loop:  28%|████████████████████▎                                                    | 5/18 [01:01<02:39, 12.26s/it]\u001b[A\n",
      "sample loop:  33%|████████████████████████▎                                                | 6/18 [01:13<02:27, 12.26s/it]\u001b[A\n",
      "sample loop:  39%|████████████████████████████▍                                            | 7/18 [01:25<02:14, 12.26s/it]\u001b[A\n",
      "sample loop:  44%|████████████████████████████████▍                                        | 8/18 [01:38<02:02, 12.26s/it]\u001b[A\n",
      "sample loop:  50%|████████████████████████████████████▌                                    | 9/18 [01:50<01:50, 12.27s/it]\u001b[A\n",
      "sample loop:  56%|████████████████████████████████████████                                | 10/18 [02:02<01:38, 12.34s/it]\u001b[A\n",
      "sample loop:  61%|████████████████████████████████████████████                            | 11/18 [02:15<01:26, 12.33s/it]\u001b[A\n",
      "sample loop:  67%|████████████████████████████████████████████████                        | 12/18 [02:27<01:13, 12.31s/it]\u001b[A\n",
      "sample loop:  72%|████████████████████████████████████████████████████                    | 13/18 [02:39<01:01, 12.29s/it]\u001b[A\n",
      "sample loop:  78%|████████████████████████████████████████████████████████                | 14/18 [02:51<00:49, 12.29s/it]\u001b[A\n",
      "sample loop:  83%|████████████████████████████████████████████████████████████            | 15/18 [03:04<00:37, 12.34s/it]\u001b[A\n",
      "sample loop:  89%|████████████████████████████████████████████████████████████████        | 16/18 [03:16<00:24, 12.33s/it]\u001b[A\n",
      "sample loop:  94%|████████████████████████████████████████████████████████████████████    | 17/18 [03:28<00:12, 12.32s/it]\u001b[A\n",
      "sample loop: 100%|████████████████████████████████████████████████████████████████████████| 18/18 [03:41<00:00, 12.29s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 10/10 [36:50<00:00, 221.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved jonathan_unclip mi outputs!\n"
     ]
    }
   ],
   "source": [
    "final_recons = None\n",
    "final_blurryrecons = None\n",
    "if save_raw:\n",
    "    raw_root = f\"{raw_path}/{mode}/{model_name}/subject{subj}/\"\n",
    "    print(\"raw_root:\", raw_root)\n",
    "    os.makedirs(raw_root,exist_ok=True)\n",
    "    torch.save(pred_clip_image, f\"{raw_root}/{image_embedding_variant}_image_voxels.pt\")\n",
    "    if dual_guidance:\n",
    "        torch.save(pred_clip_text, f\"{raw_root}/{text_embedding_variant}_text_voxels.pt\")\n",
    "    if blurry_recon:\n",
    "        torch.save(pred_blurry_vae, f\"{raw_root}/{latent_embedding_variant}_latent_voxels.pt\")\n",
    "    if retrieval:\n",
    "        torch.save(pred_retrieval, f\"{raw_root}/{retrieval_embedding_variant}_retrieval_voxels.pt\")\n",
    "\n",
    "if num_images_per_sample == 1:\n",
    "    for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "        clip_voxels = pred_clip_image[idx]\n",
    "        if dual_guidance:\n",
    "            clip_text_voxels = pred_clip_text[idx]\n",
    "        else:\n",
    "            clip_text_voxels = None\n",
    "        \n",
    "        latent_voxels=None\n",
    "        if blurry_recon:\n",
    "            latent_voxels = pred_blurry_vae[idx].unsqueeze(0)\n",
    "            blurred_image = vdvae.reconstruct(latents=latent_voxels)\n",
    "            if filter_sharpness:\n",
    "                # This helps make the output not blurry when using the VDVAE\n",
    "                blurred_image = ImageEnhance.Sharpness(blurred_image).enhance(20)\n",
    "            if filter_contrast:\n",
    "                # This boosts the structural impact of the blurred_image\n",
    "                blurred_image = ImageEnhance.Contrast(blurred_image).enhance(1.5)\n",
    "            im = transforms.ToTensor()(blurred_image)\n",
    "            if final_blurryrecons is None:\n",
    "                final_blurryrecons = im.cpu()\n",
    "            else:\n",
    "                final_blurryrecons = torch.vstack((final_blurryrecons, im.cpu()))\n",
    "                    \n",
    "        samples = utils.unclip_recon(clip_voxels.half().unsqueeze(0),\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix,\n",
    "                             num_samples=gen_rep)\n",
    "    \n",
    "        \n",
    "        if save_raw:\n",
    "            os.makedirs(f\"{raw_root}/{idx}/\", exist_ok=True)\n",
    "            for rep in range(gen_rep):\n",
    "                transforms.ToPILImage()(samples[rep]).save(f\"{raw_root}/{idx}/{rep}.png\")\n",
    "            transforms.ToPILImage()(all_images[idx]).save(f\"{raw_root}/{idx}/ground_truth.png\")\n",
    "            transforms.ToPILImage()(transforms.ToTensor()(blurred_image).cpu()).save(f\"{raw_root}/{idx}/low_level.png\")\n",
    "            torch.save(clip_voxels, f\"{raw_root}/{idx}/clip_image_voxels.pt\")\n",
    "            if dual_guidance:\n",
    "                torch.save(clip_text_voxels, f\"{raw_root}/{idx}/clip_text_voxels.pt\")\n",
    "\n",
    "        if final_recons is None:\n",
    "            final_recons = samples.unsqueeze(0).cpu()\n",
    "        else:\n",
    "            final_recons = torch.cat((final_recons, samples.unsqueeze(0).cpu()), dim=0)\n",
    "else:\n",
    "    for rep in tqdm(range(gen_rep)):\n",
    "        utils.seed_everything(seed = random.randint(0,10000000))\n",
    "        # get all reconstructions    \n",
    "        all_blurryrecons = None\n",
    "        all_recons = None\n",
    "        \n",
    "        minibatch_size = 1\n",
    "        plotting = False\n",
    "        for idx in tqdm(range(0,voxels.shape[0]), desc=\"sample loop\"):\n",
    "            clip_voxels = pred_clip_image[idx]\n",
    "            if dual_guidance:\n",
    "                clip_text_voxels = pred_clip_text[idx]\n",
    "            else:\n",
    "                clip_text_voxels = None\n",
    "                \n",
    "            blurred_image=None\n",
    "            if blurry_recon:\n",
    "                latent_voxels = pred_blurry_vae[idx].unsqueeze(0)\n",
    "                blurred_image = vdvae.reconstruct(latents=latent_voxels)\n",
    "                if filter_sharpness:\n",
    "                    # This helps make the output not blurry when using the VDVAE\n",
    "                    blurred_image = ImageEnhance.Sharpness(blurred_image).enhance(20)\n",
    "                if filter_contrast:\n",
    "                    # This boosts the structural impact of the blurred_image\n",
    "                    blurred_image = ImageEnhance.Contrast(blurred_image).enhance(1.5)\n",
    "                im = transforms.ToTensor()(blurred_image)\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im.cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im.cpu()))\n",
    "                    \n",
    "            if retrieval:\n",
    "                retrieval_voxels = pred_retrieval[idx].unsqueeze(0)\n",
    "            else:\n",
    "                retrieval_voxels = clip_voxels\n",
    "\n",
    "            samples_multi = utils.unclip_recon(clip_voxels.half().unsqueeze(0),\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix,\n",
    "                             num_samples=gen_rep)\n",
    "\n",
    "            # Refiner step\n",
    "            refined_samples_multi = enhanceRecons(samples_multi, all_predcaptions)\n",
    "            \n",
    "            samples = utils.pick_best_recon(refined_samples_multi, retrieval_voxels, reconstructor, hidden=retrieval).unsqueeze(0)\n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            \n",
    "            if save_raw:\n",
    "                os.makedirs(f\"{raw_root}/{idx}/\", exist_ok=True)\n",
    "                transforms.ToPILImage()(samples[0]).save(f\"{raw_root}/{idx}/{rep}.png\")\n",
    "                \n",
    "                if rep == 0:\n",
    "                    os.makedirs(f\"{raw_root}/{idx}/retrieval_images/\", exist_ok=True)\n",
    "                    for r_idx, image in enumerate(refined_samples_multi):\n",
    "                        transforms.ToPILImage()(image).save(f\"{raw_root}/{idx}/retrieval_images/{r_idx}.png\")\n",
    "                    transforms.ToPILImage()(all_images[idx]).save(f\"{raw_root}/{idx}/ground_truth.png\")\n",
    "                    if blurry_recon:\n",
    "                        transforms.ToPILImage()(transforms.ToTensor()(blurred_image).cpu()).save(f\"{raw_root}/{idx}/low_level.png\")\n",
    "                    torch.save(clip_voxels, f\"{raw_root}/{idx}/clip_image_voxels.pt\")\n",
    "                    if dual_guidance:\n",
    "                        torch.save(clip_text_voxels, f\"{raw_root}/{idx}/clip_text_voxels.pt\")\n",
    "                    if prompt_recon:\n",
    "                        with open(f\"{raw_root}/{idx}/predicted_caption.txt\", \"w\") as f:\n",
    "                            f.write(all_predcaptions[idx])\n",
    "            \n",
    "        if final_recons is None:\n",
    "            final_recons = all_recons.unsqueeze(1)\n",
    "            if blurry_recon:\n",
    "                final_blurryrecons = all_blurryrecons.unsqueeze(1)\n",
    "        else:\n",
    "            final_recons = torch.cat((final_recons, all_recons.unsqueeze(1)), dim=1)\n",
    "            if blurry_recon:\n",
    "                final_blurryrecons = torch.cat((final_blurryrecons, all_blurryrecons.unsqueeze(1)), dim=1)\n",
    "        \n",
    "if blurry_recon:\n",
    "    torch.save(final_blurryrecons,f\"evals/{model_name}/{model_name}_all_blurryrecons_{mode}.pt\")\n",
    "torch.save(final_recons,f\"evals/{model_name}/{model_name}_all_recons_{mode}.pt\")\n",
    "print(f\"saved {model_name} mi outputs!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8966a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
