{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['HF_HOME'] = \"../cache/\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from sd35_reconstructor import SD35_Reconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable Diffusion 3.5 Reconstructor: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenAI CLIP L...\n",
      "Loading OpenCLIP bigG...\n",
      "Loading Google T5-v1-XXL...\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "reconstructor = SD35_Reconstructor(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 154, 4096]) torch.Size([1, 2048])\n",
      "torch.Size([1, 16, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [00:13<00:08,  1.81it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# index = 11\n",
    "# t5_pred = t5_preds[index]\n",
    "# clip_text_pred = clip_text_preds[index]\n",
    "prompt = \"a woman riding a horse\"\n",
    "# image = Image.open(f\"{sc_recon_path}/{index}/5.png\")\n",
    "prompt_embed = reconstructor.embed_text(prompt)\n",
    "print(prompt_embed[0].dtype, prompt_embed[1].dtype)\n",
    "# image_embed = recon.embed_image(image)\n",
    "image = Image.open(\"/home/naxos2-raid25/kneel027/home/kneel027/IP-Adapter_MEI/assets/images/horse.jpg\")\n",
    "latent_embed = reconstructor.embed_latent(image)\n",
    "print(latent_embed.shape, latent_embed.dtype)\n",
    "# print(torch.mean(image_embed), torch.mean(prompt_embed), torch.sum(image_embed), torch.sum(prompt_embed))\n",
    "# print(prompt_embed.shape, image_embed.shape, latent_embed.shape)\n",
    "# url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "# init_image = load_image(url).resize((1024, 1024))\n",
    "\n",
    "images = reconstructor.reconstruct(latent=latent_embed, c_t=prompt_embed[0], t5=prompt_embed[1], strength=0.98, num_steps=40, n_samples=1, cfg=4.5)\n",
    "# show_images(sample)\n",
    "display(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye_imagery_sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
