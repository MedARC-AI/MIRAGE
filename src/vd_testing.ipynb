{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from versatile_diffusion import Reconstructor\n",
    "import torch\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructor: Loading model... fp16: True, clip_only: True\n",
      "\n",
      "#######################\n",
      "# Running in eps mode #\n",
      "#######################\n",
      "\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Load pth from ../cache/kl-f8.pth\n",
      "Load autoencoderkl with total 83653863 parameters,72921.759 parameter sum.\n",
      "Load optimus_bert_connector with total 109489920 parameters,18936.836 parameter sum.\n",
      "Load optimus_gpt2_connector with total 132109824 parameters,19327.192 parameter sum.\n",
      "Load pth from ../cache/optimus-vae.pth\n",
      "Load optimus_vae_next with total 241599744 parameters,-344611.688 parameter sum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load clip_image_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load clip_text_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load openai_unet_2d_next with total 859520964 parameters,100311.692 parameter sum.\n",
      "Load openai_unet_0d_next with total 1706797888 parameters,250131.434 parameter sum.\n",
      "Load vd_v2_0 with total 3746805485 parameters,206768.216 parameter sum.\n"
     ]
    }
   ],
   "source": [
    "reconstructor = Reconstructor(device='cuda:1', cache_dir='../cache', ddim_steps=20, clip_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 768]) torch.Size([1, 77, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/raid1/home/kneel027/mamba/envs/mindeye_imagery_vd/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt_im = Image.open('/home/naxos2-raid25/kneel027/home/kneel027/IP-Adapter/assets/images/woman.png')\n",
    "text_clip = reconstructor.embed_text(\"A cat\")\n",
    "image_clip = reconstructor.embed_image(prompt_im)\n",
    "print(image_clip.shape, text_clip.shape)\n",
    "# reconstruction = reconstructor.reconstruct(c_t=text_clip)\n",
    "# reconstruction = reconstructor.reconstruct(c_i=image_clip, c_t=text_clip)\n",
    "# reconstruction\n",
    "# reconstruction2 = reconstructor.reconstruct(image=reconstruction, c_t=text_clip, strength=0.85)\n",
    "# reconstruction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images...:  55%|█████▍    | 799/1460 [10:38<09:42,  1.14it/s]  "
     ]
    }
   ],
   "source": [
    "f = h5py.File(f'../dataset/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images']\n",
    "emb_batch_size = 50\n",
    "clip_image = torch.zeros((len(images), 257 * 768)).to(\"cpu\")\n",
    "for i in tqdm(range(len(images) // emb_batch_size), desc=\"Encoding images...\"):\n",
    "    batch_images = images[i * emb_batch_size:i * emb_batch_size + emb_batch_size]\n",
    "    batch_embeddings = reconstructor.embed_image(torch.from_numpy(batch_images)).reshape(emb_batch_size, -1).detach().to(\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
